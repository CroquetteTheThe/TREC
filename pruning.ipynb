{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "from rnn import *\n",
    "from seed import *\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "#from torch.autograd import Variable\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "\n",
    "#from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from question_dataset import *\n",
    "from functions import *\n",
    "from learn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start\n"
     ]
    }
   ],
   "source": [
    "print(\"Start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création training set...\n",
      "Done!\n",
      "Création dev set...\n",
      "Done!\n",
      "Création test set...\n",
      "Done!\n",
      "Création du set de pruning...\n",
      "Done!\n",
      "List of word used:\n",
      "['the', 'what', 'is', 'of', 'in', 'a', 'how', 's', 'was', 'who', 'to', ',', 'are', 'for', 'and', 'did', 'does', 'do', 'name', 'on', 'many', 'where', 'i', 'you', 'can', 'first', 'when', 'from', 'which', 'world', 'that', 'city', 'as', 'with', 'country', 'has', 'most', '.', 'u.s.', 'by', 'an', 'have', 'find', 'it', 'why', 'there', 'get', 'people', 'called', 'state', 'year', 'were', 'mean', 'be', 'american', 'president', 'largest', 'his', 'fear', 'two', 'at', 'war', 'new', 'its', 'origin', 'word', 'much', 'about', 'known', 'kind', 'company', 'between', 'game', 'film', 'long', 'day', 'movie', 'live', 'made', 'your', 'or', 'take', 'only', 'stand', 'man', 'best', 'book', 'tv', 'their', 'john', 'one', 'famous', 'color', 'all', 'show', 'star', 'used', '<bos>', '<eos>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seeding_random()\n",
    "\n",
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "data = []\n",
    "for line in file.readlines():\n",
    "    data.append(line)\n",
    "train_data = data[:round(len(data)*devLine)]\n",
    "dev_data = data[round(len(data)*devLine):]\n",
    "\n",
    "print(\"Création training set...\")\n",
    "training_set = QuestionDataset(train_data, nb_input-3)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création dev set...\")\n",
    "dev_set = QuestionDataset(dev_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création test set...\")\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)\n",
    "test_set = QuestionDataset(test_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "# Création du DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "seeding_random()\n",
    "\n",
    "train_loader = dataloader.DataLoader(training_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "dataloader_args_notshuffle = dict(shuffle=False, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "\n",
    "dev_loader = dataloader.DataLoader(dev_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "test_loader = dataloader.DataLoader(test_set, **dataloader_args_notshuffle)\n",
    "seeding_random()\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"Création du set de pruning...\")\n",
    "if SIZE_DATA_PRUNING == -1:\n",
    "    prune_data = data\n",
    "else:\n",
    "    prune_data = data[:SIZE_DATA_PRUNING]\n",
    "prune_set = QuestionDataset(prune_data, training_set.word_list)\n",
    "\n",
    "prune_loader = dataloader.DataLoader(prune_set, **dataloader_args)\n",
    "seeding_random()\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "print(\"List of word used:\")\n",
    "print(training_set.word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition per classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "    classes = [0,0,0,0,0,0]\n",
    "    for data, target in train_loader:\n",
    "        for t in list(target):\n",
    "            t = t.item()\n",
    "            classes[t] += 1\n",
    "\n",
    "    total = sum(classes)\n",
    "    rep_classes = [c/total*100 for c in classes]\n",
    "    print(\"Répartitions des données dans les classes:\")\n",
    "    for i in range(len(rep_classes)):\n",
    "        print(\"Classe numéro \" + str(i+1) + \": \" + str(rep_classes[i]) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word occurence repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "\n",
    "    word_occ = training_set.reparti_word\n",
    "    word_occ = dict(word_occ)\n",
    "    \n",
    "    total = sum([value for key, value in training_set.reparti_word.most_common(len(training_set.reparti_word))])\n",
    "    \n",
    "    values = [sum([value for key, value in training_set.reparti_word.most_common(i+1)])/total*100 for i in range(len(training_set.reparti_word))]\n",
    "\n",
    "    x = np.linspace(0, len(values), len(values))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    cnn_line, = ax.plot(x, values)\n",
    "\n",
    "    ax.set(xlabel=\"Vocabulaire unique\", ylabel=\"Couverture en %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do it after learning\n",
    "### Will prune neurons\n",
    "def oracle_prune(rnn, data_loader, nb_times):\n",
    "    accuracy = -1\n",
    "    prune_acc = accuracy\n",
    "    pruned_neur = []\n",
    "    save_state(rnn, \"first_state_rnn\")\n",
    "    save_state(rnn, \"pruned_state_rnn\")\n",
    "    pos_prune = -1\n",
    "    for _ in range(nb_times):\n",
    "        with torch.enable_grad():\n",
    "            if accuracy == -1:\n",
    "                accuracy = getEfficience(rnn, data_loader)*100\n",
    "            # Compute what will be the best amelioration\n",
    "            prev_layer = rnn.layers[0]\n",
    "            min_acc = -1\n",
    "            for i, l in enumerate(rnn.layers):\n",
    "                if i != 0:\n",
    "                    for fromm in range(prev_layer):\n",
    "                        for to in range(l):\n",
    "                            get_weights(rnn, i)[to][fromm] = 0\n",
    "                        acc = getEfficience(rnn, data_loader)*100\n",
    "                        if min_acc < acc:\n",
    "                            min_acc = acc\n",
    "                            pos = {'layer': i, 'nb_prev': fromm}\n",
    "                            pos_prune = i-1\n",
    "                        rnn = load_state(rnn, \"pruned_state_rnn\")\n",
    "                prev_layer = l\n",
    "                print(\"layer \"+str(i)+\" done! min acc = \" + str(min_acc) + \" | original acc = \"+ str(accuracy))\n",
    "\n",
    "            # Finally prune\n",
    "            prune_acc = min_acc\n",
    "            pruned_neur.append(pos_prune)\n",
    "            print(pos)\n",
    "            for to in range(rnn.layers[pos['layer']]):\n",
    "                get_weights(rnn, pos['layer'])[to][pos['nb_prev']] = 0\n",
    "        save_state(rnn, \"pruned_state_rnn\")\n",
    "\n",
    "    rnn = load_state(rnn, \"first_state_rnn\")\n",
    "    ret = rnn.layers\n",
    "    for prune in pruned_neur:\n",
    "        print(len)\n",
    "        ret[prune] -= 1\n",
    "    return ret, prune_acc\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "rnn = RNN(nb_inputs = nb_input, layers = LAYERS, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "\n",
    "seeding_random()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 1/8 [10800/10816 (100%)]\t Loss: 1.748596\t Accuracy: 21.1353550295858\n",
      "Dev set: accuracy: 21.893874029335635% | max acc: 21.893874029335635%\n",
      "\n",
      " Train Epoch: 2/8 [10800/10816 (100%)]\t Loss: 1.710031\t Accuracy: 22.651627218934912\n",
      "Dev set: accuracy: 21.440897325280414% | max acc: 21.893874029335635%\n",
      "\n",
      " Train Epoch: 3/8 [10800/10816 (100%)]\t Loss: 1.640210\t Accuracy: 23.09541420118343\n",
      "Dev set: accuracy: 21.958584987057808% | max acc: 21.958584987057808%\n",
      "\n",
      " Train Epoch: 4/8 [10800/10816 (100%)]\t Loss: 1.624143\t Accuracy: 23.92751479289941\n",
      "Dev set: accuracy: 23.209663503019843% | max acc: 23.209663503019843%\n",
      "\n",
      " Train Epoch: 5/8 [10800/10816 (100%)]\t Loss: 1.612631\t Accuracy: 27.024778106508872\n",
      "Dev set: accuracy: 27.73943054357205% | max acc: 27.73943054357205%\n",
      "\n",
      " Train Epoch: 6/8 [10800/10816 (100%)]\t Loss: 1.552352\t Accuracy: 28.929363905325445\n",
      "Dev set: accuracy: 29.89646246764452% | max acc: 29.89646246764452%\n",
      "\n",
      " Train Epoch: 7/8 [10800/10816 (100%)]\t Loss: 1.547472\t Accuracy: 31.813979289940832\n",
      "Dev set: accuracy: 31.622088006902505% | max acc: 31.622088006902505%\n",
      "\n",
      " Train Epoch: 8/8 [10800/10816 (100%)]\t Loss: 1.545432\t Accuracy: 32.71079881656805\n",
      "Dev set: accuracy: 32.11820534943917% | max acc: 32.11820534943917%\n",
      "\n",
      "Done :)\n",
      "Learned in 0:01:02.595393\n"
     ]
    }
   ],
   "source": [
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "with torch.enable_grad():\n",
    "    job = learn(rnn, train_loader, dev_loader, nb_epochs, great_analysis)\n",
    "    \n",
    "    losses_train = job[\"losses_train\"]\n",
    "    losses_dev = job[\"losses_dev\"]\n",
    "    pos_best_rnn = job[\"pos_best\"]\n",
    "    print(\"Done :)\")\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Learned in \" + str(end_time - begin_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 0 done! min acc = 99999999 | original acc = 33.02485115195444\n",
      "5\n",
      "layer 1 done! min acc = 19.253171110535853 | original acc = 33.02485115195444\n",
      "{'nb_prev': 0, 'layer': 1}\n",
      "<built-in function len>\n",
      "after one pass in oracle_pruning, we can remove:[4, 6] and get an accuracy of 19.253171110535853\n"
     ]
    }
   ],
   "source": [
    "# Prune manipulations\n",
    "accuracy = getEfficience(rnn, prune_loader)*100\n",
    "\n",
    "pruned_layers, acc_prune = oracle_prune(rnn, prune_loader, 1)\n",
    "\n",
    "print(\"after one pass in oracle_pruning, we can remove:\" + str(pruned_layers) + \" and get an accuracy of \" + str(acc_prune))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67d073f779344be89e3e17b22a8d373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='smooth', max=500, min=5, step=2), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def update_losses(smooth=1):\n",
    "    x_train = np.linspace(0, len(losses_train), len(losses_train))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax_train = fig.add_subplot(1,1,1)\n",
    "    cnn_line_train, = ax_train.plot(x_train, losses_train)\n",
    "    cnn_line_train.set_ydata(savgol_filter(losses_train, smooth, 3))\n",
    "    \n",
    "    if great_analysis:\n",
    "        x_dev = np.linspace(0, len(losses_dev), len(losses_dev))\n",
    "        ax_dev = fig.add_subplot(1,1,1)\n",
    "        cnn_line_dev, = ax_dev.plot(x_dev, losses_dev)\n",
    "        cnn_line_dev.set_ydata(savgol_filter(losses_dev, smooth, 3))\n",
    "    \n",
    "interact(update_losses, smooth=(5, 500, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n",
      "On the training set:\n",
      "Corrects: 19.535872781065088%\n",
      "\n",
      "On the dev set:\n",
      "Corrects: 18.485763589301122%\n",
      "\n",
      "On the test set:\n",
      "Moyenne des entropies: -1\n",
      "Corrects: 31.2%\n",
      "\n",
      "A présent, tu peux copier-coller ça dans le doc sur le drive :)\n",
      "100\t0.001\t8\t[5, 6]\t16\t\t-1\t8\t19.535872781065088%\t18.485763589301122%\t31.2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Congratulations!\")\n",
    "\n",
    "rnn.eval()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_train = getEfficience(rnn, train_loader)*100\n",
    "\n",
    "print(\"On the training set:\")\n",
    "print(\"Corrects: \" + str(correct_train) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_dev = getEfficience(rnn, dev_loader)*100\n",
    "\n",
    "print(\"On the dev set:\")\n",
    "print(\"Corrects: \" + str(correct_dev) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_test = getEfficience(rnn, test_loader)*100\n",
    "\n",
    "mean_entropies = -1\n",
    "\n",
    "print(\"On the test set:\")\n",
    "print(\"Moyenne des entropies: \" + str(mean_entropies))\n",
    "print(\"Corrects: \" + str(correct_test) + \"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "inputs = nb_input\n",
    "if inputs == -1:\n",
    "    inputs = len(training_set.word_list)-3\n",
    "\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(inputs)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(LAYERS)\n",
    "      +\"\\t\"+str(nb_batchs)+\"\\t\\t\"+str(mean_entropies)+\"\\t\"+str(pos_best_rnn)\n",
    "      +\"\\t\"+str(correct_train)+\"%\\t\"+str(correct_dev)+\"%\\t\"+str(correct_test)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
