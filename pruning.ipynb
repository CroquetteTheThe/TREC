{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 1700 # write -1 if you want every words\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 3\n",
    "hidden_size = 42\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 60\n",
    "\n",
    "# Random seed, don't change it if you don't know what it is\n",
    "random_seed = 42\n",
    "\n",
    "nb_batchs = 16\n",
    "\n",
    "# How many percent of your data do you use as training set\n",
    "devLine = 0.7\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If your goal is to draw graphs\n",
    "great_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding_random():\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    \n",
    "    # Special constructor\n",
    "    # | nb_most_commons can either be the number of most common words you\n",
    "    # | want to work with, OR a list of word you want to work with\n",
    "    # If nb_most_commons == -1, then all word will count\n",
    "    \n",
    "    def __init__(self, train_data, nb_most_commons=-1):\n",
    "        questions = []\n",
    "        labels = []\n",
    "\n",
    "        # Black list\n",
    "        black_list = '\\'`[@_!#$%^&*()<>?/\\|}{~:]'\n",
    "        \n",
    "        for string in train_data:\n",
    "            question_str = []\n",
    "            for x in string.split()[1:]:\n",
    "                s = \"\"\n",
    "                for c in x:\n",
    "                    if not c in black_list:\n",
    "                        s += c\n",
    "                if not s == \"\":\n",
    "                    question_str.append(s.lower())\n",
    "                        \n",
    "            labels.append(string.split()[0])\n",
    "            questions.append(question_str)\n",
    "\n",
    "        \n",
    "        if isinstance(nb_most_commons, int):\n",
    "            # Vocabulary of unique words\n",
    "            data = []\n",
    "            for q in questions:\n",
    "                for w in q:\n",
    "                    data.append(w)\n",
    "            self.reparti_word = Counter(data)\n",
    "            \n",
    "            if nb_most_commons < 0:\n",
    "                most_commons_words = self.reparti_word.most_common(len(data))\n",
    "            else:\n",
    "                most_commons_words = self.reparti_word.most_common(nb_most_commons)\n",
    "            \n",
    "            self.word_list = list([x[0] for x in most_commons_words])\n",
    "            self.word_list.append('<bos>')\n",
    "            self.word_list.append('<eos>')\n",
    "            self.word_list.append('<unk>')\n",
    "        elif isinstance(nb_most_commons, list):\n",
    "            self.word_list = nb_most_commons\n",
    "        else:\n",
    "            print(\"ERROR: second arg is neither an int, nor a list\")\n",
    "            \n",
    "        words_array = np.array(self.word_list)\n",
    "        \n",
    "        # Add tags <bos> and <eos> to questions\n",
    "        for q in questions:\n",
    "            if q[0] != '<bos>' :\n",
    "                q.insert(0, '<bos>')\n",
    "                q.append('<eos>')\n",
    "\n",
    "        # Integer encoding with OneHotEncoder\n",
    "        words_tre = words_array.reshape(len(words_array),1)\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "        # Creating a dictionnary of word and its one hot array\n",
    "        self.words_onehoted = {}\n",
    "        for i in range(0, len(words_array)):\n",
    "            self.words_onehoted[self.word_list[i]] = onehot_encoded[i]\n",
    "\n",
    "        # One hot categories\n",
    "        self.categories_num = {}\n",
    "        self.categories_num['ABBR'] = 0 # Abbreviation\n",
    "        self.categories_num['ENTY'] = 1 # Entity\n",
    "        self.categories_num['DESC'] = 2 # Description\n",
    "        self.categories_num['HUM']  = 3 # Human\n",
    "        self.categories_num['LOC']  = 4 # Location\n",
    "        self.categories_num['NUM']  = 5 # Numeric\n",
    "\n",
    "        self.batch_data = []\n",
    "        for num_question in range(len(questions)):\n",
    "            # Construction of question_onehot list.\n",
    "            question_onehot = [self.get_onehot_word(word) for word in questions[num_question]]\n",
    "\n",
    "            # Construction of category_onehot.\n",
    "            category = labels[num_question].partition(':')[0]\n",
    "            category_onehot = self.get_num_category(category)\n",
    "            self.batch_data.append([(question_onehot), (category_onehot)])\n",
    "        \n",
    "    \n",
    "    # Function to get the corresponding one hot list for a category.\n",
    "    def get_num_category(self, category):\n",
    "        return self.categories_num[category]\n",
    "\n",
    "\n",
    "    # Function to get the corresponding one hot list for a word.\n",
    "    def get_onehot_word(self, word):\n",
    "        if word in self.words_onehoted:\n",
    "            return list(self.words_onehoted[word])\n",
    "        else:\n",
    "            return list(self.words_onehoted['<unk>'])\n",
    "\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.batch_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeding_random()\n",
    "        return self.batch_data[idx]\n",
    "    \n",
    "def pad_collate(batch):\n",
    "    max_length = max([len(q[0]) for q in batch])\n",
    "\n",
    "    inputs = torch.FloatTensor([[[0. for _ in range(len(x[0][0]))] for i in range(max_length-len(x[0]))]+x[0] for x in batch])\n",
    "    outputs = torch.LongTensor([x[1] for x in batch])\n",
    "    \n",
    "    return inputs, outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création training set...\n",
      "Done!\n",
      "Création dev set...\n",
      "Done!\n",
      "Création test set...\n",
      "Done!\n",
      "List of word used:\n",
      "['the', 'what', 'is', 'of', 'in', 'a', 'how', 's', 'was', 'who', 'to', ',', 'are', 'for', 'and', 'did', 'does', 'do', 'name', 'on', 'many', 'where', 'i', 'you', 'can', 'first', 'when', 'from', 'which', 'world', 'that', 'city', 'as', 'with', 'country', 'has', 'most', '.', 'u.s.', 'by', 'an', 'have', 'find', 'it', 'why', 'there', 'people', 'get', 'called', 'state', 'were', 'year', 'mean', 'be', 'american', 'president', 'largest', 'his', 'fear', 'two', 'at', 'war', 'new', 'its', 'origin', 'word', 'much', 'about', 'known', 'kind', 'company', 'between', 'game', 'film', 'long', 'movie', 'day', 'live', 'made', 'your', 'or', 'take', 'only', 'stand', 'man', 'book', 'best', 'tv', 'their', 'john', 'one', 'famous', 'color', 'all', 'star', 'show', 'term', 'he', 'used', 'my', 'out', 'come', 'play', 'baseball', 'had', 'invented', 'into', 'call', 'number', 'make', 'countries', 'home', 'time', 'old', 'dog', 'america', 'character', 'team', 'nn', 'actor', 'not', 'three', 'information', 'average', 'river', 'states', 'some', 'highest', 'south', 'after', 'write', 'body', 'names', 'if', 'use', 'years', 'difference', 'born', 'up', 'university', 'won', 'said', 'during', 'played', 'say', 'killed', 'us', 'four', 'work', 'black', 'united', 'died', 'become', 'english', 'would', '1', 'song', 'novel', 'earth', 'good', 'named', 'wrote', 'last', 'car', 'meaning', 'computer', 'water', 'go', 'school', 'space', 'located', 'common', 'different', 'drink', 'longest', 'horse', 'sport', 'woman', 'king', 'place', 'will', 'animal', 'been', 'battle', 'date', 'north', 'me', 'her', 'makes', 'over', 'mississippi', 'mountain', 'contains', 'address', 'top', 'national', 'way', 'system', 'times', 't', 'actress', 'russian', 'college', 'internet', 'nickname', 'population', 'popular', 'island', 'served', 'causes', 'abbreviation', 'created', 'little', 'form', 'capital', 'history', 'whose', 'law', 'they', 'portrayed', 'money', 'part', 'craft', 'through', 'language', 'animals', 'death', 'west', 'product', 'bowl', 'queen', 'group', 'being', 'mother', 'randy', 'than', 'became', 'food', 'person', 'biggest', 'california', 'great', 'die', 'should', 'like', 'air', 'other', 'life', 'ball', 'start', 'office', 'married', '2', 'york', 'cities', 'james', 'spanish', 'french', 'england', 'cnn', 'definition', 'william', 'games', 'european', 'each', 'before', 'following', 'boasts', 'bill', 'general', 'musical', 'soft', 'cost', 'line', 'feet', 'win', 'fastest', 'kennedy', 'happened', 'international', 'leader', 'more', 'lives', 'building', 'children', 'love', 'eat', 'f.', 'comic', 'airport', 'big', 'super', 'san', 'cards', 'letter', 'moon', 'charles', 'title', 'features', '-', 'series', 'canada', 'board', 'baby', 'prize', 'percentage', 'british', 'germany', 'disease', 'instrument', 'female', 'another', 'sometimes', 'center', 'night', 'power', 'japanese', 'real', 'business', 'indians', 'author', 'second', 'type', 'white', 'caused', 'begin', 'indian', 'nnp', 'age', 'fire', 'ever', 'men', 'bear', 'newspaper', 'saw', 'jaws', 'park', 'latin', 'lawyer', 'seven', 'nixon', 'symbol', 'olympic', 'sea', 'travel', 'role', 'organization', 'hitler', 'colors', 'television', 'st.', 'desert', 'london', 'element', 'far', 'miles', 'found', 'someone', 'house', 'stop', 'nobel', 'website', 'list', 'richard', 'starred', 'county', 'member', 'under', 'blood', 'soviet', 'music', 'security', 'give', 'believe', 'bridge', 'green', 'five', 'oldest', 'hit', 'former', 'street', 'introduced', 'store', 'soldiers', 'once', 'greek', 'words', 'ocean', 'africa', 'founded', 'hair', 'boxing', 'singing', 'need', 'golf', 'original', 'lake', 'light', 'million', 'art', 'monopoly', 'types', 'washington', 'shea', 'six', 'el', 'back', 'runs', 'nt', 'titled', 'flight', 'them', 'cartoon', 'george', 'rate', 'birds', 'built', 'we', 'bible', 'march', 'human', 'de', 'gould', 'red', 'cold', 'thing', 'flag', 'civil', 'union', '5', '0', 'point', 'gold', 'sioux', 'full', 'never', 'miss', 'clock', 'christmas', 'produced', 'months', 'france', 'fame', 'fast', 'birth', 'dubbed', 'area', 'islands', 'jack', 'prime', 'size', 'so', 'roman', 'but', 'education', 'web', '1984', 'east', 'minister', 'texas', 'tallest', 'fought', 'cover', 'look', 'asian', 'chinese', 'ship', 'know', 'oil', 'women', 'fly', 'dead', 'main', 'worth', 'kentucky', 'tree', 'department', 'hero', 'originate', '1991', 'americans', 'paid', 'eyes', 'peter', 'whom', 'major', 'came', 'sports', 'headquarters', 'tell', 'thatcher', 'market', 'rain', 'ray', 'shot', 'player', 'painted', 'received', 'jude', 'stars', 'spumante', 'beach', 'government', 'produce', 'eye', 'films', 'historical', 'motto', 'wear', 'commercial', 'daughter', 'dick', 'host', 'girl', 'assassinated', 'hole', 'league', 'painting', 'kid', 'watch', 'reims', 'celebrated', 'favorite', 'schools', 'paper', 'see', 'amendment', 'wine', 'chemical', 'sun', 'setting', 'stock', 'standard', 'hands', '1899', 'sign', 'electric', 'god', 'berlin', 'patent', 'shakespeare', 'speed', 'ice', 'medical', 'products', 'father', 'this', 'considered', 'poem', 'wall', 'characters', 'comedian', 'species', 'tennis', 'empire', 'own', 'songs', 'remove', 'based', 'forest', 'writer', 'diego', 'presidential', 'around', 'off', 'chocolate', 'dt', 'current', 'lived', 'pearl', 'social', 'bowling', 'los', 'reason', 'beatles', 'henry', 'opera', 'video', 'desmond', 'ii', 'vietnam', 'without', 'brothers', 'massachusetts', 'constitution', 'jean', 'serve', 'steven', 'numbers', 'inspired', 'down', 'rights', 'lyrics', 'run', 'cd', 'balls', 'card', 'angeles', 'artist', 'weight', 'build', 'often', 'winter', 'emperor', '11', '3', 'mrs.', 'put', 'singer', 'followed', 'head', 'students', 'classical', 'la', 'acid', 'career', 'harbor', 'books', 'free', 'while', 'always', 'secretary', 'function', 'family', 'bomb', 'week', 'because', 'square', 'creature', 'johnny', 'governor', 'took', 'spielberg', 'letters', 'magic', 'spoken', 'vegas', 'rock', 'jackson', 'bounty', 'equal', 'nations', 'rascals', 'council', 'oceans', 'irish', 'bond', 'band', 'famed', '10', 'high', 'hand', 'sold', 'story', 'mount', 'featured', 'create', 'spy', 'e-mail', 'export', 'poet', 'india', 'starring', 'rule', 'beer', 'cowboy', '8', 'african', 'brand', 'twins', '15', 'radio', 'michelangelo', 'lakes', 'ask', 'expression', 'las', 'inside', 'golden', 'alaska', 'plant', 'astronauts', 'football', 'van', 'online', 'silly', 'golfer', 'telephone', '19', 'read', 'field', 'himself', 'europe', 'income', 'elements', 'she', 'dc', 'claim', 'bureau', 'pellegrin', 'nature', 'turn', 'pope', 'done', 'century', 'maurizio', 'december', 'code', 'ireland', 'sings', 'basketball', 'japan', 'tiger', 'loss', 'small', 'machines', 'central', 'cross', 'middle', 'toll', 'russia', 'leading', 'oscar', 'italian', 'party', 'literary', 'yellow', 'mountains', 'goodall', 'hollywood', 'jane', 'eggs', 'tale', 'nine', 'written', 'players', 'gamble', 'logan', 'depression', 'plays', 'santa', 'thunder', 'project', 'paint', 'watergate', 'prince', '21', 'perfect', 'buried', 'magazine', 'operating', 'minimum', 'border', 'course', 'events', 'stole', 'languages', 'ohio', 'told', 'aids', '7', 'fuel', 'conference', 'clean', 'religion', 'wage', 'cancer', 'bottle', 'grow', 'brown', '6', 'claimed', 'swimming', 'rivers', 'olympics', 'broadway', 'corpus', 'birthday', 'discovered', 'broken', 'same', 'procter', 'level', 'end', 'classic', 'clothes', 'declared', 'strip', 'am', 'swimmer', '1965', 'ten', 'bee', 'taste', 'pole', 'promote', 'answers', 'also', 'christian', 'site', 'occupation', 'produces', 'louis', 'email', 'independence', 'just', 'purchase', 'jimmy', 'll', 'file', 'reach', 'gay', 'contact', '1939', 'costner', 'journal', 'every', 'receive', 'sleep', 'cat', 'l.a.', 'microsoft', 'jewish', 'published', 'least', 'chicken', 'acted', 'hearing', 'outside', 'clothing', 'blind', 'went', 'stone', 'chicago', 'boy', 'phone', 'continent', 'kevin', 'appeared', '1969', 'investigation', 'pennsylvania', 'tuberculosis', 'tom', 'composer', 'triangle', 'rum', 'award', 'putty', 'ad', 'jersey', 'season', 'criminal', 'fox', 'records', 'qigong', 'young', 'tokyo', 'label', 'beers', 'peace', 'drive', 'claims', 'directed', 'son', 'apples', 'fifth', 'days', 'energy', 'charlie', 'master', 'simpsons', 'cash', 'bay', 'professor', 'appear', 'bone', 'reign', '1980', 'these', 'jesus', 'orange', 'hydrogen', '1994', 'position', 'callosum', 'florence', 'awarded', 'election', 'glass', 'event', 'battery', 'questions', 'against', 'folic', 'correct', 'must', 'iq', 'executed', 'price', 'camp', 'pacific', 'mozambique', 'fish', 'ages', 'face', 'victoria', 'fred', 'raise', 'championship', 'degrees', 'next', 'chemicals', 'poker', 'planted', 'page', 'eleven', 'bob', '1967', 'gas', 'set', 'milk', 'missouri', 'ford', 'cube', 'robert', 'southern', 'museum', 'titanic', 'delaware', 'meant', '1972', 'act', 'any', 'tour', 'sons', 'church', 'answers.com', 'occur', 'developed', 'itself', 'rogers', '12', 'steel', 'cork', 'tube', 'child', 'cells', 'egg', 'club', 'railroad', 'nicknamed', 'mark', 'autobiography', 'jones', 'c', 'november', 'beat', 'order', 'successful', 'mouth', 'greatest', 'army', 'low', 'muppets', 'intercourse', 'k', 'right', 'hold', 'case', 'nadia', 'andrew', 'substance', 'pass', 'buy', 'woodrow', 'australia', 'aspartame', 'upon', 'leave', 'maker', 'members', 'attack', '6th', 'away', 'dickens', 'hearst', 'arctic', 'sells', 'technique', '1963', 'comaneci', 'getting', 'ears', 'bird', 'salt', 'astronaut', '1983', 'wilson', 'formed', 'elected', 'court', 'gate', 'purpose', 'daily', 'o', 'score', 'may', 'roll', 'captain', 'manned', 'justice', 'republic', 'gives', 'china', 'castle', 'exist', 'male', 'question', 'no', 'caribbean', 'ben', 'peak', 'rocky', 'ibm-compatible', 'mercury', 'side', 'weapon', 'pregnancy', 'record', 'holds', 'stage', 'troops', 'dogtown', 'review', 'johnson', 'facial', 'seen', 'tax', 'northernmost', 'town', 'associated', 'design', 'growing', 'labels', 'on-line', 'aaron', 'got', 'started', 'yahoo', 'cocktail', 'describe', 'lead', 'revolution', 'cream', 'conditioning', 'near', 'closest', 'clouds', 'represented', 'windsor', 'want', 'voice', 'comes', 'glitters', 'humans', 'august', 'various', 'buffalo', 'theme', 'cup', 'soap', 'sitcom', 'dakota', 'provider', 'suffer', 'publish', 'wolfe', 'include', '1998', 'fortune', 'einstein', 'profession', 'percent', 'working', 'areas', 'michael', 'contract', 'spice', 'daycare', 'gates', 'behind', 'points', 'royal', 'marvel', 'still', 'monkey', 'vhs', 'rose', 'dry', 'restaurant', 'atlantic', 'then', 'mayor', 'pizza', 'blue', 'ways', 'cigarette', 'flintstones', 'columbus', 'safe', 'commonly', 'husband', 'wash', 'inch', 'community', 'now', 'arma', 'pound', 'left', 'kept', 'quit', 'insurance', 'trip', 'seaport', 'popeye', 'novelist', 'cartoons', 'takes', 'articles', 'arnold', 'sensitive', 'collect', 'louie', 'tower', 'writing', '22', 'wife', 'creator', 'balance', 'browns', 'literature', 'steps', 'mutiny', 'danube', 'pop', 'let', 'success', 'stewart', 'mile', 'federal', 'shows', '19th-century', 'invent', 'detective', 'zealand', 'association', 'stereo', 'mosquito', 'gandhi', 'powerful', 'objects', 'tried', 'heart', 'convicted', 'pro', 'fans', 'fare', 'model', 'mexico', 'nationality', 'society', 'soccer', 'relationship', 'mary', 'sees', 'hate', 'using', 'peanut', 'tony', 'congressman', 'plan', 'turned', 'manufacturer', '16th', 'snow', 'producer', 'horses', 'brazil', 'cucumber', 'broadcasting', 'citizen', 'robinson', 'ride', 'cooler', 'landing', 'syndrome', 'races', 'romans', 'latitude', 'contain', 'rocks', 'contest', 'tutu', 'brain', 'israel', 'expectant', 'sound', 'minor', 'daughters', 'natural', 'committee', 'piggy', 'lee', 'bull', 'tornado', 'winnie', 'sheep', 'define', 'typical', 'nazi', 'lincoln', 'something', 'significant', 'mission', 'venus', 'sperm', 'incredible', 'january', 'keep', 'crop', 'hazmat', 'mr.', 'myrtle', 'allowed', 'attractions', 'statistics', 'hat', 'twice', '13', 'third', 'firm', 'nevada', 'tie', 'normal', 'borders', 'album', 'powell', 'sex', 'going', 'comics', 'un', 'example', 'anniversary', 'martin', 'ends', 'ago', 'mormons', 'fungal', 'calories', 'confederate', 'box', 'planet', 'korea', 'painter', 'sort', 'cosmology', 'minutes', 'bug', 'muffins', 'circle', 'early', 'mouse', 'd.', 'tourist', 'along', 'having', 'celebrities', 'hulk', 'director', '1973', 'generator', 'pounds', 'infection', 'canal', 'sang', 'research', 'network', 'roosevelt', 'skin', 'coach', 'degas', 'boys', 'battles', 'program', 'medium', '1942', 'appearance', 'virgin', 'd.c.', 'sharp', 'longitude', 'road', 'drug', 'root', 'disabilities', 'killer', 'spent', 'nothing', 'abbreviated', 'caffeine', 'thomas', 'lowest', 'race', 'liberty', 'deal', 'michigan', 'italy', 'pull-tab', 'colin', 'reading', 'david', ';', '1960', 'academy', 'bought', 'assassination', 'houses', 'wheel', 'patented', 'northeast', 'release', 'oat', 'source', '1989', 'dictator', '1920s', 'worst', 'trees', '1978', 'don', 'peachy', 'occupy', 'congress', 'florida', 'jockey', 'flies', 'temple', 'pooh', 'fingers', 'regarding', 'several', 'superstar', 'bibliography', 'holy', 'station', 'festival', 'holes', 'georgia', 'closing', 'heavily', 'sink', 'founder', 'playing', 'typewriter', 'feature', 'silver', 'recorded', 'inventor', 'join', 'police', 'claus', 'monet', 'exchange', 'direct', 'count', 'brought', 'al', 'flags', 'companies', 'wars', 'process', 'canyon', 'fein', 'kuwait', 'vietnamese', '1981', 'zip', 'stuart', 'burned', 'killing', 'marx', 'western', 'superman', 'display', 'honorary', '5th', 'wonders', 'camaro', 'alley', 'baking', 'airports', 'wears', 'nuclear', 'began', 'bears', 'owns', 'chronic', 'native', 'highway', 'pressure', 'madonna', 'lovers', 'nicholas', 'prophet', 'seas', 'handheld', 'scholar', 'pitcher', 'background', 'muhammad', 'sinn', 'l.', 'paso', 'yankees', 'clip', 'bars', 'birthstone', 'responsible', 'coffee', 'pink', 'lady', 'geese', 'dam', 'landed', 'less', 'caffeinated', 'fatal', 'arch', 'railway', 'canadian', 'medicine', 'held', 'automobile', 'maiden', 'addresses', 'instead', 'close', 'easter', 'trial', 'match', 'increase', 'motors', 'eruption', '1797-185', 'cocaine', 'adult', 'follow', 'beauty', 'aldrin', 'marked', 'colored', 'our', 'stands', 'correctly', 'fresh', 'oz', 'could', 'iron', 'beethoven', 'werewolf', 'teenager', 'chairman', 'laugh-in', 'attend', 'adventures', 'goddess', 'attacks', '24', 'individuals', 'kids', 'eternity', 'doonesbury', 'andy', 'mccarren', 'log', 'mao', 'stamp', 'beaver', 'hockey', 'virginia', 'angels', 'force', 'fired', 'wines', 'enough', 'vbp', 'large', 'windows', 'serving', 'catholic', 'bar', 'maid-rites', 'ring', 'physical', 'dew', 'iii', 'electricity', 'terrorist', 'athlete', 'harry', 'nazis', 'inches', 'christopher', 'july', 'affect', 'presidency', 'cage', 'seized', 'resource', 'database', 'cars', 'rubik', 'stopped', 'gulf', 'toy', 'brooks', 'murder', '49', 'saint', 'virtual', 'sexual', 'prewitt', 'vacuum', 'explorer', 'bees', 'ads', 'command', 'bet', 'led', 'bastille', 'likely', 'champions', 'seattle', 'future', 'markets', 'included', 'happen', 'scrabble', 'apollo', 'rubber', 'month', 'bombay', 'signature', 'sing', 'engine', 'italians', 'hitting', 'wonder', 'asked', 'calculator', 'saturday', 'millennium', 'here', 'nns', 'official', 'lord', 'fictional', 'disc', 'boat', 'enter', 'variety', 'dealt', 'hugo', 'illinois', 'mammal', 'kansas', 'cairo', 'j.', 'bronze', 'issue', 'established', 'era', 'statue', 'universe', 'making', 'shuttle', 'owned', 'gene', 'heavier', 'deer', 'bush', 'develop', 'j.r.r.', 'razor', 'protein', 'williams', 'rome', 'anthony', 'barbados', 'involved', 'chapter', 'april', 'hurricane', 'private', 'bert', 'suit', 'nino', 'hardest', 'whitcomb', 'caesar', 'equator', 'f', 'gogh', 'plo', 'sodium', 'jerry', 'garry', 'met', 'poe', 'scarlett', 'presidents', 'morning', 'sale', 'equipment', 'singles', 'carter', 'gods', 'spain', 'tolkien', 'flavor', 'aged', 'grand', 'ip', 'establish', 'artists', 'havoc', 'covers', 'carolina', '1953', 'wings', 'sides', 'owner', 'ancient', 'floor', 'trinidad', 'layers', 'fourth', 'franklin', 'value', 'spelling', 'mystery', 'ranger', 'monarch', 'pairs', 'raid', 'period', 'ended', 'bacall', 'register', 'wyoming', 'plants', 'debut', 'suite', 'possession', 'range', 'performer', 'marley', 'learning', 'scoundrel', 'well', 'attempts', 'tennessee', 'released', 'ioc', 'ticket', 'surname', 'housewife', 'total', 'animated', 'francisco', 'yogi', 'vermouth', 'dollar', 'caliente', 'injection', 'mckinley', 'tribe', 'territory', 'alphabet', 'n', 'elephant', 'paul', 'worked', 'according', 'judson', 'keyboard', '<bos>', '<eos>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seeding_random()\n",
    "\n",
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "data = []\n",
    "for line in file.readlines():\n",
    "    data.append(line)\n",
    "train_data = data[:round(len(data)*devLine)]\n",
    "dev_data = data[round(len(data)*devLine):]\n",
    "\n",
    "print(\"Création training set...\")\n",
    "training_set = QuestionDataset(train_data, nb_input-3)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création dev set...\")\n",
    "dev_set = QuestionDataset(dev_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création test set...\")\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)\n",
    "test_set = QuestionDataset(test_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "# Création du DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "seeding_random()\n",
    "\n",
    "train_loader = dataloader.DataLoader(training_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "dataloader_args_notshuffle = dict(shuffle=False, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "\n",
    "dev_loader = dataloader.DataLoader(dev_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "test_loader = dataloader.DataLoader(test_set, **dataloader_args_notshuffle)\n",
    "seeding_random()\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"List of word used:\")\n",
    "print(training_set.word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition per classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "    classes = [0,0,0,0,0,0]\n",
    "    for data, target in train_loader:\n",
    "        for t in list(target):\n",
    "            t = t.item()\n",
    "            classes[t] += 1\n",
    "\n",
    "    total = sum(classes)\n",
    "    rep_classes = [c/total*100 for c in classes]\n",
    "    print(\"Répartitions des données dans les classes:\")\n",
    "    for i in range(len(rep_classes)):\n",
    "        print(\"Classe numéro \" + str(i+1) + \": \" + str(rep_classes[i]) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word occurence repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "\n",
    "    word_occ = training_set.reparti_word\n",
    "    word_occ = dict(word_occ)\n",
    "    \n",
    "    total = sum([value for key, value in training_set.reparti_word.most_common(len(training_set.reparti_word))])\n",
    "    \n",
    "    values = [sum([value for key, value in training_set.reparti_word.most_common(i+1)])/total*100 for i in range(len(training_set.reparti_word))]\n",
    "\n",
    "    x = np.linspace(0, len(values), len(values))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    cnn_line, = ax.plot(x, values)\n",
    "\n",
    "    ax.set(xlabel=\"Vocabulaire unique\", ylabel=\"Couverture en %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0., batch_first=True, nonlinearity='relu')\n",
    "        self.inter = nn.Linear(nb_neurons, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = torch.zeros(self.nb_layers, inputs.size(0), self.nb_neurons)\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        x, hn = self.rnn(inputs, h0)\n",
    "        \n",
    "        x = self.inter(hn[0])\n",
    "        #print(x)\n",
    "        #x = tensor([list(i[-1]) for i in x])\n",
    "        #print(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "#TODO\n",
    "#Entropy mean might be near to zero\n",
    "def getEntropies(rnn, batch_list):\n",
    "    entropy_list = []\n",
    "    #value, counts = np.unique(out, return_counts=True)\n",
    "    #entropy_list.append(entropy(out, base=None))\n",
    "    return [-1]\n",
    "\n",
    "\n",
    "# return correct_percent\n",
    "def getEfficience(rnn, batch_list) :\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        out = rnn(data).data\n",
    "        \n",
    "        _, predicted = torch.max(out.data, dim=1)\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return total_correct / total\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, data_loader, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_rnn = rnn\n",
    "    max_acc_dev = -1\n",
    "    pos_best_rnn = 0;\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_correct = 0\n",
    "        total_target = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            #rnn.train()\n",
    "            seeding_random()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            rnn.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Get the Accuracy\n",
    "            \n",
    "            _, predicted = torch.max(output.data, dim=1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total_correct += correct\n",
    "            total_target += target.size(0)\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 500 == 0 or batch_idx % 500 == 1 or batch_idx == len(data_loader)-1:\n",
    "                print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}\\t Accuracy: {}'.format(\n",
    "                    epoch+1,\n",
    "                    num_epochs,\n",
    "                    batch_idx * len(data), \n",
    "                    len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader), \n",
    "                    loss.data.item(),\n",
    "                    (total_correct / total_target) * 100),\n",
    "                    end='')\n",
    "                losses_train.append(loss.data.item())\n",
    "                if great_analysis:\n",
    "                    dev_data, dev_target = next(iter(dev_loader))\n",
    "                    dev_data, dev_target = dev_data.to(device), dev_target.to(device)\n",
    "                    output = rnn(dev_data)\n",
    "                    loss = criterion(output, dev_target)\n",
    "                    losses_dev.append(loss.data.item())\n",
    "                    \n",
    "                    \n",
    "        print()\n",
    "        acc_dev = getEfficience(rnn, dev_loader)*100\n",
    "        if acc_dev > max_acc_dev:\n",
    "            max_acc_dev = acc_dev\n",
    "            best_rnn = rnn\n",
    "            pos_best_rnn = epoch\n",
    "        \n",
    "        print(\"Dev set: accuracy: \" + str(acc_dev) + \"% | max acc: \" + str(max_acc_dev)+\"%\")\n",
    "        print()\n",
    "    rnn = best_rnn\n",
    "    # Return losses list, you can print them later if you want\n",
    "    return {\"losses_train\":losses_train, \"losses_dev\":losses_dev, \"pos_best\":pos_best_rnn+1, \"best_rnn\":best_rnn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 1/60 [10800/10816 (100%)]\t Loss: 1.427401\t Accuracy: 37.15791420118343\n",
      "Dev set: accuracy: 54.939603106125965% | max acc: 54.939603106125965%\n",
      "\n",
      " Train Epoch: 2/60 [10800/10816 (100%)]\t Loss: 1.229449\t Accuracy: 66.61427514792899\n",
      "Dev set: accuracy: 66.1130284728214% | max acc: 66.1130284728214%\n",
      "\n",
      " Train Epoch: 3/60 [10800/10816 (100%)]\t Loss: 1.527411\t Accuracy: 66.47559171597634\n",
      "Dev set: accuracy: 59.51251078515962% | max acc: 66.1130284728214%\n",
      "\n",
      " Train Epoch: 4/60 [10800/10816 (100%)]\t Loss: 1.392637\t Accuracy: 73.45599112426035\n",
      "Dev set: accuracy: 72.49784296807593% | max acc: 72.49784296807593%\n",
      "\n",
      " Train Epoch: 5/60 [10800/10816 (100%)]\t Loss: 1.147794\t Accuracy: 81.05584319526628\n",
      "Dev set: accuracy: 73.51164797239% | max acc: 73.51164797239%\n",
      "\n",
      " Train Epoch: 6/60 [10800/10816 (100%)]\t Loss: 1.099725\t Accuracy: 81.98039940828401\n",
      "Dev set: accuracy: 66.78170836928386% | max acc: 73.51164797239%\n",
      "\n",
      " Train Epoch: 7/60 [10800/10816 (100%)]\t Loss: 1.172981\t Accuracy: 85.56767751479289\n",
      "Dev set: accuracy: 78.53753235547886% | max acc: 78.53753235547886%\n",
      "\n",
      " Train Epoch: 8/60 [10800/10816 (100%)]\t Loss: 1.044481\t Accuracy: 88.11020710059172\n",
      "Dev set: accuracy: 81.98878343399483% | max acc: 81.98878343399483%\n",
      "\n",
      " Train Epoch: 9/60 [10800/10816 (100%)]\t Loss: 1.044808\t Accuracy: 90.15347633136095\n",
      "Dev set: accuracy: 83.30457290767903% | max acc: 83.30457290767903%\n",
      "\n",
      " Train Epoch: 10/60 [10800/10816 (100%)]\t Loss: 1.043986\t Accuracy: 87.71264792899409\n",
      "Dev set: accuracy: 83.08886971527178% | max acc: 83.30457290767903%\n",
      "\n",
      " Train Epoch: 11/60 [10800/10816 (100%)]\t Loss: 1.047200\t Accuracy: 90.93010355029585\n",
      "Dev set: accuracy: 83.11044003451251% | max acc: 83.30457290767903%\n",
      "\n",
      " Train Epoch: 12/60 [10800/10816 (100%)]\t Loss: 1.053467\t Accuracy: 81.05584319526628\n",
      "Dev set: accuracy: 76.46678170836928% | max acc: 83.30457290767903%\n",
      "\n",
      " Train Epoch: 13/60 [10800/10816 (100%)]\t Loss: 1.046308\t Accuracy: 88.9330621301775\n",
      "Dev set: accuracy: 82.70060396893874% | max acc: 83.30457290767903%\n",
      "\n",
      " Train Epoch: 14/60 [10800/10816 (100%)]\t Loss: 1.052392\t Accuracy: 83.31176035502959\n",
      "Dev set: accuracy: 80.6729939603106% | max acc: 83.30457290767903%\n",
      "\n",
      " Train Epoch: 15/60 [10800/10816 (100%)]\t Loss: 1.057460\t Accuracy: 89.57100591715977\n",
      "Dev set: accuracy: 82.95944779982743% | max acc: 83.30457290767903%\n",
      "\n",
      " Train Epoch: 16/60 [10800/10816 (100%)]\t Loss: 1.045637\t Accuracy: 91.9008875739645\n",
      "Dev set: accuracy: 83.9732528041415% | max acc: 83.9732528041415%\n",
      "\n",
      " Train Epoch: 17/60 [10800/10816 (100%)]\t Loss: 1.046923\t Accuracy: 91.54955621301775\n",
      "Dev set: accuracy: 84.16738567730803% | max acc: 84.16738567730803%\n",
      "\n",
      " Train Epoch: 18/60 [10800/10816 (100%)]\t Loss: 1.044182\t Accuracy: 92.98261834319527\n",
      "Dev set: accuracy: 84.8360655737705% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 19/60 [10800/10816 (100%)]\t Loss: 1.045078\t Accuracy: 89.41383136094674\n",
      "Dev set: accuracy: 83.80069025021571% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 20/60 [10800/10816 (100%)]\t Loss: 1.043943\t Accuracy: 92.36316568047337\n",
      "Dev set: accuracy: 84.36151855047454% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 21/60 [10800/10816 (100%)]\t Loss: 1.044308\t Accuracy: 93.47263313609467\n",
      "Dev set: accuracy: 84.57722174288179% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 22/60 [10800/10816 (100%)]\t Loss: 1.044167\t Accuracy: 93.53735207100591\n",
      "Dev set: accuracy: 84.68507333908542% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 23/60 [10800/10816 (100%)]\t Loss: 1.351975\t Accuracy: 87.31508875739645\n",
      "Dev set: accuracy: 79.40034512510785% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 24/60 [10800/10816 (100%)]\t Loss: 1.298560\t Accuracy: 87.76812130177515\n",
      "Dev set: accuracy: 81.08283002588438% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 25/60 [10800/10816 (100%)]\t Loss: 1.217912\t Accuracy: 90.04252958579882\n",
      "Dev set: accuracy: 82.37704918032787% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 26/60 [10800/10816 (100%)]\t Loss: 1.233858\t Accuracy: 92.33542899408283\n",
      "Dev set: accuracy: 81.94564279551338% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 27/60 [10800/10816 (100%)]\t Loss: 1.164469\t Accuracy: 93.20451183431953\n",
      "Dev set: accuracy: 84.8360655737705% | max acc: 84.8360655737705%\n",
      "\n",
      " Train Epoch: 28/60 [10800/10816 (100%)]\t Loss: 1.043710\t Accuracy: 94.13831360946746\n",
      "Dev set: accuracy: 85.9792924935289% | max acc: 85.9792924935289%\n",
      "\n",
      " Train Epoch: 29/60 [10800/10816 (100%)]\t Loss: 1.099215\t Accuracy: 92.82544378698225\n",
      "Dev set: accuracy: 84.98705780845557% | max acc: 85.9792924935289%\n",
      "\n",
      " Train Epoch: 30/60 [10800/10816 (100%)]\t Loss: 1.090531\t Accuracy: 94.62832840236686\n",
      "Dev set: accuracy: 86.19499568593615% | max acc: 86.19499568593615%\n",
      "\n",
      " Train Epoch: 31/60 [10800/10816 (100%)]\t Loss: 1.043749\t Accuracy: 95.04437869822485\n",
      "Dev set: accuracy: 84.1458153580673% | max acc: 86.19499568593615%\n",
      "\n",
      " Train Epoch: 32/60 [10800/10816 (100%)]\t Loss: 1.043779\t Accuracy: 95.28476331360946\n",
      "Dev set: accuracy: 86.77739430543572% | max acc: 86.77739430543572%\n",
      "\n",
      " Train Epoch: 33/60 [10800/10816 (100%)]\t Loss: 1.121137\t Accuracy: 95.1090976331361\n",
      "Dev set: accuracy: 81.06125970664367% | max acc: 86.77739430543572%\n",
      "\n",
      " Train Epoch: 34/60 [10800/10816 (100%)]\t Loss: 1.167643\t Accuracy: 93.02884615384616\n",
      "Dev set: accuracy: 80.93183779119931% | max acc: 86.77739430543572%\n",
      "\n",
      " Train Epoch: 35/60 [10800/10816 (100%)]\t Loss: 1.043711\t Accuracy: 93.44489644970415\n",
      "Dev set: accuracy: 86.99309749784297% | max acc: 86.99309749784297%\n",
      "\n",
      " Train Epoch: 36/60 [10800/10816 (100%)]\t Loss: 1.043959\t Accuracy: 95.88572485207101\n",
      "Dev set: accuracy: 86.755823986195% | max acc: 86.99309749784297%\n",
      "\n",
      " Train Epoch: 37/60 [10800/10816 (100%)]\t Loss: 1.043647\t Accuracy: 95.9319526627219\n",
      "Dev set: accuracy: 87.68334771354615% | max acc: 87.68334771354615%\n",
      "\n",
      " Train Epoch: 38/60 [10800/10816 (100%)]\t Loss: 1.043663\t Accuracy: 96.34800295857988\n",
      "Dev set: accuracy: 87.89905090595342% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 39/60 [10800/10816 (100%)]\t Loss: 1.043771\t Accuracy: 94.84097633136095\n",
      "Dev set: accuracy: 86.45383951682484% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 40/60 [10800/10816 (100%)]\t Loss: 1.043667\t Accuracy: 94.05510355029585\n",
      "Dev set: accuracy: 86.08714408973253% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 41/60 [10800/10816 (100%)]\t Loss: 1.043625\t Accuracy: 96.14460059171599\n",
      "Dev set: accuracy: 87.64020707506471% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 42/60 [10800/10816 (100%)]\t Loss: 1.043616\t Accuracy: 96.37573964497041\n",
      "Dev set: accuracy: 87.61863675582399% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 43/60 [10800/10816 (100%)]\t Loss: 1.103749\t Accuracy: 96.16309171597634\n",
      "Dev set: accuracy: 86.82053494391717% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 44/60 [10800/10816 (100%)]\t Loss: 1.043651\t Accuracy: 96.18158284023669\n",
      "Dev set: accuracy: 87.53235547886109% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 45/60 [10800/10816 (100%)]\t Loss: 1.043669\t Accuracy: 96.37573964497041\n",
      "Dev set: accuracy: 87.27351164797238% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 46/60 [10800/10816 (100%)]\t Loss: 1.043609\t Accuracy: 96.20007396449705\n",
      "Dev set: accuracy: 87.51078515962037% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 47/60 [10800/10816 (100%)]\t Loss: 1.043597\t Accuracy: 95.73779585798816\n",
      "Dev set: accuracy: 87.74805867126834% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 48/60 [10800/10816 (100%)]\t Loss: 1.043632\t Accuracy: 95.63609467455622\n",
      "Dev set: accuracy: 87.66177739430545% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 49/60 [10800/10816 (100%)]\t Loss: 1.051901\t Accuracy: 96.44045857988166\n",
      "Dev set: accuracy: 87.55392579810182% | max acc: 87.89905090595342%\n",
      "\n",
      " Train Epoch: 50/60 [10800/10816 (100%)]\t Loss: 1.048034\t Accuracy: 96.55140532544378\n",
      "Dev set: accuracy: 87.96376186367559% | max acc: 87.96376186367559%\n",
      "\n",
      " Train Epoch: 51/60 [10800/10816 (100%)]\t Loss: 1.044882\t Accuracy: 96.7917899408284\n",
      "Dev set: accuracy: 88.05004314063848% | max acc: 88.05004314063848%\n",
      "\n",
      " Train Epoch: 52/60 [10800/10816 (100%)]\t Loss: 1.043601\t Accuracy: 96.23705621301775\n",
      "Dev set: accuracy: 88.11475409836066% | max acc: 88.11475409836066%\n",
      "\n",
      " Train Epoch: 53/60 [10800/10816 (100%)]\t Loss: 1.049074\t Accuracy: 96.20931952662723\n",
      "Dev set: accuracy: 87.51078515962037% | max acc: 88.11475409836066%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 54/60 [10800/10816 (100%)]\t Loss: 1.043596\t Accuracy: 95.48816568047337\n",
      "Dev set: accuracy: 87.01466781708369% | max acc: 88.11475409836066%\n",
      "\n",
      " Train Epoch: 55/60 [10800/10816 (100%)]\t Loss: 1.044889\t Accuracy: 91.6974852071006\n",
      "Dev set: accuracy: 85.80672993960312% | max acc: 88.11475409836066%\n",
      "\n",
      " Train Epoch: 56/60 [10800/10816 (100%)]\t Loss: 1.043762\t Accuracy: 95.46967455621301\n",
      "Dev set: accuracy: 86.4969801553063% | max acc: 88.11475409836066%\n",
      "\n",
      " Train Epoch: 57/60 [10800/10816 (100%)]\t Loss: 1.043599\t Accuracy: 96.19082840236686\n",
      "Dev set: accuracy: 87.23037100949094% | max acc: 88.11475409836066%\n",
      "\n",
      " Train Epoch: 58/60 [10800/10816 (100%)]\t Loss: 1.087368\t Accuracy: 96.48668639053254\n",
      "Dev set: accuracy: 86.08714408973253% | max acc: 88.11475409836066%\n",
      "\n",
      " Train Epoch: 59/60 [10800/10816 (100%)]\t Loss: 1.043617\t Accuracy: 96.28328402366864\n",
      "Dev set: accuracy: 87.98533218291631% | max acc: 88.11475409836066%\n",
      "\n",
      " Train Epoch: 60/60 [10800/10816 (100%)]\t Loss: 1.043603\t Accuracy: 96.47744082840237\n",
      "Dev set: accuracy: 87.61863675582399% | max acc: 88.11475409836066%\n",
      "\n",
      "Done :)\n",
      "Learned in 0:39:10.749277\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "rnn = RNN(nb_inputs = len(training_set.word_list), nb_layers=nb_hidd_lay,\n",
    "          nb_neurons=hidden_size, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "with torch.enable_grad():\n",
    "    job = learn(rnn, train_loader, nb_epochs)\n",
    "    losses_train = job[\"losses_train\"]\n",
    "    losses_dev = job[\"losses_dev\"]\n",
    "    pos_best_rnn = job[\"pos_best\"]\n",
    "    rnn = job[\"best_rnn\"]\n",
    "    print(\"Done :)\")\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Learned in \" + str(end_time - begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f55694a18e4d3d869cba3412cdbdf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='smooth', max=500, min=5, step=2), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def update_losses(smooth=1):\n",
    "    x_train = np.linspace(0, len(losses_train), len(losses_train))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax_train = fig.add_subplot(1,1,1)\n",
    "    cnn_line_train, = ax_train.plot(x_train, losses_train)\n",
    "    cnn_line_train.set_ydata(savgol_filter(losses_train, smooth, 3))\n",
    "    \n",
    "    if great_analysis:\n",
    "        x_dev = np.linspace(0, len(losses_dev), len(losses_dev))\n",
    "        ax_dev = fig.add_subplot(1,1,1)\n",
    "        cnn_line_dev, = ax_dev.plot(x_dev, losses_dev)\n",
    "        cnn_line_dev.set_ydata(savgol_filter(losses_dev, smooth, 3))\n",
    "    \n",
    "interact(update_losses, smooth=(5, 500, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n",
      "On the training set:\n",
      "Corrects: 96.50517751479289%\n",
      "\n",
      "On the dev set:\n",
      "Corrects: 87.61863675582399%\n",
      "\n",
      "On the test set:\n",
      "Moyenne des entropies: -1\n",
      "Corrects: 81.39999999999999%\n",
      "\n",
      "A présent, tu peux copier-coller ça dans le doc sur le drive :)\n",
      "1700\t0.001\t60\t3\t42\t16\t\t-1\t52\t96.50517751479289%\t87.61863675582399%\t81.39999999999999%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Congratulations!\")\n",
    "\n",
    "rnn.eval()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_train = getEfficience(rnn, train_loader)*100\n",
    "\n",
    "print(\"On the training set:\")\n",
    "print(\"Corrects: \" + str(correct_train) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_dev = getEfficience(rnn, dev_loader)*100\n",
    "\n",
    "print(\"On the dev set:\")\n",
    "print(\"Corrects: \" + str(correct_dev) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_test = getEfficience(rnn, test_loader)*100\n",
    "\n",
    "mean_entropies = -1\n",
    "\n",
    "print(\"On the test set:\")\n",
    "print(\"Moyenne des entropies: \" + str(mean_entropies))\n",
    "print(\"Corrects: \" + str(correct_test) + \"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "inputs = nb_input\n",
    "if inputs == -1:\n",
    "    inputs = len(training_set.word_list)-3\n",
    "\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(inputs)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(nb_hidd_lay)\n",
    "      +\"\\t\"+str(hidden_size)+\"\\t\"+str(nb_batchs)+\"\\t\\t\"+str(mean_entropies)+\"\\t\"+str(pos_best_rnn)\n",
    "      +\"\\t\"+str(correct_train)+\"%\\t\"+str(correct_dev)+\"%\\t\"+str(correct_test)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
