{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 1700 # write -1 if you want every words\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "LAYERS = [10, 10, 10, 10, 10, 10, 10, 10]\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 60\n",
    "\n",
    "# Random seed, don't change it if you don't know what it is\n",
    "random_seed = 42\n",
    "\n",
    "nb_batchs = 16\n",
    "\n",
    "# How many percent of your data do you use as training set\n",
    "devLine = 0.7\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If your goal is to draw graphs\n",
    "great_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding_random():\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    \n",
    "    # Special constructor\n",
    "    # | nb_most_commons can either be the number of most common words you\n",
    "    # | want to work with, OR a list of word you want to work with\n",
    "    # If nb_most_commons == -1, then all word will count\n",
    "    \n",
    "    def __init__(self, train_data, nb_most_commons=-1):\n",
    "        questions = []\n",
    "        labels = []\n",
    "\n",
    "        # Black list\n",
    "        black_list = '\\'`[@_!#$%^&*()<>?/\\|}{~:]'\n",
    "        \n",
    "        for string in train_data:\n",
    "            question_str = []\n",
    "            for x in string.split()[1:]:\n",
    "                s = \"\"\n",
    "                for c in x:\n",
    "                    if not c in black_list:\n",
    "                        s += c\n",
    "                if not s == \"\":\n",
    "                    question_str.append(s.lower())\n",
    "                        \n",
    "            labels.append(string.split()[0])\n",
    "            questions.append(question_str)\n",
    "\n",
    "        \n",
    "        if isinstance(nb_most_commons, int):\n",
    "            # Vocabulary of unique words\n",
    "            data = []\n",
    "            for q in questions:\n",
    "                for w in q:\n",
    "                    data.append(w)\n",
    "            self.reparti_word = Counter(data)\n",
    "            \n",
    "            if nb_most_commons < 0:\n",
    "                most_commons_words = self.reparti_word.most_common(len(data))\n",
    "            else:\n",
    "                most_commons_words = self.reparti_word.most_common(nb_most_commons)\n",
    "            \n",
    "            self.word_list = list([x[0] for x in most_commons_words])\n",
    "            self.word_list.append('<bos>')\n",
    "            self.word_list.append('<eos>')\n",
    "            self.word_list.append('<unk>')\n",
    "        elif isinstance(nb_most_commons, list):\n",
    "            self.word_list = nb_most_commons\n",
    "        else:\n",
    "            print(\"ERROR: second arg is neither an int, nor a list\")\n",
    "            \n",
    "        words_array = np.array(self.word_list)\n",
    "        \n",
    "        # Add tags <bos> and <eos> to questions\n",
    "        for q in questions:\n",
    "            if q[0] != '<bos>' :\n",
    "                q.insert(0, '<bos>')\n",
    "                q.append('<eos>')\n",
    "\n",
    "        # Integer encoding with OneHotEncoder\n",
    "        words_tre = words_array.reshape(len(words_array),1)\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "        # Creating a dictionnary of word and its one hot array\n",
    "        self.words_onehoted = {}\n",
    "        for i in range(0, len(words_array)):\n",
    "            self.words_onehoted[self.word_list[i]] = onehot_encoded[i]\n",
    "\n",
    "        # One hot categories\n",
    "        self.categories_num = {}\n",
    "        self.categories_num['ABBR'] = 0 # Abbreviation\n",
    "        self.categories_num['ENTY'] = 1 # Entity\n",
    "        self.categories_num['DESC'] = 2 # Description\n",
    "        self.categories_num['HUM']  = 3 # Human\n",
    "        self.categories_num['LOC']  = 4 # Location\n",
    "        self.categories_num['NUM']  = 5 # Numeric\n",
    "\n",
    "        self.batch_data = []\n",
    "        for num_question in range(len(questions)):\n",
    "            # Construction of question_onehot list.\n",
    "            question_onehot = [self.get_onehot_word(word) for word in questions[num_question]]\n",
    "\n",
    "            # Construction of category_onehot.\n",
    "            category = labels[num_question].partition(':')[0]\n",
    "            category_onehot = self.get_num_category(category)\n",
    "            self.batch_data.append([(question_onehot), (category_onehot)])\n",
    "        \n",
    "    \n",
    "    # Function to get the corresponding one hot list for a category.\n",
    "    def get_num_category(self, category):\n",
    "        return self.categories_num[category]\n",
    "\n",
    "\n",
    "    # Function to get the corresponding one hot list for a word.\n",
    "    def get_onehot_word(self, word):\n",
    "        if word in self.words_onehoted:\n",
    "            return list(self.words_onehoted[word])\n",
    "        else:\n",
    "            return list(self.words_onehoted['<unk>'])\n",
    "\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.batch_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeding_random()\n",
    "        return self.batch_data[idx]\n",
    "    \n",
    "def pad_collate(batch):\n",
    "    max_length = max([len(q[0]) for q in batch])\n",
    "\n",
    "    inputs = torch.FloatTensor([[[0. for _ in range(len(x[0][0]))] for i in range(max_length-len(x[0]))]+x[0] for x in batch])\n",
    "    outputs = torch.LongTensor([x[1] for x in batch])\n",
    "    \n",
    "    return inputs, outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création training set...\n",
      "Done!\n",
      "Création dev set...\n",
      "Done!\n",
      "Création test set...\n",
      "Done!\n",
      "List of word used:\n",
      "['the', 'what', 'is', 'of', 'in', 'a', 'how', 's', 'was', 'who', 'to', ',', 'are', 'for', 'and', 'did', 'does', 'do', 'name', 'on', 'many', 'where', 'i', 'you', 'can', 'first', 'when', 'from', 'which', 'world', 'that', 'city', 'as', 'with', 'country', 'has', 'most', '.', 'u.s.', 'by', 'an', 'have', 'find', 'it', 'why', 'there', 'get', 'people', 'called', 'state', 'year', 'were', 'mean', 'be', 'american', 'president', 'largest', 'his', 'fear', 'two', 'at', 'war', 'new', 'its', 'origin', 'word', 'much', 'about', 'known', 'kind', 'between', 'company', 'game', 'film', 'long', 'day', 'movie', 'live', 'made', 'your', 'or', 'take', 'only', 'stand', 'man', 'best', 'book', 'tv', 'their', 'one', 'john', 'famous', 'color', 'all', 'star', 'show', 'he', 'term', 'used', 'my', 'out', 'come', 'play', 'baseball', 'invented', 'had', 'into', 'call', 'number', 'countries', 'make', 'home', 'america', 'time', 'old', 'dog', 'character', 'team', 'actor', 'not', 'nn', 'three', 'information', 'river', 'average', 'states', 'south', 'highest', 'some', 'after', 'names', 'write', 'body', 'born', 'if', 'use', 'difference', 'years', 'university', 'up', 'won', 'said', 'during', 'played', 'killed', 'say', 'us', 'four', 'black', 'work', 'died', 'english', 'united', 'become', 'would', '1', 'song', 'novel', 'computer', 'earth', 'meaning', 'wrote', 'good', 'car', 'named', 'last', 'water', 'located', 'space', 'go', 'school', 'different', 'longest', 'drink', 'common', 'sport', 'horse', 'animal', 'woman', 'place', 'will', 'king', 'been', 'battle', 'her', 'mountain', 'makes', 'me', 'north', 'over', 'date', 'mississippi', 'way', 'system', 'top', 'contains', 'address', 'national', 'actress', 'russian', 'times', 't', 'island', 'nickname', 'popular', 'college', 'internet', 'population', 'form', 'served', 'causes', 'little', 'capital', 'created', 'abbreviation', 'portrayed', 'history', 'money', 'they', 'law', 'whose', 'part', 'product', 'west', 'craft', 'through', 'animals', 'death', 'language', 'than', 'queen', 'bowl', 'randy', 'group', 'became', 'mother', 'being', 'die', 'great', 'should', 'person', 'food', 'biggest', 'california', 'like', 'other', 'office', 'air', 'life', 'start', 'ball', 'york', 'cities', 'james', 'married', 'french', 'spanish', 'england', '2', 'european', 'william', 'following', 'before', 'cnn', 'each', 'definition', 'games', 'general', 'soft', 'bill', 'musical', 'boasts', 'cost', 'feet', 'win', 'happened', 'line', 'kennedy', 'fastest', 'more', 'international', 'leader', 'children', 'lives', 'eat', 'love', 'f.', 'building', 'cards', 'big', 'san', 'comic', 'super', 'letter', 'charles', 'moon', 'title', 'airport', '-', 'features', 'disease', 'percentage', 'board', 'baby', 'instrument', 'series', 'canada', 'female', 'british', 'germany', 'prize', 'author', 'begin', 'second', 'center', 'power', 'white', 'indians', 'type', 'nnp', 'business', 'caused', 'night', 'another', 'indian', 'japanese', 'real', 'sometimes', 'bear', 'symbol', 'seven', 'fire', 'age', 'men', 'ever', 'olympic', 'sea', 'nixon', 'park', 'saw', 'jaws', 'newspaper', 'lawyer', 'latin', 'television', 'stop', 'desert', 'organization', 'hitler', 'london', 'element', 'travel', 'colors', 'miles', 'someone', 'found', 'far', 'st.', 'role', 'house', 'give', 'bridge', 'blood', 'former', 'county', 'street', 'richard', 'oldest', 'believe', 'under', 'security', 'music', 'website', 'hit', 'soviet', 'list', 'green', 'nobel', 'member', 'five', 'starred', 'original', 'need', 'lake', 'million', 'store', 'soldiers', 'greek', 'art', 'africa', 'ocean', 'singing', 'boxing', 'once', 'hair', 'words', 'golf', 'light', 'founded', 'introduced', 'el', 'george', 'rate', 'washington', 'them', 'titled', 'bible', 'flight', 'nt', 'birds', 'de', 'we', 'cartoon', 'six', 'back', 'runs', 'monopoly', 'types', 'march', 'built', 'shea', 'gould', 'human', '0', 'months', 'produced', 'area', 'sioux', 'union', 'dubbed', 'christmas', 'clock', 'flag', 'never', 'jack', 'cold', 'birth', 'islands', 'prime', 'point', 'miss', 'civil', 'full', 'france', 'fame', 'thing', 'gold', '5', 'fast', 'red', 'fought', 'know', 'texas', 'look', 'tallest', 'minister', 'asian', 'education', 'size', 'but', 'so', 'cover', '1984', 'ship', 'roman', 'chinese', 'web', 'east', 'eyes', 'ray', 'major', '1991', 'sports', 'originate', 'whom', 'paid', 'rain', 'women', 'spumante', 'americans', 'kentucky', 'peter', 'shot', 'dead', 'tree', 'thatcher', 'worth', 'received', 'fly', 'jude', 'stars', 'painted', 'market', 'hero', 'oil', 'main', 'tell', 'department', 'came', 'headquarters', 'player', 'paper', 'dick', 'sun', 'amendment', 'beach', 'host', 'assassinated', 'historical', 'wear', 'girl', 'eye', 'daughter', 'stock', 'films', 'produce', 'commercial', 'favorite', 'schools', 'chemical', 'standard', 'reims', 'setting', 'hole', 'government', 'celebrated', 'painting', 'wine', 'kid', 'see', 'motto', 'league', 'watch', 'pearl', 'songs', 'empire', 'based', 'diego', 'products', 'remove', 'reason', 'chocolate', 'god', 'comedian', 'poem', 'off', 'father', 'social', 'ice', 'speed', 'shakespeare', 'medical', 'electric', 'characters', 'sign', 'wall', 'writer', 'own', 'this', 'dt', 'los', 'around', 'considered', '1899', 'berlin', 'lived', 'presidential', 'tennis', 'patent', 'forest', 'species', 'current', 'hands', 'bowling', 'run', 'rights', 'desmond', 'classical', 'winter', 'henry', 'emperor', 'students', 'career', 'lyrics', 'constitution', '3', '11', 'acid', 'cd', 'serve', 'weight', 'harbor', 'beatles', 'angeles', 'ii', 'opera', 'down', 'la', 'books', 'balls', 'free', 'head', 'put', 'often', 'inspired', 'singer', 'steven', 'jean', 'massachusetts', 'video', 'card', 'without', 'numbers', 'mrs.', 'brothers', 'build', 'artist', 'followed', 'vietnam', 'vegas', 'governor', 'family', 'bomb', 'sold', 'jackson', 'while', 'johnny', 'irish', 'bond', 'mount', 'council', 'nations', 'always', 'e-mail', 'square', 'spy', 'oceans', 'starring', '8', 'creature', 'rock', 'featured', 'bounty', 'india', 'poet', 'rascals', 'rule', 'week', 'function', 'famed', 'spoken', 'secretary', 'band', 'create', '10', 'spielberg', 'beer', 'magic', 'story', 'export', 'hand', 'high', 'letters', 'equal', 'took', 'because', 'cowboy', 'lakes', 'december', 'alaska', 'golfer', 'century', 'tiger', 'himself', 'twins', '19', 'brand', 'europe', 'astronauts', 'machines', 'inside', 'code', 'michelangelo', 'football', 'she', 'sings', 'basketball', 'telephone', 'silly', 'bureau', 'small', 'pellegrin', 'elements', 'plant', '15', 'las', 'done', 'dc', 'loss', 'japan', 'nature', 'online', 'pope', 'turn', 'read', 'radio', 'ireland', 'van', 'ask', 'claim', 'maurizio', 'field', 'income', 'central', 'african', 'expression', 'golden', 'perfect', 'gamble', 'yellow', 'buried', 'jane', 'claimed', 'written', 'depression', 'level', 'minimum', 'religion', 'broadway', 'italian', 'stole', 'cross', 'bottle', 'procter', 'operating', 'birthday', 'prince', 'magazine', 'cancer', 'watergate', 'swimming', 'conference', 'corpus', 'paint', 'thunder', 'logan', 'broken', 'russia', 'languages', 'wage', 'middle', 'grow', 'clean', 'olympics', 'events', 'told', 'mountains', 'hollywood', 'oscar', 'plays', 'players', 'party', 'aids', 'tale', 'rivers', 'project', 'ohio', 'toll', 'same', 'course', 'eggs', 'brown', '7', '21', 'goodall', 'border', 'leading', 'fuel', 'discovered', 'santa', 'nine', 'literary', '6', 'declared', 'site', 'bee', 'microsoft', 'clothing', 'investigation', 'purchase', 'am', 'drive', 'pole', 'contact', 'son', 'costner', 'fox', 'l.a.', 'also', 'rum', 'appeared', 'louis', '1969', 'claims', 'young', '1939', 'published', 'swimmer', 'occupation', 'gay', 'file', 'composer', 'outside', 'tokyo', 'least', 'label', 'just', 'jimmy', 'strip', 'email', 'produces', 'fifth', 'peace', 'receive', 'hearing', 'jewish', 'boy', 'clothes', 'journal', 'went', 'christian', 'blind', 'end', 'independence', 'tom', 'stone', 'chicken', 'reach', 'criminal', 'season', 'promote', 'ad', 'answers', 'tuberculosis', 'chicago', 'triangle', 'acted', 'cat', 'taste', 'putty', 'jersey', 'qigong', 'every', 'classic', 'ten', 'directed', 'records', 'apples', 'phone', 'll', 'pennsylvania', 'continent', 'beers', 'award', '1965', 'kevin', 'sleep', 'florence', 'position', 'executed', 'page', 'occur', 'face', 'these', 'sons', 'charlie', 'mozambique', 'steel', 'poker', 'event', 'awarded', 'missouri', 'simpsons', 'energy', 'railroad', 'cork', 'titanic', '1972', 'reign', 'meant', 'rogers', 'delaware', 'days', 'correct', 'eleven', 'fish', 'child', 'itself', 'cash', 'answers.com', 'bone', 'egg', 'championship', 'election', 'bob', 'raise', 'glass', 'appear', 'master', '1980', 'mark', 'cells', 'bay', 'degrees', 'tube', 'camp', 'must', 'iq', 'robert', 'victoria', '12', 'cube', 'club', 'southern', 'chemicals', 'act', 'hydrogen', 'jesus', 'milk', 'next', '1994', 'pacific', 'developed', 'planted', 'fred', '1967', 'any', 'set', 'museum', 'nicknamed', 'gas', 'ages', 'price', 'callosum', 'questions', 'tour', 'battery', 'folic', 'church', 'professor', 'ford', 'orange', 'against', 'hold', 'castle', 'review', 'weapon', 'no', 'maker', 'australia', 'nadia', '1983', 'purpose', 'growing', 'sells', 'ibm-compatible', 'astronaut', 'design', 'town', 'captain', 'army', 'k', 'andrew', 'tax', 'arctic', 'c', 'associated', 'roll', 'male', 'record', 'november', '1963', 'ears', 'low', '6th', 'getting', 'autobiography', 'attack', 'woodrow', 'seen', 'comaneci', 'facial', 'wilson', 'case', 'stage', 'troops', 'rocky', 'holds', 'bird', 'intercourse', 'substance', 'greatest', 'beat', 'score', 'order', 'pass', 'successful', 'dogtown', 'china', 'gate', 'daily', 'northernmost', 'side', 'question', 'muppets', 'pregnancy', 'mouth', 'salt', 'manned', 'formed', 'members', 'right', 'mercury', 'o', 'elected', 'justice', 'peak', 'ben', 'caribbean', 'exist', 'aspartame', 'gives', 'technique', 'republic', 'upon', 'buy', 'away', 'leave', 'jones', 'may', 'johnson', 'court', 'hearst', 'dickens', 'seaport', 'writing', 'lead', 'gates', 'still', 'quit', 'citizen', 'mutiny', 'august', 'community', 'mayor', 'brazil', 'publish', 'dakota', 'sees', 'husband', 'monkey', 'creator', 'yahoo', 'cocktail', 'mile', 'producer', 'shows', 'then', 'describe', 'gandhi', 'glitters', 'labels', 'sensitive', 'syndrome', 'revolution', 'association', 'fare', 'ways', 'marvel', 'started', 'humans', 'wolfe', '22', 'trip', 'horses', 'cartoons', '1998', 'represented', 'mexico', 'aaron', 'snow', '16th', 'success', 'comes', 'commonly', 'popeye', 'inch', 'pizza', 'takes', 'spice', 'arma', 'near', 'soccer', 'windsor', 'invent', 'include', 'pop', 'cucumber', 'convicted', 'now', 'michael', 'pound', 'heart', 'robinson', 'cigarette', 'balance', 'stewart', 'working', 'hate', 'restaurant', 'got', 'contract', 'rose', 'mary', 'broadcasting', 'society', 'columbus', 'stereo', 'areas', 'buffalo', 'danube', 'peanut', 'daycare', 'relationship', 'conditioning', 'articles', 'powerful', 'louie', 'races', 'points', 'royal', 'literature', 'profession', 'behind', 'mosquito', 'kept', 'pro', 'on-line', 'suffer', 'provider', 'voice', 'wash', 'steps', 'fans', 'objects', 'want', 'cooler', 'romans', 'sitcom', 'nationality', 'flintstones', 'percent', 'zealand', '19th-century', 'plan', 'using', 'soap', 'insurance', 'atlantic', 'safe', 'cup', 'landing', 'dry', 'theme', 'collect', 'congressman', 'vhs', 'cream', 'novelist', 'wife', 'tony', 'detective', 'manufacturer', 'einstein', 'blue', 'let', 'closest', 'fortune', 'browns', 'clouds', 'arnold', 'tower', 'federal', 'ride', 'various', 'model', 'left', 'tried', 'turned', 'sound', 'planet', 'committee', 'january', 'generator', 'normal', 'third', 'natural', 'bought', 'allowed', 'virgin', 'daughters', 'incredible', 'jockey', 'congress', 'nevada', 'fungal', 'battles', 'muffins', 'source', 'infection', 'spent', 'program', 'flies', 'example', 'myrtle', '1978', 'boys', 'hat', 'powell', 'caffeine', 'disabilities', 'piggy', 'firm', 'cosmology', 'worst', 'keep', 'ago', 'reading', 'occupy', 'contain', 'define', 'anniversary', 'significant', 'bull', 'hulk', 'assassination', 'pull-tab', '1960', 'statistics', 'tornado', 'temple', 'drug', 'italy', 'academy', 'latitude', 'calories', 'trees', 'colin', 'killer', 'canal', 'early', 'd.', 'degas', 'release', 'tutu', 'painter', 'sharp', 'david', 'nothing', 'minor', 'minutes', 'lee', 'abbreviated', ';', 'd.c.', '1989', 'rocks', 'sort', 'peachy', 'attractions', 'network', 'liberty', 'appearance', 'lincoln', 'patented', 'something', 'northeast', 'crop', '13', 'michigan', 'celebrities', 'un', 'nazi', 'hazmat', 'winnie', 'sheep', 'brain', 'dictator', 'root', 'mission', 'houses', '1920s', 'martin', 'comics', 'mormons', 'deal', 'medium', 'skin', 'florida', 'pooh', 'research', 'coach', 'box', 'having', 'oat', 'album', 'don', 'tourist', 'along', 'venus', 'twice', 'bug', 'sperm', 'sang', 'ends', '1973', 'mr.', 'thomas', 'tie', 'contest', 'mouse', 'confederate', 'going', 'roosevelt', 'fingers', 'race', 'road', 'borders', '1942', 'longitude', 'pounds', 'wheel', 'circle', 'israel', 'expectant', 'sex', 'director', 'typical', 'lowest', 'korea', 'inventor', 'likely', 'lord', 'automobile', 'increase', 'cars', 'mccarren', 'electricity', 'vacuum', 'force', 'harry', 'join', 'kids', 'chronic', 'airports', 'typewriter', 'bibliography', 'murder', 'stopped', 'fictional', 'honorary', 'individuals', 'zip', 'flags', 'enough', 'ring', 'killing', 'hitting', 'adventures', 'police', 'sinn', 'beauty', 'count', 'brooks', 'handheld', 'sing', 'aldrin', 'saint', 'athlete', 'presidency', 'physical', 'virginia', 'brought', 'match', 'engine', 'enter', 'native', 'seattle', 'virtual', 'birthstone', 'paso', 'maiden', 'mao', 'toy', 'correctly', 'coffee', 'seized', 'fein', 'rubber', 'claus', 'nicholas', 'lovers', 'highway', 'italians', 'pitcher', 'command', 'christopher', 'alley', 'adult', 'several', 'follow', 'baking', 'wonders', 'bet', 'muhammad', 'iii', 'held', 'pressure', 'heavily', 'our', 'addresses', 'madonna', 'superman', 'al', 'western', 'less', '49', 'sexual', 'fresh', 'prophet', 'serving', 'easter', 'caffeinated', 'vbp', 'l.', 'recorded', 'asked', 'happen', 'colored', 'calculator', 'georgia', 'bears', '5th', 'seas', 'champions', 'medicine', 'beaver', 'canadian', 'sink', 'regarding', 'boat', '1981', 'arch', 'markets', 'explorer', 'andy', 'chairman', 'bombay', 'millennium', 'affect', 'instead', 'railway', 'wears', 'station', 'yankees', 'month', 'beethoven', 'closing', 'windows', 'marx', 'could', 'display', 'wines', 'pink', 'large', 'nuclear', 'founder', 'cocaine', '24', 'catholic', 'motors', 'superstar', 'included', 'led', 'angels', 'marked', 'kuwait', 'holes', 'iron', 'doonesbury', 'geese', 'wars', 'stuart', 'attend', 'background', 'rubik', 'resource', 'responsible', 'eruption', 'vietnamese', 'began', 'silver', 'scrabble', 'burned', 'official', 'wonder', 'ads', 'bees', 'bastille', 'close', 'saturday', 'attacks', '1797-185', 'stamp', 'lady', 'holy', 'stands', 'oz', 'canyon', 'process', 'nns', 'cage', 'bars', 'teenager', 'direct', 'camaro', 'monet', 'werewolf', 'terrorist', 'scholar', 'laugh-in', 'companies', 'maid-rites', 'clip', 'signature', 'gulf', 'landed', 'exchange', 'fired', 'prewitt', 'trial', 'july', 'here', 'fatal', 'owns', 'festival', 'dew', 'database', 'future', 'goddess', 'hockey', 'eternity', 'disc', 'bar', 'playing', 'apollo', 'dam', 'feature', 'inches', 'nazis', 'log', 'making', 'spain', 'bronze', 'ranger', 'clark', 'artificial', 'presidents', 'kansas', 'hamblen', 'pairs', 'contemptible', 'major-league', 'speak', 'gave', 'floor', 'row', 'keyboard', 'happens', 'protein', 'kalahari', 'ended', 'kasparov', 'wyoming', 'miami', 'whip', 'surname', 'surrounds', 'lauren', 'complete', 'raid', 'dr.', 'estate', 'rome', 'rich', 'closed', 'illinois', 'judson', 'dumpty', 'sale', 'tennessee', 'learning', 'williams', 'ticket', 'pit', 'architecture', '1953', 'chief', 'alice', 'secret', 'april', 'range', 'plants', 'bottles', 'efficient', 'released', 'lobster', 'enterprise', 'owned', 'covers', 'n', 'issue', 'according', 'bombing', 'bert', 'value', 'lose', 'sisters', 'animated', 'bulls', '16', 'unaccounted', 'dna', 'performer', 'february', 'equator', 'supreme', 'elephant', 'mckinley', 'barbados', 'humpty', 'approximate', 'grand', 'toes', 'spelling', 'handle', 'credit', 'nino', 'marks', 'tolkien', 'hemisphere', 'importer', 'companion', 'period', 'requirement', 'total', 'shuttle', 'franklin', 'singles', 'costs', 'francisco', '197', 'garry', 'sister', 'ancient', 'equipment', 'belgium', 'gods', 'deer', 'paul', 'hurricane', 'ip', 'scoundrel', 'teddy', 'no.', 'forces', 'constellation', 'flourish', 'era', 'variety', 'banned', 'hardest', 'dollar', 'numerals', 'establish', 'f', 'morning', '<bos>', '<eos>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seeding_random()\n",
    "\n",
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "data = []\n",
    "for line in file.readlines():\n",
    "    data.append(line)\n",
    "train_data = data[:round(len(data)*devLine)]\n",
    "dev_data = data[round(len(data)*devLine):]\n",
    "\n",
    "print(\"Création training set...\")\n",
    "training_set = QuestionDataset(train_data, nb_input-3)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création dev set...\")\n",
    "dev_set = QuestionDataset(dev_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création test set...\")\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)\n",
    "test_set = QuestionDataset(test_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "# Création du DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "seeding_random()\n",
    "\n",
    "train_loader = dataloader.DataLoader(training_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "dataloader_args_notshuffle = dict(shuffle=False, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "\n",
    "dev_loader = dataloader.DataLoader(dev_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "test_loader = dataloader.DataLoader(test_set, **dataloader_args_notshuffle)\n",
    "seeding_random()\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"List of word used:\")\n",
    "print(training_set.word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition per classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "    classes = [0,0,0,0,0,0]\n",
    "    for data, target in train_loader:\n",
    "        for t in list(target):\n",
    "            t = t.item()\n",
    "            classes[t] += 1\n",
    "\n",
    "    total = sum(classes)\n",
    "    rep_classes = [c/total*100 for c in classes]\n",
    "    print(\"Répartitions des données dans les classes:\")\n",
    "    for i in range(len(rep_classes)):\n",
    "        print(\"Classe numéro \" + str(i+1) + \": \" + str(rep_classes[i]) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word occurence repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "\n",
    "    word_occ = training_set.reparti_word\n",
    "    word_occ = dict(word_occ)\n",
    "    \n",
    "    total = sum([value for key, value in training_set.reparti_word.most_common(len(training_set.reparti_word))])\n",
    "    \n",
    "    values = [sum([value for key, value in training_set.reparti_word.most_common(i+1)])/total*100 for i in range(len(training_set.reparti_word))]\n",
    "\n",
    "    x = np.linspace(0, len(values), len(values))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    cnn_line, = ax.plot(x, values)\n",
    "\n",
    "    ax.set(xlabel=\"Vocabulaire unique\", ylabel=\"Couverture en %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, layers, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        prev_layer = nb_inputs\n",
    "        for i, l in enumerate(layers):\n",
    "            name_attr = \"rnn\"+str(i)\n",
    "            setattr(self, name_attr, nn.RNN(input_size=prev_layer, num_layers=1,\n",
    "               hidden_size=l, dropout=0., batch_first=True, nonlinearity='relu'))\n",
    "            prev_layer = l\n",
    "        \n",
    "        #TODO: use also last rnn layer\n",
    "        name_attr = \"rnn\"+str(len(layers))\n",
    "        setattr(self, name_attr, nn.RNN(input_size=prev_layer, num_layers=1,\n",
    "           hidden_size=nb_outputs, dropout=0., batch_first=True, nonlinearity='relu'))\n",
    "        \n",
    "        \n",
    "        #self.inter = nn.Linear(prev_layer, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.layers = layers\n",
    "        print(self.__dict__)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inp = inputs\n",
    "        \n",
    "        for i in range(len(self.layers)):\n",
    "            h0 = torch.zeros(1, inp.size(0), self.layers[i])\n",
    "            if use_cuda:\n",
    "                h0 = h0.to(\"cuda\")\n",
    "            name_attr = \"rnn\"+str(i)\n",
    "            #print(getattr(self, name_attr))\n",
    "            inp, hn = getattr(self, name_attr)(inp, h0)\n",
    "         \n",
    "        h0 = torch.zeros(1, inp.size(0), self.output_dim)\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        name_attr = \"rnn\"+str(len(self.layers))\n",
    "        #print(getattr(self, name_attr))\n",
    "        inp, hn = getattr(self, name_attr)(inp, h0)\n",
    "\n",
    "        #x = self.inter(hn[0])\n",
    "        \n",
    "        x = self.sm(hn[0])\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "#TODO\n",
    "#Entropy mean might be near to zero\n",
    "def getEntropies(rnn, batch_list):\n",
    "    entropy_list = []\n",
    "    #value, counts = np.unique(out, return_counts=True)\n",
    "    #entropy_list.append(entropy(out, base=None))\n",
    "    return [-1]\n",
    "\n",
    "\n",
    "# return correct_percent\n",
    "def getEfficience(rnn, batch_list) :\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        out = rnn(data).data\n",
    "        \n",
    "        _, predicted = torch.max(out.data, dim=1)\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return total_correct / total\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, data_loader, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_rnn = rnn\n",
    "    max_acc_dev = -1\n",
    "    pos_best_rnn = 0;\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_correct = 0\n",
    "        total_target = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            #rnn.train()\n",
    "            seeding_random()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            rnn.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Get the Accuracy\n",
    "            \n",
    "            _, predicted = torch.max(output.data, dim=1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total_correct += correct\n",
    "            total_target += target.size(0)\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 500 == 0 or batch_idx % 500 == 1 or batch_idx == len(data_loader)-1:\n",
    "                print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}\\t Accuracy: {}'.format(\n",
    "                    epoch+1,\n",
    "                    num_epochs,\n",
    "                    batch_idx * len(data), \n",
    "                    len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader), \n",
    "                    loss.data.item(),\n",
    "                    (total_correct / total_target) * 100),\n",
    "                    end='')\n",
    "                losses_train.append(loss.data.item())\n",
    "                if great_analysis:\n",
    "                    dev_data, dev_target = next(iter(dev_loader))\n",
    "                    dev_data, dev_target = dev_data.to(device), dev_target.to(device)\n",
    "                    output = rnn(dev_data)\n",
    "                    loss = criterion(output, dev_target)\n",
    "                    losses_dev.append(loss.data.item())\n",
    "                    \n",
    "                    \n",
    "        print()\n",
    "        acc_dev = getEfficience(rnn, dev_loader)*100\n",
    "        if acc_dev > max_acc_dev:\n",
    "            max_acc_dev = acc_dev\n",
    "            best_rnn = rnn\n",
    "            pos_best_rnn = epoch\n",
    "        \n",
    "        print(\"Dev set: accuracy: \" + str(acc_dev) + \"% | max acc: \" + str(max_acc_dev)+\"%\")\n",
    "        print()\n",
    "    rnn = best_rnn\n",
    "    # Return losses list, you can print them later if you want\n",
    "    return {\"losses_train\":losses_train, \"losses_dev\":losses_dev, \"pos_best\":pos_best_rnn+1, \"best_rnn\":best_rnn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruning implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(rnn, data_loader, acc_drop_max):\n",
    "    accuracy = -1\n",
    "    prune_acc = accuracy\n",
    "    while accuracy - prune_acc <= acc_drop_max:\n",
    "        with torch.enable_grad():\n",
    "            job = learn(rnn, data_loader, nb_epochs)\n",
    "            if accuracy == -1:\n",
    "                accuracy = getEfficience(rnn, data_loader)*100\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 1700, '_modules': OrderedDict([('rnn0', RNN(1700, 10, batch_first=True)), ('rnn1', RNN(10, 10, batch_first=True)), ('rnn2', RNN(10, 10, batch_first=True)), ('rnn3', RNN(10, 10, batch_first=True)), ('rnn4', RNN(10, 10, batch_first=True)), ('rnn5', RNN(10, 10, batch_first=True)), ('rnn6', RNN(10, 10, batch_first=True)), ('rnn7', RNN(10, 10, batch_first=True)), ('rnn8', RNN(10, 6, batch_first=True)), ('sm', Softmax())]), '_backward_hooks': OrderedDict(), 'layers': [10, 10, 10, 10, 10, 10, 10, 10], '_forward_pre_hooks': OrderedDict(), '_state_dict_hooks': OrderedDict(), '_forward_hooks': OrderedDict(), '_buffers': OrderedDict(), 'output_dim': 6, 'training': True, '_backend': <torch.nn.backends.thnn.THNNFunctionBackend object at 0x7f65d3f3b2e8>, 'optimizer': Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      "), '_parameters': OrderedDict(), '_load_state_dict_pre_hooks': OrderedDict()}\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "rnn = RNN(nb_inputs = len(training_set.word_list), layers = LAYERS, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "\n",
    "seeding_random()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prune(rnn, train_loader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 1/60 [10800/10816 (100%)]\t Loss: 1.636646\t Accuracy: 28.106508875739642\n",
      "Dev set: accuracy: 36.36755823986195% | max acc: 36.36755823986195%\n",
      "\n",
      " Train Epoch: 2/60 [10800/10816 (100%)]\t Loss: 1.420530\t Accuracy: 39.968565088757394\n",
      "Dev set: accuracy: 51.59620362381363% | max acc: 51.59620362381363%\n",
      "\n",
      " Train Epoch: 3/60 [10800/10816 (100%)]\t Loss: 1.303144\t Accuracy: 53.9478550295858\n",
      "Dev set: accuracy: 54.702329594477995% | max acc: 54.702329594477995%\n",
      "\n",
      " Train Epoch: 4/60 [16/10816 (0%)]\t Loss: 1.496957\t Accuracy: 46.875"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-acc5a2b2d051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlosses_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"losses_train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-f14bfcc3d1b0>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(rnn, data_loader, num_epochs)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "with torch.enable_grad():\n",
    "    job = learn(rnn, train_loader, nb_epochs)\n",
    "    \n",
    "    losses_train = job[\"losses_train\"]\n",
    "    losses_dev = job[\"losses_dev\"]\n",
    "    pos_best_rnn = job[\"pos_best\"]\n",
    "    rnn = job[\"best_rnn\"]\n",
    "    print(\"Done :)\")\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Learned in \" + str(end_time - begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7928dc791bef4ba291c689d59d08c687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='smooth', max=500, min=5, step=2), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def update_losses(smooth=1):\n",
    "    x_train = np.linspace(0, len(losses_train), len(losses_train))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax_train = fig.add_subplot(1,1,1)\n",
    "    cnn_line_train, = ax_train.plot(x_train, losses_train)\n",
    "    cnn_line_train.set_ydata(savgol_filter(losses_train, smooth, 3))\n",
    "    \n",
    "    if great_analysis:\n",
    "        x_dev = np.linspace(0, len(losses_dev), len(losses_dev))\n",
    "        ax_dev = fig.add_subplot(1,1,1)\n",
    "        cnn_line_dev, = ax_dev.plot(x_dev, losses_dev)\n",
    "        cnn_line_dev.set_ydata(savgol_filter(losses_dev, smooth, 3))\n",
    "    \n",
    "interact(update_losses, smooth=(5, 500, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n",
      "On the training set:\n",
      "Corrects: 23.788831360946748%\n",
      "\n",
      "On the dev set:\n",
      "Corrects: 22.7351164797239%\n",
      "\n",
      "On the test set:\n",
      "Moyenne des entropies: -1\n",
      "Corrects: 18.8%\n",
      "\n",
      "A présent, tu peux copier-coller ça dans le doc sur le drive :)\n",
      "1700\t0.001\t60\t[10, 10, 10, 10, 10, 10, 10, 10]\t16\t\t-1\t5\t23.788831360946748%\t22.7351164797239%\t18.8%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Congratulations!\")\n",
    "\n",
    "rnn.eval()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_train = getEfficience(rnn, train_loader)*100\n",
    "\n",
    "print(\"On the training set:\")\n",
    "print(\"Corrects: \" + str(correct_train) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_dev = getEfficience(rnn, dev_loader)*100\n",
    "\n",
    "print(\"On the dev set:\")\n",
    "print(\"Corrects: \" + str(correct_dev) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_test = getEfficience(rnn, test_loader)*100\n",
    "\n",
    "mean_entropies = -1\n",
    "\n",
    "print(\"On the test set:\")\n",
    "print(\"Moyenne des entropies: \" + str(mean_entropies))\n",
    "print(\"Corrects: \" + str(correct_test) + \"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "inputs = nb_input\n",
    "if inputs == -1:\n",
    "    inputs = len(training_set.word_list)-3\n",
    "\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(inputs)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(LAYERS)\n",
    "      +\"\\t\"+str(nb_batchs)+\"\\t\\t\"+str(mean_entropies)+\"\\t\"+str(pos_best_rnn)\n",
    "      +\"\\t\"+str(correct_train)+\"%\\t\"+str(correct_dev)+\"%\\t\"+str(correct_test)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
