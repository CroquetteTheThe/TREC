{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amélioration : Transformer les mots en probabilité d'appartenance aux catégrories.\n",
    "\n",
    "1. Récupérer les mots.\n",
    "2. Calculer, pour chaque mot de chaque phrase, la moyenne d'appartenance aux catégories de ce mot.\n",
    "3. Remplacer les OneHot par ces embeddings.\n",
    "4. Effectuer des transformations sur le RNN si besoin est.\n",
    "5. Comparer les résultats avec la baseline.\n",
    "6. Faire un retour sur ces résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 1700 # write -1 if you want every words\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 8\n",
    "hidden_size = 10\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 80\n",
    "\n",
    "# Random seed, don't change it if you don't know what it is\n",
    "random_seed = 42\n",
    "\n",
    "nb_batchs = 16\n",
    "\n",
    "# How many percent of your data do you use as training set\n",
    "devLine = 0.7\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If your goal is to draw graphs\n",
    "great_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding_random():\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    \n",
    "    # Special constructor\n",
    "    # | nb_most_commons can either be the number of most common words you\n",
    "    # | want to work with, OR a list of word you want to work with\n",
    "    # If nb_most_commons == -1, then all word will count\n",
    "    \n",
    "    def __init__(self, train_data, nb_most_commons=-1):\n",
    "        questions = []\n",
    "        labels = []\n",
    "\n",
    "        # Black list\n",
    "        black_list = '\\'`[@_!#$%^&*()<>?/\\|}{~:]'\n",
    "        \n",
    "        for string in train_data:\n",
    "            question_str = []\n",
    "            for x in string.split()[1:]:\n",
    "                s = \"\"\n",
    "                for c in x:\n",
    "                    if not c in black_list:\n",
    "                        s += c\n",
    "                if not s == \"\":\n",
    "                    question_str.append(s.lower())\n",
    "                        \n",
    "            labels.append(string.split()[0])\n",
    "            questions.append(question_str)\n",
    "\n",
    "        \n",
    "        if isinstance(nb_most_commons, int):\n",
    "            # Vocabulary of unique words\n",
    "            data = []\n",
    "            for q in questions:\n",
    "                for w in q:\n",
    "                    data.append(w)\n",
    "            self.reparti_word = Counter(data)\n",
    "            \n",
    "            if nb_most_commons < 0:\n",
    "                most_commons_words = self.reparti_word.most_common(len(data))\n",
    "            else:\n",
    "                most_commons_words = self.reparti_word.most_common(nb_most_commons)\n",
    "            \n",
    "            self.word_list = list([x[0] for x in most_commons_words])\n",
    "            self.word_list.append('<bos>')\n",
    "            self.word_list.append('<eos>')\n",
    "            self.word_list.append('<unk>')\n",
    "        elif isinstance(nb_most_commons, list):\n",
    "            self.word_list = nb_most_commons\n",
    "        else:\n",
    "            print(\"ERROR: second arg is neither an int, nor a list\")\n",
    "            \n",
    "        words_array = np.array(self.word_list)\n",
    "        \n",
    "        # Add tags <bos> and <eos> to questions\n",
    "        for q in questions:\n",
    "            if q[0] != '<bos>' :\n",
    "                q.insert(0, '<bos>')\n",
    "                q.append('<eos>')\n",
    "        \n",
    "        # WIP\n",
    "        def mot_cle(words, questions, labels):\n",
    "            words_embedded = {}\n",
    "            for i in range(len(words)):\n",
    "                embedding = [0,0,0,0,0,0]\n",
    "                # embedding['ABBR'] # Abbreviation\n",
    "                # embedding['ENTY'] # Entity\n",
    "                # embedding['DESC'] # Description\n",
    "                # embedding['HUM']  # Human\n",
    "                # embedding['LOC']  # Location\n",
    "                # embedding['NUM']  # Numeric\n",
    "                count = 0\n",
    "                for y in range(len(questions)):\n",
    "                    if words[i] in questions[y]:\n",
    "                        if labels[y] == \"ABBR\":\n",
    "                            embedding[0] += 1\n",
    "                        elif labels[y] ==\"ENTY\":\n",
    "                            embedding[1] += 1\n",
    "                        elif labels[y] ==\"DESC\":\n",
    "                            embedding[2] += 1\n",
    "                        elif labels[y] ==\"HUM\":\n",
    "                            embedding[3] += 1\n",
    "                        elif labels[y] ==\"LOC\":\n",
    "                            embedding[4] += 1\n",
    "                        elif labels[y] ==\"NUM\":\n",
    "                            embedding[5] += 1\n",
    "                        count += 1\n",
    "                for l in range(len(embedding)):\n",
    "                    if count > 0:\n",
    "                        embedding[l] /= count\n",
    "                words_embedded[words[i]] = embedding\n",
    "            return words_embedded\n",
    "        \n",
    "        self.words_embedded = mot_cle(words_array, questions, labels)\n",
    "        # WIP On va sans doute remplacer cette partie.\n",
    "        #\n",
    "        ## Integer encoding with OneHotEncoder\n",
    "        #words_tre = words_array.reshape(len(words_array),1)\n",
    "        #one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "        #onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "        ## Creating a dictionnary of word and its one hot array\n",
    "        #self.words_onehoted = {}\n",
    "        #for i in range(0, len(words_array)):\n",
    "        #    self.words_onehoted[self.word_list[i]] = onehot_encoded[i]\n",
    "        #    \n",
    "        # WIP Fin de la partie à remplacer.\n",
    "\n",
    "        # One hot categories\n",
    "        self.categories_num = {}\n",
    "        self.categories_num['ABBR'] = 0 # Abbreviation\n",
    "        self.categories_num['ENTY'] = 1 # Entity\n",
    "        self.categories_num['DESC'] = 2 # Description\n",
    "        self.categories_num['HUM']  = 3 # Human\n",
    "        self.categories_num['LOC']  = 4 # Location\n",
    "        self.categories_num['NUM']  = 5 # Numeric\n",
    "\n",
    "        self.batch_data = []\n",
    "        for num_question in range(len(questions)):\n",
    "            # WIP On va aussi remplacer cette ligne\n",
    "            ## Construction of question_onehot list.\n",
    "            #question_onehot = [self.get_onehot_word(word) for word in questions[num_question]]\n",
    "            question_embedding = [self.get_embedding(word) for word in questions[num_question]]\n",
    "\n",
    "            # Construction of category_onehot.\n",
    "            category = labels[num_question].partition(':')[0]\n",
    "            category_onehot = self.get_num_category(category)\n",
    "            # WIP Ici aussi\n",
    "            #self.batch_data.append([(question_onehot), (category_onehot)])\n",
    "            self.batch_data.append([(question_embedding), (category_onehot)])\n",
    "        \n",
    "    \n",
    "    # Function to get the corresponding one hot list for a category.\n",
    "    def get_num_category(self, category):\n",
    "        return self.categories_num[category]\n",
    "\n",
    "\n",
    "    # Function to get the corresponding one hot list for a word.\n",
    "    def get_onehot_word(self, word):\n",
    "        if word in self.words_onehoted:\n",
    "            return list(self.words_onehoted[word])\n",
    "        else:\n",
    "            return list(self.words_onehoted['<unk>'])\n",
    "        \n",
    "    def get_embedding(self, word):\n",
    "        if word in self.words_embedded:\n",
    "            return list(self.words_embedded[word])\n",
    "        else:\n",
    "            # If word is unknown (<unk> tag), we give it a neutral embedding.\n",
    "            return [0,0,0,0,0,0]\n",
    "\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.batch_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeding_random()\n",
    "        return self.batch_data[idx]\n",
    "    \n",
    "def pad_collate(batch):\n",
    "    max_length = max([len(q[0]) for q in batch])\n",
    "\n",
    "    inputs = torch.FloatTensor([[[0. for _ in range(len(x[0][0]))] for i in range(max_length-len(x[0]))]+x[0] for x in batch])\n",
    "    outputs = torch.LongTensor([x[1] for x in batch])\n",
    "    \n",
    "    return inputs, outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création training set...\n",
      "Done!\n",
      "Création dev set...\n",
      "Done!\n",
      "Création test set...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seeding_random()\n",
    "\n",
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "data = []\n",
    "for line in file.readlines():\n",
    "    data.append(line)\n",
    "train_data = data[:round(len(data)*devLine)]\n",
    "dev_data = data[round(len(data)*devLine):]\n",
    "\n",
    "print(\"Création training set...\")\n",
    "training_set = QuestionDataset(train_data, nb_input-3)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création dev set...\")\n",
    "dev_set = QuestionDataset(dev_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création test set...\")\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)\n",
    "test_set = QuestionDataset(test_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour cette fonction, on va utiliser words, question et labels en exemple.\n",
    "# La vraie fonction utilisera soit words_array soit words_tre.\n",
    "# Elle utilisera aussi les vrais questions et labels pour identifier les mots.\n",
    "\n",
    "questions = [\"What films featured the character Popeye Doyle ?\", \"How can I find a list of celebrities ' real names ?\", \"What are liver enzymes ?\"]\n",
    "labels = [\"ENTY\", \"DESC\", \"DESC\"]\n",
    "words = [\"what\", \"films\", \"featured\", \"the\", \"character\", \"popeye\", \"doyle\", \"?\", \"how\", \"can\", \"i\", \"find\", \"a\", \"list\", \"of\", \"celebrities\", \"'\", \"real\", \"names\", \"are\", \"liver\", \"enzymes\"]\n",
    "\n",
    "def mot_cle(words, questions, labels):\n",
    "    words_embedded = {}\n",
    "    for i in range(len(words)):\n",
    "        embedding = [0,0,0,0,0,0]\n",
    "        # embedding['ABBR'] # Abbreviation\n",
    "        # embedding['ENTY'] # Entity\n",
    "        # embedding['DESC'] # Description\n",
    "        # embedding['HUM']  # Human\n",
    "        # embedding['LOC']  # Location\n",
    "        # embedding['NUM']  # Numeric\n",
    "        count = 0\n",
    "        for y in range(len(questions)):\n",
    "            if words[i] in questions[y].lower():\n",
    "                if labels[y] == \"ABBR\":\n",
    "                    embedding[0] += 1\n",
    "                elif labels[y] ==\"ENTY\":\n",
    "                    embedding[1] += 1\n",
    "                elif labels[y] ==\"DESC\":\n",
    "                    embedding[2] += 1\n",
    "                elif labels[y] ==\"HUM\":\n",
    "                    embedding[3] += 1\n",
    "                elif labels[y] ==\"LOC\":\n",
    "                    embedding[4] += 1\n",
    "                elif labels[y] ==\"NUM\":\n",
    "                    embedding[5] += 1\n",
    "                count += 1\n",
    "        for l in range(len(embedding)):\n",
    "            embedding[l] /= count\n",
    "        words_embedded[words[i]] = embedding\n",
    "    return words_embedded\n",
    "\n",
    "embedding = mot_cle(words,questions,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.3333333333333333, 0.6666666666666666, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embedding(embedding, word):\n",
    "    if word in embedding:\n",
    "        return list(embedding[word])\n",
    "    else:\n",
    "        # If word is unknown (<unk> tag), we give it a neutral embedding.\n",
    "        return [0,0,0,0,0,0]\n",
    "    \n",
    "get_embedding(embedding,\"?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Si possible, ne pas toucher en dessous avant les tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "List of word used:\n",
      "['the', 'what', 'is', 'of', 'in', 'a', 'how', 's', 'was', 'who', 'to', ',', 'are', 'for', 'and', 'did', 'does', 'do', 'name', 'on', 'many', 'where', 'i', 'you', 'can', 'first', 'when', 'from', 'which', 'world', 'that', 'city', 'as', 'with', 'country', 'has', 'most', '.', 'u.s.', 'by', 'an', 'have', 'find', 'it', 'why', 'there', 'people', 'get', 'called', 'state', 'year', 'were', 'mean', 'be', 'american', 'president', 'largest', 'his', 'fear', 'two', 'at', 'war', 'new', 'its', 'origin', 'word', 'much', 'about', 'known', 'kind', 'company', 'between', 'game', 'film', 'long', 'day', 'movie', 'live', 'made', 'your', 'or', 'take', 'only', 'stand', 'man', 'book', 'best', 'tv', 'their', 'john', 'one', 'famous', 'color', 'all', 'star', 'show', 'term', 'he', 'used', 'my', 'out', 'come', 'play', 'baseball', 'invented', 'had', 'into', 'call', 'make', 'countries', 'number', 'home', 'old', 'dog', 'time', 'america', 'character', 'team', 'three', 'actor', 'not', 'nn', 'river', 'information', 'average', 'states', 'highest', 'south', 'some', 'after', 'names', 'write', 'body', 'born', 'use', 'years', 'difference', 'if', 'up', 'university', 'during', 'said', 'won', 'killed', 'played', 'say', 'four', 'us', 'work', 'black', 'become', 'english', 'united', 'died', 'would', '1', 'song', 'novel', 'meaning', 'named', 'earth', 'car', 'computer', 'wrote', 'last', 'good', 'water', 'go', 'space', 'located', 'school', 'common', 'different', 'drink', 'longest', 'horse', 'sport', 'place', 'king', 'will', 'animal', 'woman', 'battle', 'been', 'date', 'over', 'mississippi', 'makes', 'me', 'north', 'mountain', 'her', 'system', 'national', 'way', 'contains', 'top', 'address', 'actress', 'russian', 't', 'times', 'nickname', 'internet', 'population', 'island', 'college', 'popular', 'form', 'abbreviation', 'capital', 'causes', 'little', 'created', 'served', 'they', 'history', 'part', 'money', 'law', 'portrayed', 'whose', 'west', 'animals', 'through', 'product', 'death', 'craft', 'language', 'bowl', 'mother', 'randy', 'queen', 'than', 'became', 'being', 'group', 'great', 'should', 'biggest', 'die', 'california', 'person', 'food', 'air', 'start', 'ball', 'like', 'other', 'life', 'office', 'french', 'cities', '2', 'james', 'married', 'spanish', 'england', 'york', 'definition', 'european', 'cnn', 'following', 'before', 'games', 'william', 'each', 'boasts', 'cost', 'musical', 'general', 'bill', 'soft', 'feet', 'line', 'fastest', 'kennedy', 'win', 'happened', 'international', 'lives', 'eat', 'f.', 'leader', 'more', 'children', 'love', 'building', 'super', 'cards', 'features', 'comic', 'moon', 'letter', 'charles', '-', 'big', 'airport', 'san', 'title', 'female', 'series', 'baby', 'instrument', 'percentage', 'board', 'british', 'disease', 'canada', 'prize', 'germany', 'real', 'sometimes', 'nnp', 'white', 'begin', 'japanese', 'indian', 'business', 'caused', 'author', 'center', 'indians', 'night', 'second', 'type', 'another', 'power', 'ever', 'olympic', 'newspaper', 'bear', 'symbol', 'jaws', 'latin', 'sea', 'saw', 'lawyer', 'fire', 'men', 'park', 'age', 'seven', 'nixon', 'st.', 'travel', 'house', 'found', 'element', 'desert', 'organization', 'role', 'colors', 'stop', 'television', 'hitler', 'miles', 'london', 'someone', 'far', 'list', 'oldest', 'believe', 'website', 'under', 'starred', 'five', 'blood', 'county', 'street', 'green', 'member', 'security', 'nobel', 'soviet', 'give', 'bridge', 'hit', 'former', 'music', 'richard', 'boxing', 'golf', 'singing', 'hair', 'founded', 'africa', 'introduced', 'ocean', 'million', 'once', 'soldiers', 'original', 'need', 'light', 'words', 'greek', 'art', 'store', 'lake', 'monopoly', 'six', 'de', 'cartoon', 'back', 'nt', 'george', 'washington', 'flight', 'built', 'el', 'rate', 'we', 'them', 'types', 'titled', 'bible', 'runs', 'march', 'shea', 'gould', 'human', 'birds', 'full', 'islands', 'cold', 'point', 'prime', 'months', 'france', 'birth', 'gold', 'fame', 'area', 'union', 'miss', 'jack', 'dubbed', 'clock', 'never', 'thing', 'sioux', 'fast', 'produced', '5', '0', 'flag', 'civil', 'christmas', 'red', 'chinese', 'cover', 'minister', 'web', 'education', 'ship', 'tallest', 'so', '1984', 'fought', 'size', 'texas', 'but', 'roman', 'east', 'asian', 'know', 'look', 'rain', 'headquarters', 'jude', 'received', 'spumante', 'sports', 'market', 'thatcher', 'department', 'dead', 'ray', 'hero', 'eyes', 'came', 'peter', '1991', 'women', 'painted', 'oil', 'main', 'worth', 'kentucky', 'shot', 'originate', 'paid', 'stars', 'whom', 'americans', 'player', 'fly', 'tell', 'tree', 'major', 'films', 'amendment', 'motto', 'beach', 'paper', 'reims', 'painting', 'historical', 'standard', 'eye', 'watch', 'commercial', 'schools', 'girl', 'assassinated', 'hole', 'government', 'host', 'wear', 'kid', 'sun', 'wine', 'stock', 'see', 'daughter', 'setting', 'league', 'celebrated', 'chemical', 'favorite', 'produce', 'dick', 'considered', 'bowling', 'lived', 'los', 'species', 'off', 'products', 'poem', 'around', 'wall', 'forest', 'based', 'dt', 'berlin', 'social', 'pearl', 'remove', 'speed', 'shakespeare', 'chocolate', 'ice', 'empire', 'reason', 'this', 'presidential', '1899', 'patent', 'characters', 'sign', 'electric', 'diego', 'hands', 'songs', 'own', 'writer', 'current', 'medical', 'tennis', 'god', 'comedian', 'father', 'constitution', '11', 'free', 'inspired', 'angeles', 'cd', 'head', 'brothers', 'lyrics', 'acid', 'books', 'jean', 'put', 'beatles', 'build', 'winter', '3', 'weight', 'harbor', 'followed', 'down', 'serve', 'opera', 'mrs.', 'emperor', 'card', 'vietnam', 'classical', 'artist', 'steven', 'rights', 'often', 'video', 'desmond', 'students', 'la', 'numbers', 'run', 'without', 'massachusetts', 'henry', 'balls', 'career', 'singer', 'ii', 'featured', 'bounty', 'family', 'cowboy', 'johnny', 'rule', 'oceans', 'starring', 'rascals', 'vegas', 'mount', 'spielberg', 'spy', 'band', 'creature', '8', 'bond', 'poet', 'function', 'equal', 'irish', 'beer', 'governor', 'hand', 'always', 'export', 'magic', 'nations', 'e-mail', 'because', 'secretary', 'week', 'council', 'high', 'india', 'bomb', 'spoken', 'while', 'sold', 'create', 'took', 'famed', 'square', 'jackson', '10', 'story', 'letters', 'rock', 'silly', 'nature', 'ask', 'small', 'central', 'japan', 'michelangelo', 'read', 'tiger', 'pope', 'elements', 'sings', 'maurizio', 'pellegrin', 'field', 'december', 'dc', 'las', 'loss', 'turn', 'done', 'golfer', 'online', 'radio', 'telephone', 'golden', 'inside', 'lakes', 'plant', 'machines', 'expression', 'bureau', '15', '19', 'she', 'claim', 'ireland', 'europe', 'himself', 'century', 'astronauts', 'income', 'alaska', 'van', 'twins', 'code', 'football', 'brand', 'basketball', 'african', 'russia', 'stole', 'nine', 'course', 'aids', 'perfect', 'religion', 'fuel', 'corpus', 'hollywood', 'party', 'oscar', 'broadway', 'cross', 'discovered', 'border', 'grow', 'prince', 'broken', 'watergate', 'swimming', '21', 'olympics', 'level', '7', 'buried', 'ohio', 'leading', 'cancer', 'mountains', 'rivers', 'yellow', 'jane', 'goodall', 'procter', 'gamble', 'tale', 'logan', 'thunder', 'santa', 'operating', 'brown', 'clean', 'players', 'toll', 'claimed', 'paint', 'literary', 'languages', '6', 'bottle', 'told', 'depression', 'italian', 'written', 'minimum', 'wage', 'middle', 'events', 'project', 'plays', 'same', 'birthday', 'eggs', 'conference', 'magazine', 'louis', 'clothing', 'putty', 'young', 'fox', 'classic', 'claims', 'purchase', 'produces', 'season', 'just', 'directed', 'tom', 'occupation', 'journal', 'published', 'pole', 'taste', 'qigong', 'independence', 'composer', 'rum', 'am', 'ad', 'microsoft', 'chicken', 'fifth', 'swimmer', 'peace', 'christian', 'jewish', '1969', 'll', 'declared', 'records', 'went', 'award', 'triangle', '1965', 'promote', 'beers', 'clothes', 'answers', 'appeared', 'bee', 'outside', 'label', 'kevin', 'costner', 'file', 'acted', 'boy', 'drive', 'gay', 'pennsylvania', '1939', 'site', 'cat', 'apples', 'blind', 'stone', 'chicago', 'investigation', 'phone', 'email', 'contact', 'ten', 'hearing', 'receive', 'jimmy', 'also', 'l.a.', 'reach', 'end', 'least', 'sleep', 'criminal', 'tuberculosis', 'continent', 'son', 'jersey', 'strip', 'tokyo', 'every', 'cork', 'executed', 'bay', 'cash', 'next', 'callosum', 'page', 'jesus', 'fred', 'election', 'church', 'delaware', 'raise', 'iq', '1967', 'occur', 'southern', 'nicknamed', 'planted', '1972', 'titanic', 'folic', 'railroad', 'against', 'steel', '1980', '1994', 'sons', 'egg', 'florence', 'chemicals', 'answers.com', 'robert', 'bone', 'tour', 'club', 'act', 'missouri', 'rogers', 'energy', '12', 'correct', 'must', 'bob', 'any', 'questions', 'master', 'charlie', 'reign', 'child', 'pacific', 'degrees', 'meant', 'tube', 'poker', 'professor', 'milk', 'battery', 'museum', 'championship', 'these', 'cells', 'victoria', 'hydrogen', 'event', 'camp', 'price', 'cube', 'position', 'mozambique', 'glass', 'itself', 'mark', 'ford', 'awarded', 'orange', 'appear', 'gas', 'face', 'developed', 'ages', 'eleven', 'simpsons', 'fish', 'set', 'days', 'leave', 'score', 'sells', 'exist', 'caribbean', 'weapon', 'stage', 'bird', '6th', 'maker', 'elected', 'k', 'northernmost', 'andrew', 'design', 'technique', 'daily', 'c', 'review', 'astronaut', 'november', 'getting', 'pregnancy', 'arctic', 'buy', 'successful', 'case', 'troops', 'republic', 'ears', 'peak', 'intercourse', 'autobiography', 'purpose', 'jones', 'hold', 'gate', 'order', 'facial', 'army', 'members', 'associated', 'ibm-compatible', 'substance', 'question', 'low', 'beat', 'ben', 'johnson', 'australia', 'nadia', 'comaneci', 'growing', 'upon', 'woodrow', 'wilson', 'right', 'pass', 'holds', 'manned', 'away', 'dogtown', 'attack', 'castle', '1983', 'captain', 'mercury', 'o', 'side', 'tax', 'dickens', 'record', 'seen', 'mouth', 'hearst', 'male', 'court', 'justice', 'gives', 'roll', '1963', 'aspartame', 'muppets', 'rocky', 'china', 'greatest', 'town', 'no', 'formed', 'may', 'salt', 'then', 'popeye', 'monkey', 'browns', 'profession', 'objects', 'gandhi', 'insurance', 'inch', 'articles', 'collect', 'points', 'community', 'robinson', 'yahoo', 'tony', 'cocktail', 'relationship', 'danube', '19th-century', 'zealand', 'broadcasting', 'flintstones', 'trip', 'mutiny', 'landing', 'brazil', 'rose', 'mayor', 'steps', 'labels', 'revolution', 'shows', 'plan', 'areas', 'syndrome', 'royal', 'races', 'safe', 'cigarette', 'fortune', 'windsor', 'mosquito', 'arma', 'success', 'ways', 'lead', '16th', 'balance', 'columbus', 'tower', 'romans', '22', 'einstein', 'heart', 'behind', 'literature', 'association', 'closest', 'soap', 'represented', 'dry', 'powerful', 'still', 'soccer', 'conditioning', 'model', 'snow', 'theme', 'comes', 'pop', 'stereo', 'manufacturer', 'sitcom', 'kept', 'cooler', 'cucumber', 'novelist', 'spice', 'sensitive', 'let', 'dakota', 'seaport', 'on-line', 'wife', 'turned', 'contract', 'fans', 'include', 'daycare', 'provider', 'gates', 'suffer', 'arnold', 'sees', 'pound', 'publish', 'wash', 'peanut', 'convicted', 'buffalo', 'horses', 'louie', 'percent', 'want', 'ride', 'hate', 'glitters', 'producer', 'quit', 'detective', 'husband', 'congressman', 'stewart', 'various', 'atlantic', 'humans', 'takes', '1998', 'got', 'tried', 'blue', 'voice', 'cartoons', 'august', 'fare', 'near', 'using', 'vhs', 'cup', 'mile', 'michael', 'wolfe', 'invent', 'commonly', 'society', 'mary', 'left', 'citizen', 'clouds', 'pizza', 'restaurant', 'cream', 'writing', 'describe', 'now', 'creator', 'aaron', 'federal', 'pro', 'nationality', 'marvel', 'mexico', 'started', 'working', 'celebrities', 'deal', 'myrtle', 'mormons', 'venus', '1920s', 'tourist', 'attractions', 'painter', 'occupy', 'bull', 'circle', 'medium', 'don', 'temple', 'album', 'root', 'canal', 'director', '1960', 'expectant', 'sharp', 'minor', 'minutes', 'significant', 'lowest', 'bought', '1942', 'academy', 'ago', 'latitude', 'longitude', 'normal', 'calories', 'hazmat', 'pounds', 'piggy', 'define', 'coach', '1973', 'disabilities', '1989', 'assassination', 'lee', 'crop', 'generator', 'patented', 'nevada', 'tie', 'michigan', 'dictator', 'wheel', 'liberty', 'spent', 'sheep', 'thomas', 'degas', 'rocks', 'borders', 'peachy', 'oat', 'muffins', 'incredible', 'hulk', 'sang', 'reading', 'allowed', 'israel', 'going', 'tornado', 'sort', 'contest', 'flies', 'tutu', 'martin', 'worst', 'abbreviated', 'sperm', 'nazi', 'lincoln', 'nothing', 'early', 'third', 'road', 'bug', 'skin', 'fingers', 'congress', 'colin', 'powell', 'italy', 'fungal', 'infection', 'jockey', 'houses', 'keep', 'killer', 'northeast', 'january', '1978', 'daughters', 'comics', 'box', ';', 'committee', 'boys', 'caffeine', 'race', 'confederate', 'example', 'release', 'something', 'research', 'brain', 'david', 'drug', 'anniversary', 'battles', 'typical', 'un', 'mr.', 'twice', 'winnie', 'pooh', 'appearance', 'contain', 'mission', 'natural', 'statistics', '13', 'hat', 'cosmology', 'd.', 'pull-tab', 'd.c.', 'program', 'virgin', 'having', 'sound', 'source', 'firm', 'along', 'korea', 'roosevelt', 'sex', 'ends', 'mouse', 'trees', 'planet', 'network', 'florida', 'yankees', 'airports', 'bibliography', 'inventor', 'goddess', 'adventures', 'l.', 'inches', 'wonder', 'apollo', 'prophet', 'wears', 'flags', 'founder', 'log', 'command', 'nazis', 'nns', 'resource', 'marx', 'markets', 'dew', 'scholar', 'champions', 'bears', 'signature', 'stuart', 'database', '5th', 'catholic', 'athlete', 'clip', 'electricity', 'colored', '49', 'could', 'nicholas', 'cage', 'police', 'seized', 'cocaine', 'honorary', 'process', 'stopped', 'explorer', 'began', 'correctly', 'beethoven', 'brought', 'harry', 'bar', 'ring', 'vbp', 'playing', 'burned', 'eruption', 'windows', 'wines', 'seattle', 'official', 'direct', 'pink', 'christopher', 'italians', 'bet', 'toy', 'doonesbury', 'likely', 'werewolf', 'automobile', 'sink', 'paso', 'boat', 'match', 'adult', 'stands', 'join', 'iii', 'individuals', 'motors', 'instead', '1797-185', 'closing', 'teenager', 'angels', 'prewitt', 'here', 'eternity', 'marked', 'mao', 'maid-rites', 'responsible', 'fresh', 'festival', 'maiden', 'lovers', 'superstar', 'mccarren', 'beaver', 'stamp', 'physical', 'lord', 'muhammad', 'monet', 'background', 'baking', 'bees', 'claus', 'addresses', 'ads', 'holes', 'terrorist', 'several', 'serving', 'bastille', 'holy', 'fatal', 'handheld', 'calculator', 'pressure', 'easter', 'laugh-in', 'saint', 'arch', 'increase', 'virginia', 'killing', 'regarding', 'nuclear', 'chronic', 'attacks', 'highway', 'typewriter', 'gulf', 'railway', 'bombay', 'enough', 'virtual', 'feature', 'seas', 'landed', 'geese', '1981', 'recorded', 'aldrin', 'station', 'pitcher', 'millennium', 'heavily', 'caffeinated', 'vacuum', 'rubik', 'fired', 'bars', 'murder', 'fictional', 'saturday', 'disc', 'vietnamese', 'birthstone', 'hitting', 'alley', 'cars', 'less', 'madonna', 'scrabble', 'brooks', 'force', 'native', 'medicine', 'enter', 'chairman', 'sinn', 'fein', 'camaro', 'attend', 'kuwait', 'owns', 'display', 'companies', 'large', '24', 'july', 'andy', 'beauty', 'canyon', 'al', 'engine', 'western', 'silver', 'happen', 'zip', 'superman', 'iron', 'canadian', 'presidency', 'close', 'wonders', 'georgia', 'count', 'future', 'hockey', 'our', 'exchange', 'sexual', 'included', 'oz', 'rubber', 'wars', 'led', 'lady', 'follow', 'month', 'dam', 'sing', 'coffee', 'asked', 'affect', 'held', 'trial', 'kids', 'develop', 'contemptible', 'scoundrel', 'lunch', 'hunter', 'heavier', 'scarlett', 'vowel', 'register', 'speak', 'total', 'learning', 'issue', 'j.r.r.', 'tolkien', 'architecture', 'flourish', 'era', 'hamblen', 'complete', 'sisters', 'sides', 'raid', 'worms', 'sale', 'wide', 'valley', 'tribe', 'f', 'involved', 'hemisphere', 'attempts', 'kalahari', 'ventura', 'deer', 'requirement', 'spacewalk', 'ground', 'bombing', 'pan', 'marks', 'bottles', 'nelson', 'dealt', 'according', 'genesis', 'lethal', 'injection', 'stadium', 'miami', 'teddy', 'havoc', 'caesar', 'piece', 'conrad', 'dollar', 'analysis', 'unknown', 'toes', 'owned', 'tend', 'unaccounted', 'ratified', 'row', 'well', 'handle', 'billy', 'marley', 'rich', 'carolina', 'dentist', '197', 'debut', 'belgium', 'dna', 'vermouth', 'nino', 'gene', 'flavor', 'singles', 'ranger', 'yogi', 'includes', 'statue', 'indies', 'importer', 'walker', 'aged', 'zones', 'stanford', 'gained', 'territory', 'wyoming', 'illinois', 'tourists', 'whitcomb', 'judson', 'creeps', 'housewife', 'j.', 'equipment', 'hardest', 'artificial', 'trinidad', 'carl', 'equator', 'bulls', 'chapter', 'monarch', 'chief', 'artists', 'razor', 'costs', 'outer', 'protein', 'surrounds', 'causing', 'movement', 'keyboard', 'mckinley', '4', 'kansas', 'estate', 'layers', 'ip', 'pairs', '<bos>', '<eos>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "# Création du DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "seeding_random()\n",
    "\n",
    "train_loader = dataloader.DataLoader(training_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "dataloader_args_notshuffle = dict(shuffle=False, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "\n",
    "dev_loader = dataloader.DataLoader(dev_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "test_loader = dataloader.DataLoader(test_set, **dataloader_args_notshuffle)\n",
    "seeding_random()\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"List of word used:\")\n",
    "print(training_set.word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition per classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "    classes = [0,0,0,0,0,0]\n",
    "    for data, target in train_loader:\n",
    "        for t in list(target):\n",
    "            t = t.item()\n",
    "            classes[t] += 1\n",
    "\n",
    "    total = sum(classes)\n",
    "    rep_classes = [c/total*100 for c in classes]\n",
    "    print(\"Répartitions des données dans les classes:\")\n",
    "    for i in range(len(rep_classes)):\n",
    "        print(\"Classe numéro \" + str(i+1) + \": \" + str(rep_classes[i]) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word occurence repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "\n",
    "    word_occ = training_set.reparti_word\n",
    "    word_occ = dict(word_occ)\n",
    "    \n",
    "    total = sum([value for key, value in training_set.reparti_word.most_common(len(training_set.reparti_word))])\n",
    "    \n",
    "    values = [sum([value for key, value in training_set.reparti_word.most_common(i+1)])/total*100 for i in range(len(training_set.reparti_word))]\n",
    "\n",
    "    x = np.linspace(0, len(values), len(values))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    cnn_line, = ax.plot(x, values)\n",
    "\n",
    "    ax.set(xlabel=\"Vocabulaire unique\", ylabel=\"Couverture en %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0., batch_first=True, nonlinearity='relu')\n",
    "        self.inter = nn.Linear(nb_neurons, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = torch.zeros(self.nb_layers, inputs.size(0), self.nb_neurons)\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        x, hn = self.rnn(inputs, h0)\n",
    "        \n",
    "        x = self.inter(hn[0])\n",
    "        #print(x)\n",
    "        #x = tensor([list(i[-1]) for i in x])\n",
    "        #print(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "#TODO\n",
    "#Entropy mean might be near to zero\n",
    "def getEntropies(rnn, batch_list):\n",
    "    entropy_list = []\n",
    "    #value, counts = np.unique(out, return_counts=True)\n",
    "    #entropy_list.append(entropy(out, base=None))\n",
    "    return [-1]\n",
    "\n",
    "\n",
    "# return correct_percent\n",
    "def getEfficience(rnn, batch_list) :\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        out = rnn(data).data\n",
    "        \n",
    "        _, predicted = torch.max(out.data, dim=1)\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return total_correct / total\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, data_loader, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_rnn = rnn\n",
    "    max_acc_dev = -1\n",
    "    pos_best_rnn = 0;\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_correct = 0\n",
    "        total_target = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            #rnn.train()\n",
    "            seeding_random()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            rnn.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Get the Accuracy\n",
    "            \n",
    "            _, predicted = torch.max(output.data, dim=1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total_correct += correct\n",
    "            total_target += target.size(0)\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 500 == 0 or batch_idx % 500 == 1 or batch_idx == len(data_loader)-1:\n",
    "                print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}\\t Accuracy: {}'.format(\n",
    "                    epoch+1,\n",
    "                    num_epochs,\n",
    "                    batch_idx * len(data), \n",
    "                    len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader), \n",
    "                    loss.data.item(),\n",
    "                    (total_correct / total_target) * 100),\n",
    "                    end='')\n",
    "                losses_train.append(loss.data.item())\n",
    "                if great_analysis:\n",
    "                    dev_data, dev_target = next(iter(dev_loader))\n",
    "                    dev_data, dev_target = dev_data.to(device), dev_target.to(device)\n",
    "                    output = rnn(dev_data)\n",
    "                    loss = criterion(output, dev_target)\n",
    "                    losses_dev.append(loss.data.item())\n",
    "                    \n",
    "                    \n",
    "        print()\n",
    "        acc_dev = getEfficience(rnn, dev_loader)*100\n",
    "        if acc_dev > max_acc_dev:\n",
    "            max_acc_dev = acc_dev\n",
    "            best_rnn = rnn\n",
    "            pos_best_rnn = epoch\n",
    "        \n",
    "        print(\"Dev set: accuracy: \" + str(acc_dev) + \"% | max acc: \" + str(max_acc_dev)+\"%\")\n",
    "        print()\n",
    "    rnn = best_rnn\n",
    "    # Return losses list, you can print them later if you want\n",
    "    return {\"losses_train\":losses_train, \"losses_dev\":losses_dev, \"pos_best\":pos_best_rnn+1, \"best_rnn\":best_rnn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2318888afd32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m           nb_neurons=hidden_size, nb_outputs=nb_output, learning_rate=lr)\n\u001b[1;32m      7\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mseeding_random\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "rnn = RNN(nb_inputs = len(training_set.word_list), nb_layers=nb_hidd_lay,\n",
    "          nb_neurons=hidden_size, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "with torch.enable_grad():\n",
    "    job = learn(rnn, train_loader, nb_epochs)\n",
    "    losses_train = job[\"losses_train\"]\n",
    "    losses_dev = job[\"losses_dev\"]\n",
    "    pos_best_rnn = job[\"pos_best\"]\n",
    "    rnn = job[\"best_rnn\"]\n",
    "    print(\"Done :)\")\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Learned in \" + str(end_time - begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def update_losses(smooth=1):\n",
    "    x_train = np.linspace(0, len(losses_train), len(losses_train))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax_train = fig.add_subplot(1,1,1)\n",
    "    cnn_line_train, = ax_train.plot(x_train, losses_train)\n",
    "    cnn_line_train.set_ydata(savgol_filter(losses_train, smooth, 3))\n",
    "    \n",
    "    if great_analysis:\n",
    "        x_dev = np.linspace(0, len(losses_dev), len(losses_dev))\n",
    "        ax_dev = fig.add_subplot(1,1,1)\n",
    "        cnn_line_dev, = ax_dev.plot(x_dev, losses_dev)\n",
    "        cnn_line_dev.set_ydata(savgol_filter(losses_dev, smooth, 3))\n",
    "    \n",
    "interact(update_losses, smooth=(5, 500, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Congratulations!\")\n",
    "\n",
    "rnn.eval()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_train = getEfficience(rnn, train_loader)*100\n",
    "\n",
    "print(\"On the training set:\")\n",
    "print(\"Corrects: \" + str(correct_train) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_dev = getEfficience(rnn, dev_loader)*100\n",
    "\n",
    "print(\"On the dev set:\")\n",
    "print(\"Corrects: \" + str(correct_dev) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_test = getEfficience(rnn, test_loader)*100\n",
    "\n",
    "mean_entropies = -1\n",
    "\n",
    "print(\"On the test set:\")\n",
    "print(\"Moyenne des entropies: \" + str(mean_entropies))\n",
    "print(\"Corrects: \" + str(correct_test) + \"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "inputs = nb_input\n",
    "if inputs == -1:\n",
    "    inputs = len(training_set.word_list)-3\n",
    "\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(inputs)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(nb_hidd_lay)\n",
    "      +\"\\t\"+str(hidden_size)+\"\\t\"+str(nb_batchs)+\"\\t\\t\"+str(mean_entropies)+\"\\t\"+str(pos_best_rnn)\n",
    "      +\"\\t\"+str(correct_train)+\"%\\t\"+str(correct_dev)+\"%\\t\"+str(correct_test)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
