{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 1700 # write -1 if you want every words\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 8\n",
    "hidden_size = 10\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 80\n",
    "\n",
    "# Random seed, don't change it if you don't know what it is\n",
    "random_seed = 42\n",
    "\n",
    "nb_batchs = 16\n",
    "\n",
    "# How many percent of your data do you use as training set\n",
    "devLine = 0.7\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If your goal is to draw graphs\n",
    "great_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding_random():\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    \n",
    "    # Special constructor\n",
    "    # | nb_most_commons can either be the number of most common words you\n",
    "    # | want to work with, OR a list of word you want to work with\n",
    "    # If nb_most_commons == -1, then all word will count\n",
    "    \n",
    "    def __init__(self, train_data, nb_most_commons=-1):\n",
    "        questions = []\n",
    "        labels = []\n",
    "\n",
    "        # Black list\n",
    "        black_list = '\\'`[@_!#$%^&*()<>?/\\|}{~:]'\n",
    "        \n",
    "        for string in train_data:\n",
    "            question_str = []\n",
    "            for x in string.split()[1:]:\n",
    "                s = \"\"\n",
    "                for c in x:\n",
    "                    if not c in black_list:\n",
    "                        s += c\n",
    "                if not s == \"\":\n",
    "                    question_str.append(s.lower())\n",
    "                        \n",
    "            labels.append(string.split()[0])\n",
    "            questions.append(question_str)\n",
    "\n",
    "        \n",
    "        if isinstance(nb_most_commons, int):\n",
    "            # Vocabulary of unique words\n",
    "            data = []\n",
    "            for q in questions:\n",
    "                for w in q:\n",
    "                    data.append(w)\n",
    "            self.reparti_word = Counter(data)\n",
    "            \n",
    "            if nb_most_commons < 0:\n",
    "                most_commons_words = self.reparti_word.most_common(len(data))\n",
    "            else:\n",
    "                most_commons_words = self.reparti_word.most_common(nb_most_commons)\n",
    "            \n",
    "            self.word_list = list([x[0] for x in most_commons_words])\n",
    "            self.word_list.append('<bos>')\n",
    "            self.word_list.append('<eos>')\n",
    "            self.word_list.append('<unk>')\n",
    "        elif isinstance(nb_most_commons, list):\n",
    "            self.word_list = nb_most_commons\n",
    "        else:\n",
    "            print(\"ERROR: second arg is neither an int, nor a list\")\n",
    "            \n",
    "        words_array = np.array(self.word_list)\n",
    "        \n",
    "        # Add tags <bos> and <eos> to questions\n",
    "        for q in questions:\n",
    "            if q[0] != '<bos>' :\n",
    "                q.insert(0, '<bos>')\n",
    "                q.append('<eos>')\n",
    "\n",
    "        # Integer encoding with OneHotEncoder\n",
    "        words_tre = words_array.reshape(len(words_array),1)\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "        # Creating a dictionnary of word and its one hot array\n",
    "        self.words_onehoted = {}\n",
    "        for i in range(0, len(words_array)):\n",
    "            self.words_onehoted[self.word_list[i]] = onehot_encoded[i]\n",
    "\n",
    "        # One hot categories\n",
    "        self.categories_num = {}\n",
    "        self.categories_num['ABBR'] = 0 # Abbreviation\n",
    "        self.categories_num['ENTY'] = 1 # Entity\n",
    "        self.categories_num['DESC'] = 2 # Description\n",
    "        self.categories_num['HUM']  = 3 # Human\n",
    "        self.categories_num['LOC']  = 4 # Location\n",
    "        self.categories_num['NUM']  = 5 # Numeric\n",
    "\n",
    "        self.batch_data = []\n",
    "        for num_question in range(len(questions)):\n",
    "            # Construction of question_onehot list.\n",
    "            question_onehot = [self.get_onehot_word(word) for word in questions[num_question]]\n",
    "\n",
    "            # Construction of category_onehot.\n",
    "            category = labels[num_question].partition(':')[0]\n",
    "            category_onehot = self.get_num_category(category)\n",
    "            self.batch_data.append([(question_onehot), (category_onehot)])\n",
    "        \n",
    "    \n",
    "    # Function to get the corresponding one hot list for a category.\n",
    "    def get_num_category(self, category):\n",
    "        return self.categories_num[category]\n",
    "\n",
    "\n",
    "    # Function to get the corresponding one hot list for a word.\n",
    "    def get_onehot_word(self, word):\n",
    "        if word in self.words_onehoted:\n",
    "            return list(self.words_onehoted[word])\n",
    "        else:\n",
    "            return list(self.words_onehoted['<unk>'])\n",
    "\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.batch_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeding_random()\n",
    "        return self.batch_data[idx]\n",
    "    \n",
    "def pad_collate(batch):\n",
    "    max_length = max([len(q[0]) for q in batch])\n",
    "\n",
    "    inputs = torch.FloatTensor([[[0. for _ in range(len(x[0][0]))] for i in range(max_length-len(x[0]))]+x[0] for x in batch])\n",
    "    outputs = torch.LongTensor([x[1] for x in batch])\n",
    "    \n",
    "    return inputs, outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création training set...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seeding_random()\n",
    "\n",
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "data = []\n",
    "for line in file.readlines():\n",
    "    data.append(line)\n",
    "train_data = data[:round(len(data)*devLine)]\n",
    "dev_data = data[round(len(data)*devLine):]\n",
    "\n",
    "print(\"Création training set...\")\n",
    "training_set = QuestionDataset(train_data, nb_input-3)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création dev set...\")\n",
    "dev_set = QuestionDataset(dev_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création test set...\")\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)\n",
    "test_set = QuestionDataset(test_data, training_set.word_list)\n",
    "seeding_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "seeding_random()\n",
    "\n",
    "train_loader = dataloader.DataLoader(training_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "dataloader_args_notshuffle = dict(shuffle=False, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "\n",
    "dev_loader = dataloader.DataLoader(dev_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "test_loader = dataloader.DataLoader(test_set, **dataloader_args_notshuffle)\n",
    "seeding_random()\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"List of word used:\")\n",
    "print(training_set.word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition per classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "    classes = [0,0,0,0,0,0]\n",
    "    for data, target in train_loader:\n",
    "        for t in list(target):\n",
    "            t = t.item()\n",
    "            classes[t] += 1\n",
    "\n",
    "    total = sum(classes)\n",
    "    rep_classes = [c/total*100 for c in classes]\n",
    "    print(\"Répartitions des données dans les classes:\")\n",
    "    for i in range(len(rep_classes)):\n",
    "        print(\"Classe numéro \" + str(i+1) + \": \" + str(rep_classes[i]) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word occurence repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "\n",
    "    word_occ = training_set.reparti_word\n",
    "    word_occ = dict(word_occ)\n",
    "    \n",
    "    total = sum([value for key, value in training_set.reparti_word.most_common(len(training_set.reparti_word))])\n",
    "    \n",
    "    values = [sum([value for key, value in training_set.reparti_word.most_common(i+1)])/total*100 for i in range(len(training_set.reparti_word))]\n",
    "\n",
    "    x = np.linspace(0, len(values), len(values))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    cnn_line, = ax.plot(x, values)\n",
    "\n",
    "    ax.set(xlabel=\"Vocabulaire unique\", ylabel=\"Couverture en %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0., batch_first=True, nonlinearity='relu')\n",
    "        self.inter = nn.Linear(nb_neurons, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = torch.zeros(self.nb_layers, inputs.size(0), self.nb_neurons)\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        x, hn = self.rnn(inputs, h0)\n",
    "        \n",
    "        x = self.inter(hn[0])\n",
    "        #print(x)\n",
    "        #x = tensor([list(i[-1]) for i in x])\n",
    "        #print(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "#TODO\n",
    "#Entropy mean might be near to zero\n",
    "def getEntropies(rnn, batch_list):\n",
    "    entropy_list = []\n",
    "    #value, counts = np.unique(out, return_counts=True)\n",
    "    #entropy_list.append(entropy(out, base=None))\n",
    "    return [-1]\n",
    "\n",
    "\n",
    "# return correct_percent\n",
    "def getEfficience(rnn, batch_list) :\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        out = rnn(data).data\n",
    "        \n",
    "        _, predicted = torch.max(out.data, dim=1)\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return total_correct / total\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, data_loader, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_rnn = rnn\n",
    "    max_acc_dev = -1\n",
    "    pos_best_rnn = 0;\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_correct = 0\n",
    "        total_target = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            #rnn.train()\n",
    "            seeding_random()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            rnn.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Get the Accuracy\n",
    "            \n",
    "            _, predicted = torch.max(output.data, dim=1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total_correct += correct\n",
    "            total_target += target.size(0)\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 500 == 0 or batch_idx % 500 == 1 or batch_idx == len(data_loader)-1:\n",
    "                print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}\\t Accuracy: {}'.format(\n",
    "                    epoch+1,\n",
    "                    num_epochs,\n",
    "                    batch_idx * len(data), \n",
    "                    len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader), \n",
    "                    loss.data.item(),\n",
    "                    (total_correct / total_target) * 100),\n",
    "                    end='')\n",
    "                losses_train.append(loss.data.item())\n",
    "                if great_analysis:\n",
    "                    dev_data, dev_target = next(iter(dev_loader))\n",
    "                    dev_data, dev_target = dev_data.to(device), dev_target.to(device)\n",
    "                    output = rnn(dev_data)\n",
    "                    loss = criterion(output, dev_target)\n",
    "                    losses_dev.append(loss.data.item())\n",
    "                    \n",
    "                    \n",
    "        print()\n",
    "        acc_dev = getEfficience(rnn, dev_loader)*100\n",
    "        if acc_dev > max_acc_dev:\n",
    "            max_acc_dev = acc_dev\n",
    "            best_rnn = rnn\n",
    "            pos_best_rnn = epoch\n",
    "        \n",
    "        print(\"Dev set: accuracy: \" + str(acc_dev) + \"% | max acc: \" + str(max_acc_dev)+\"%\")\n",
    "        print()\n",
    "    rnn = best_rnn\n",
    "    # Return losses list, you can print them later if you want\n",
    "    return {\"losses_train\":losses_train, \"losses_dev\":losses_dev, \"pos_best\":pos_best_rnn+1, \"best_rnn\":best_rnn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "rnn = RNN(nb_inputs = len(training_set.word_list), nb_layers=nb_hidd_lay,\n",
    "          nb_neurons=hidden_size, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "with torch.enable_grad():\n",
    "    job = learn(rnn, train_loader, nb_epochs)\n",
    "    losses_train = job[\"losses_train\"]\n",
    "    losses_dev = job[\"losses_dev\"]\n",
    "    pos_best_rnn = job[\"pos_best\"]\n",
    "    rnn = job[\"best_rnn\"]\n",
    "    print(\"Done :)\")\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Learned in \" + str(end_time - begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def update_losses(smooth=1):\n",
    "    x_train = np.linspace(0, len(losses_train), len(losses_train))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax_train = fig.add_subplot(1,1,1)\n",
    "    cnn_line_train, = ax_train.plot(x_train, losses_train)\n",
    "    cnn_line_train.set_ydata(savgol_filter(losses_train, smooth, 3))\n",
    "    \n",
    "    if great_analysis:\n",
    "        x_dev = np.linspace(0, len(losses_dev), len(losses_dev))\n",
    "        ax_dev = fig.add_subplot(1,1,1)\n",
    "        cnn_line_dev, = ax_dev.plot(x_dev, losses_dev)\n",
    "        cnn_line_dev.set_ydata(savgol_filter(losses_dev, smooth, 3))\n",
    "    \n",
    "interact(update_losses, smooth=(5, 500, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Congratulations!\")\n",
    "\n",
    "rnn.eval()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_train = getEfficience(rnn, train_loader)*100\n",
    "\n",
    "print(\"On the training set:\")\n",
    "print(\"Corrects: \" + str(correct_train) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_dev = getEfficience(rnn, dev_loader)*100\n",
    "\n",
    "print(\"On the dev set:\")\n",
    "print(\"Corrects: \" + str(correct_dev) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_test = getEfficience(rnn, test_loader)*100\n",
    "\n",
    "mean_entropies = -1\n",
    "\n",
    "print(\"On the test set:\")\n",
    "print(\"Moyenne des entropies: \" + str(mean_entropies))\n",
    "print(\"Corrects: \" + str(correct_test) + \"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "inputs = nb_input\n",
    "if inputs == -1:\n",
    "    inputs = len(training_set.word_list)-3\n",
    "\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(inputs)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(nb_hidd_lay)\n",
    "      +\"\\t\"+str(hidden_size)+\"\\t\"+str(nb_batchs)+\"\\t\\t\"+str(mean_entropies)+\"\\t\"+str(pos_best_rnn)\n",
    "      +\"\\t\"+str(correct_train)+\"%\\t\"+str(correct_dev)+\"%\\t\"+str(correct_test)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
