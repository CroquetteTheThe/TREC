{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 1700 # write -1 if you want every words\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 8\n",
    "hidden_size = 10\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 80\n",
    "\n",
    "# Random seed, don't change it if you don't know what it is\n",
    "random_seed = 42\n",
    "\n",
    "nb_batchs = 16\n",
    "\n",
    "# How many percent of your data do you use as training set\n",
    "devLine = 0.7\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If your goal is to draw graphs\n",
    "great_analysis = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding_random():\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    \n",
    "    # Special constructor\n",
    "    # | nb_most_commons can either be the number of most common words you\n",
    "    # | want to work with, OR a list of word you want to work with\n",
    "    # If nb_most_commons == -1, then all word will count\n",
    "    \n",
    "    def __init__(self, train_data, nb_most_commons=-1):\n",
    "        questions = []\n",
    "        labels = []\n",
    "\n",
    "        # Black list\n",
    "        black_list = '\\'`[@_!#$%^&*()<>?/\\|}{~:]'\n",
    "        \n",
    "        for string in train_data:\n",
    "            question_str = []\n",
    "            for x in string.split()[1:]:\n",
    "                s = \"\"\n",
    "                for c in x:\n",
    "                    if not c in black_list:\n",
    "                        s += c\n",
    "                if not s == \"\":\n",
    "                    question_str.append(s.lower())\n",
    "                        \n",
    "            labels.append(string.split()[0])\n",
    "            questions.append(question_str)\n",
    "\n",
    "        \n",
    "        if isinstance(nb_most_commons, int):\n",
    "            # Vocabulary of unique words\n",
    "            data = []\n",
    "            for q in questions:\n",
    "                for w in q:\n",
    "                    data.append(w)\n",
    "            self.reparti_word = Counter(data)\n",
    "            \n",
    "            if nb_most_commons < 0:\n",
    "                most_commons_words = self.reparti_word.most_common(len(data))\n",
    "            else:\n",
    "                most_commons_words = self.reparti_word.most_common(nb_most_commons)\n",
    "            \n",
    "            self.word_list = list([x[0] for x in most_commons_words])\n",
    "            self.word_list.append('<bos>')\n",
    "            self.word_list.append('<eos>')\n",
    "            self.word_list.append('<unk>')\n",
    "        elif isinstance(nb_most_commons, list):\n",
    "            self.word_list = nb_most_commons\n",
    "        else:\n",
    "            print(\"ERROR: second arg is neither an int, nor a list\")\n",
    "            \n",
    "        words_array = np.array(self.word_list)\n",
    "        \n",
    "        # Add tags <bos> and <eos> to questions\n",
    "        for q in questions:\n",
    "            if q[0] != '<bos>' :\n",
    "                q.insert(0, '<bos>')\n",
    "                q.append('<eos>')\n",
    "\n",
    "        # Integer encoding with OneHotEncoder\n",
    "        words_tre = words_array.reshape(len(words_array),1)\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "        # Creating a dictionnary of word and its one hot array\n",
    "        self.words_onehoted = {}\n",
    "        for i in range(0, len(words_array)):\n",
    "            self.words_onehoted[self.word_list[i]] = onehot_encoded[i]\n",
    "\n",
    "        # One hot categories\n",
    "        self.categories_num = {}\n",
    "        self.categories_num['ABBR'] = 0 # Abbreviation\n",
    "        self.categories_num['ENTY'] = 1 # Entity\n",
    "        self.categories_num['DESC'] = 2 # Description\n",
    "        self.categories_num['HUM']  = 3 # Human\n",
    "        self.categories_num['LOC']  = 4 # Location\n",
    "        self.categories_num['NUM']  = 5 # Numeric\n",
    "\n",
    "        self.batch_data = []\n",
    "        for num_question in range(len(questions)):\n",
    "            # Construction of question_onehot list.\n",
    "            question_onehot = [self.get_onehot_word(word) for word in questions[num_question]]\n",
    "\n",
    "            # Construction of category_onehot.\n",
    "            category = labels[num_question].partition(':')[0]\n",
    "            category_onehot = self.get_num_category(category)\n",
    "            self.batch_data.append([(question_onehot), (category_onehot)])\n",
    "        \n",
    "    \n",
    "    # Function to get the corresponding one hot list for a category.\n",
    "    def get_num_category(self, category):\n",
    "        return self.categories_num[category]\n",
    "\n",
    "\n",
    "    # Function to get the corresponding one hot list for a word.\n",
    "    def get_onehot_word(self, word):\n",
    "        if word in self.words_onehoted:\n",
    "            return list(self.words_onehoted[word])\n",
    "        else:\n",
    "            return list(self.words_onehoted['<unk>'])\n",
    "\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.batch_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeding_random()\n",
    "        return self.batch_data[idx]\n",
    "    \n",
    "def pad_collate(batch):\n",
    "    max_length = max([len(q[0]) for q in batch])\n",
    "\n",
    "    inputs = torch.FloatTensor([[[0. for _ in range(len(x[0][0]))] for i in range(max_length-len(x[0]))]+x[0] for x in batch])\n",
    "    outputs = torch.LongTensor([x[1] for x in batch])\n",
    "    \n",
    "    return inputs, outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création training set...\n",
      "Done!\n",
      "Création dev set...\n",
      "Done!\n",
      "Création test set...\n",
      "Done!\n",
      "List of word used:\n",
      "['the', 'what', 'is', 'of', 'in', 'a', 'how', 's', 'was', 'who', 'to', ',', 'are', 'for', 'and', 'did', 'does', 'do', 'name', 'on', 'many', 'where', 'i', 'you', 'can', 'first', 'when', 'from', 'which', 'world', 'that', 'city', 'as', 'with', 'country', 'has', 'most', '.', 'u.s.', 'by', 'an', 'have', 'find', 'it', 'why', 'there', 'get', 'people', 'called', 'state', 'year', 'were', 'mean', 'be', 'american', 'president', 'largest', 'his', 'fear', 'two', 'at', 'war', 'new', 'its', 'origin', 'word', 'much', 'about', 'known', 'kind', 'company', 'between', 'game', 'film', 'long', 'movie', 'day', 'live', 'made', 'your', 'or', 'take', 'only', 'stand', 'man', 'best', 'book', 'tv', 'their', 'one', 'john', 'famous', 'all', 'color', 'show', 'star', 'term', 'he', 'used', 'my', 'out', 'play', 'come', 'baseball', 'invented', 'had', 'into', 'call', 'number', 'countries', 'make', 'home', 'dog', 'time', 'america', 'old', 'character', 'team', 'actor', 'three', 'nn', 'not', 'information', 'average', 'river', 'states', 'some', 'highest', 'south', 'after', 'body', 'names', 'write', 'if', 'born', 'years', 'use', 'difference', 'university', 'up', 'said', 'won', 'during', 'say', 'killed', 'played', 'us', 'four', 'work', 'black', 'english', 'become', 'united', 'died', '1', 'would', 'novel', 'song', 'named', 'wrote', 'meaning', 'computer', 'last', 'good', 'earth', 'car', 'located', 'space', 'water', 'go', 'school', 'drink', 'longest', 'different', 'common', 'sport', 'horse', 'animal', 'place', 'will', 'woman', 'king', 'been', 'battle', 'makes', 'mountain', 'mississippi', 'her', 'over', 'date', 'me', 'north', 'contains', 'system', 'national', 'top', 'way', 'address', 't', 'russian', 'times', 'actress', 'island', 'population', 'college', 'nickname', 'internet', 'popular', 'abbreviation', 'form', 'created', 'served', 'little', 'causes', 'capital', 'money', 'law', 'part', 'whose', 'history', 'portrayed', 'they', 'animals', 'through', 'west', 'craft', 'death', 'language', 'product', 'became', 'being', 'group', 'mother', 'randy', 'than', 'bowl', 'queen', 'should', 'person', 'biggest', 'die', 'california', 'food', 'great', 'other', 'start', 'life', 'air', 'office', 'like', 'ball', 'york', '2', 'cities', 'james', 'england', 'french', 'married', 'spanish', 'william', 'before', 'cnn', 'definition', 'european', 'games', 'each', 'following', 'boasts', 'bill', 'general', 'cost', 'soft', 'musical', 'kennedy', 'happened', 'line', 'feet', 'win', 'fastest', 'international', 'eat', 'more', 'f.', 'children', 'building', 'lives', 'love', 'leader', 'features', 'san', 'airport', 'title', '-', 'super', 'big', 'charles', 'moon', 'comic', 'letter', 'cards', 'canada', 'female', 'disease', 'instrument', 'percentage', 'series', 'germany', 'british', 'baby', 'prize', 'board', 'author', 'business', 'center', 'night', 'indian', 'caused', 'white', 'sometimes', 'real', 'another', 'japanese', 'indians', 'begin', 'power', 'type', 'second', 'nnp', 'sea', 'olympic', 'saw', 'seven', 'fire', 'bear', 'ever', 'park', 'latin', 'age', 'symbol', 'newspaper', 'nixon', 'lawyer', 'jaws', 'men', 'hitler', 'organization', 'house', 'element', 'someone', 'miles', 'travel', 'colors', 'role', 'television', 'london', 'st.', 'desert', 'found', 'stop', 'far', 'under', 'give', 'believe', 'blood', 'soviet', 'member', 'green', 'former', 'list', 'richard', 'oldest', 'hit', 'music', 'county', 'starred', 'five', 'security', 'bridge', 'street', 'website', 'nobel', 'boxing', 'founded', 'light', 'once', 'singing', 'million', 'words', 'africa', 'original', 'greek', 'art', 'introduced', 'ocean', 'soldiers', 'need', 'hair', 'lake', 'store', 'golf', 'built', 'rate', 'we', 'back', 'types', 'de', 'six', 'them', 'washington', 'titled', 'march', 'bible', 'gould', 'runs', 'el', 'human', 'george', 'monopoly', 'flight', 'cartoon', 'shea', 'nt', 'birds', 'islands', 'full', '0', 'sioux', 'red', 'civil', 'area', 'prime', 'never', 'dubbed', 'point', 'france', 'thing', 'jack', 'produced', 'birth', 'flag', 'fast', 'union', 'fame', '5', 'months', 'miss', 'gold', 'clock', 'cold', 'christmas', 'cover', 'look', 'education', 'know', 'asian', 'minister', 'roman', 'chinese', 'so', 'but', 'fought', 'ship', 'texas', '1984', 'web', 'size', 'tallest', 'east', 'ray', 'americans', 'whom', 'market', 'shot', 'peter', 'oil', 'spumante', 'headquarters', 'hero', 'department', 'major', 'sports', 'paid', 'tell', 'player', 'originate', '1991', 'fly', 'painted', 'came', 'rain', 'kentucky', 'thatcher', 'dead', 'tree', 'stars', 'eyes', 'received', 'jude', 'women', 'worth', 'main', 'girl', 'daughter', 'hole', 'sun', 'commercial', 'dick', 'host', 'produce', 'paper', 'amendment', 'see', 'government', 'eye', 'beach', 'celebrated', 'schools', 'kid', 'league', 'watch', 'painting', 'historical', 'wine', 'reims', 'films', 'motto', 'chemical', 'assassinated', 'standard', 'favorite', 'setting', 'stock', 'wear', 'patent', '1899', 'reason', 'comedian', 'dt', 'diego', 'products', 'presidential', 'hands', 'writer', 'sign', 'considered', 'god', 'tennis', 'ice', 'current', 'wall', 'bowling', 'father', 'off', 'remove', 'empire', 'songs', 'pearl', 'social', 'los', 'poem', 'based', 'electric', 'around', 'own', 'medical', 'characters', 'forest', 'berlin', 'species', 'speed', 'shakespeare', 'chocolate', 'this', 'lived', 'la', 'artist', 'brothers', 'card', 'emperor', 'winter', '3', 'down', 'career', 'numbers', '11', 'constitution', 'opera', 'angeles', 'without', 'serve', 'head', 'beatles', 'books', 'often', 'singer', 'mrs.', 'rights', 'desmond', 'students', 'cd', 'build', 'vietnam', 'weight', 'followed', 'jean', 'acid', 'ii', 'run', 'inspired', 'harbor', 'classical', 'henry', 'free', 'massachusetts', 'lyrics', 'put', 'video', 'balls', 'steven', 'high', 'equal', 'sold', 'starring', 'secretary', 'rock', 'week', 'magic', 'story', 'johnny', 'square', 'council', 'create', 'hand', 'function', 'vegas', '8', 'spielberg', 'governor', 'letters', 'featured', 'mount', 'took', 'irish', 'creature', '10', 'bomb', 'band', 'rascals', 'nations', 'spoken', 'spy', 'beer', 'bond', 'india', 'e-mail', 'while', 'poet', 'oceans', 'export', 'famed', 'always', 'cowboy', 'family', 'jackson', 'because', 'rule', 'bounty', 'pope', 'himself', 'basketball', 'african', 'done', 'michelangelo', 'alaska', 'expression', 'turn', '19', 'she', 'field', 'telephone', 'loss', 'radio', 'brand', 'nature', 'maurizio', 'golden', 'lakes', 'ask', 'football', 'europe', 'century', 'bureau', 'las', 'read', 'golfer', 'dc', 'claim', 'japan', 'inside', 'pellegrin', 'tiger', 'income', 'van', 'sings', 'december', 'central', 'twins', 'elements', 'ireland', 'small', 'silly', 'machines', '15', 'astronauts', 'plant', 'online', 'code', 'buried', 'written', 'broadway', 'olympics', 'nine', 'players', 'course', 'cross', 'tale', 'rivers', 'depression', 'religion', 'birthday', 'border', 'clean', 'events', 'bottle', 'claimed', 'gamble', 'wage', 'broken', 'toll', 'cancer', 'santa', 'grow', 'swimming', 'prince', 'corpus', 'stole', 'jane', 'eggs', 'logan', 'level', '21', 'hollywood', 'brown', 'languages', 'party', 'procter', 'middle', 'literary', 'project', 'paint', 'perfect', 'leading', 'fuel', 'ohio', 'aids', 'same', 'minimum', 'italian', 'goodall', 'russia', 'operating', '7', 'discovered', 'yellow', 'plays', 'oscar', 'told', 'magazine', 'watergate', 'thunder', 'conference', 'mountains', '6', 'jersey', 'season', 'sleep', 'swimmer', 'promote', 'boy', 'cat', 'produces', 'went', 'published', 'bee', 'purchase', 'acted', 'every', 'just', 'strip', 'peace', 'microsoft', 'tokyo', 'least', 'ad', 'directed', 'independence', 'drive', 'tuberculosis', 'pennsylvania', 'pole', 'costner', 'label', 'award', 'stone', 'young', 'louis', 'tom', 'journal', 'classic', 'christian', '1939', 'kevin', 'claims', 'taste', 'phone', 'clothes', 'am', 'putty', 'fifth', 'rum', 'beers', 'ten', 'jewish', '1965', 'qigong', 'appeared', 'also', 'jimmy', 'receive', 'fox', 'reach', 'outside', 'file', 'son', 'clothing', 'answers', 'investigation', 'chicken', 'blind', 'site', 'occupation', 'chicago', 'l.a.', 'contact', 'hearing', 'criminal', 'gay', 'email', 'end', 'apples', 'composer', 'triangle', 'declared', 'records', 'continent', '1969', 'll', 'developed', 'correct', 'awarded', 'museum', 'victoria', 'questions', 'event', 'missouri', 'poker', 'tube', 'ages', 'mozambique', 'energy', 'bay', 'club', 'cork', 'railroad', 'cells', 'bob', 'reign', 'sons', 'charlie', 'degrees', 'answers.com', 'face', 'battery', 'election', 'child', 'meant', 'page', 'these', 'folic', 'orange', 'raise', '1994', 'itself', 'cube', 'hydrogen', '1980', 'position', 'appear', 'any', 'southern', 'robert', 'steel', 'delaware', 'titanic', '12', 'next', 'set', 'fred', 'jesus', 'must', 'planted', 'championship', 'simpsons', 'rogers', 'act', 'milk', 'against', 'glass', 'bone', 'cash', 'church', 'pacific', 'callosum', 'executed', 'master', 'tour', 'camp', 'occur', 'mark', 'chemicals', 'ford', '1967', 'price', '1972', 'gas', 'nicknamed', 'professor', 'days', 'florence', 'egg', 'iq', 'fish', 'eleven', 'score', 'leave', 'sells', 'november', 'caribbean', 'elected', 'dogtown', 'buy', 'maker', 'growing', 'stage', 'weapon', 'male', 'facial', 'exist', 'question', 'mercury', 'salt', 'northernmost', 'captain', 'c', 'woodrow', 'daily', 'getting', 'members', 'wilson', 'jones', 'successful', 'k', 'holds', 'nadia', 'intercourse', 'gives', '1963', 'arctic', 'rocky', 'republic', 'roll', 'formed', 'muppets', 'justice', 'attack', 'o', 'case', 'no', 'peak', '1983', 'mouth', 'associated', 'andrew', 'pregnancy', 'australia', 'side', 'hearst', 'astronaut', 'autobiography', 'ben', 'order', 'away', 'comaneci', 'beat', 'pass', 'greatest', 'substance', 'right', 'dickens', 'ibm-compatible', 'review', 'aspartame', 'castle', 'upon', '6th', 'record', 'ears', 'low', 'manned', 'tax', 'may', 'army', 'johnson', 'court', 'design', 'seen', 'gate', 'hold', 'troops', 'purpose', 'town', 'technique', 'china', 'bird', 'producer', 'include', 'quit', 'glitters', '19th-century', 'tried', 'revolution', 'behind', 'windsor', 'voice', 'cigarette', 'syndrome', 'horses', 'safe', 'landing', 'marvel', 'mayor', 'detective', 'tower', 'conditioning', 'zealand', 'trip', 'inch', 'takes', 'writing', 'pound', 'ways', 'husband', 'provider', 'brazil', 'soap', 'creator', 'turned', 'snow', 'then', 'robinson', 'citizen', 'commonly', 'mutiny', 'humans', 'hate', 'plan', 'association', 'congressman', '22', 'danube', 'balance', 'rose', 'federal', '16th', 'mile', 'wolfe', 'still', 'convicted', 'lead', 'soccer', 'various', 'blue', 'spice', 'dry', 'gates', 'dakota', 'popeye', 'shows', 'insurance', 'mary', 'august', 'kept', 'columbus', 'cocktail', '1998', 'invent', 'monkey', 'steps', 'gandhi', 'fans', 'pop', 'mosquito', 'wash', 'cup', 'ride', 'closest', 'fortune', 'romans', 'objects', 'points', 'working', 'atlantic', 'pro', 'flintstones', 'community', 'sitcom', 'sees', 'arnold', 'model', 'cartoons', 'now', 'percent', 'fare', 'tony', 'arma', 'society', 'novelist', 'michael', 'nationality', 'restaurant', 'daycare', 'using', 'powerful', 'suffer', 'publish', 'got', 'collect', 'manufacturer', 'wife', 'cucumber', 'labels', 'relationship', 'left', 'buffalo', 'seaport', 'louie', 'royal', 'want', 'let', 'vhs', 'areas', 'theme', 'represented', 'literature', 'success', 'races', 'comes', 'browns', 'clouds', 'sensitive', 'stewart', 'peanut', 'heart', 'started', 'aaron', 'einstein', 'cream', 'broadcasting', 'pizza', 'yahoo', 'articles', 'near', 'describe', 'contract', 'mexico', 'profession', 'stereo', 'cooler', 'on-line', 'expectant', 'powell', 'martin', 'winnie', 'borders', 'going', 'caffeine', 'northeast', 'roosevelt', 'jockey', 'abbreviated', 'bought', 'root', 'latitude', 'tourist', 'piggy', 'patented', 'thomas', 'minor', 'nevada', 'normal', 'worst', 'hulk', 'nazi', 'circle', 'twice', 'hazmat', 'rocks', 'longitude', '1942', 'muffins', 'congress', 'sex', 'sang', 'lowest', 'spent', '1989', 'attractions', 'having', '1920s', ';', 'skin', 'pounds', 'keep', 'occupy', 'third', 'mouse', 'wheel', 'fungal', 'early', 'hat', 'mr.', 'program', 'deal', 'sheep', 'flies', 'drug', '13', 'virgin', 'contain', 'reading', 'incredible', 'minutes', 'contest', 'example', 'david', 'box', 'canal', 'coach', 'ends', 'anniversary', 'florida', 'confederate', 'brain', 'bull', 'korea', 'peachy', 'pull-tab', 'israel', 'painter', 'firm', 'italy', 'colin', 'allowed', 'calories', 'statistics', 'fingers', 'bug', 'tornado', 'define', 'degas', 'd.c.', 'academy', 'temple', 'significant', 'boys', 'committee', 'something', 'release', 'network', 'sort', 'lee', 'source', 'killer', '1973', 'ago', 'houses', 'typical', 'oat', 'd.', 'along', 'nothing', 'assassination', 'comics', 'dictator', 'road', 'venus', 'tie', 'tutu', 'don', 'crop', 'appearance', 'infection', 'cosmology', 'january', 'sharp', '1978', 'lincoln', 'album', 'trees', 'race', 'generator', 'sperm', '1960', 'medium', 'disabilities', 'liberty', 'daughters', 'research', 'planet', 'pooh', 'myrtle', 'un', 'michigan', 'mission', 'natural', 'mormons', 'battles', 'sound', 'director', 'celebrities', 'murder', 'windows', 'presidency', 'official', 'several', 'playing', 'bar', 'superman', 'founder', 'log', 'fired', 'coffee', 'airports', 'process', 'landed', '1797-185', 'nuclear', 'georgia', 'owns', 'fein', 'brooks', 'prophet', 'doonesbury', 'join', 'dew', 'asked', 'july', 'maid-rites', 'saturday', 'regarding', 'seattle', 'goddess', 'pitcher', 'fresh', 'display', 'our', 'increase', 'led', 'disc', 'teenager', 'muhammad', 'sink', 'here', 'laugh-in', 'highway', 'serving', 'wears', 'yankees', '49', 'monet', 'kids', 'western', 'background', 'correctly', '1981', 'canyon', 'fatal', 'burned', 'clip', 'flags', 'sexual', 'likely', 'canadian', 'harry', 'pink', 'stopped', 'saint', 'sinn', 'addresses', 'happen', 'automobile', 'lady', 'geese', 'iron', 'athlete', 'seized', 'affect', 'signature', 'werewolf', 'calculator', 'apollo', 'italians', 'mccarren', 'beauty', 'nazis', 'vietnamese', 'bastille', 'bars', 'oz', 'colored', 'champions', 'stamp', 'adult', 'kuwait', 'boat', 'less', 'wars', 'camaro', 'nicholas', '5th', 'count', 'heavily', 'large', 'bet', 'close', 'rubber', 'prewitt', 'wines', 'markets', 'scholar', 'mao', 'l.', 'hitting', 'pressure', 'caffeinated', 'andy', 'beaver', 'companies', 'command', 'gulf', 'responsible', 'al', 'instead', 'stuart', 'killing', 'enough', 'cars', 'motors', 'aldrin', 'baking', 'included', 'fictional', 'typewriter', 'adventures', 'catholic', 'wonder', 'vacuum', 'exchange', 'dam', 'medicine', 'scrabble', 'brought', 'month', 'claus', 'hockey', 'rubik', 'lovers', 'arch', 'marked', 'chronic', 'easter', 'festival', 'police', 'physical', 'handheld', 'holes', 'engine', 'ads', 'bees', 'virtual', 'trial', 'explorer', 'holy', 'ring', 'resource', 'attend', 'feature', 'terrorist', 'toy', 'seas', 'bibliography', 'railway', 'cocaine', 'virginia', 'attacks', 'madonna', 'direct', 'nns', 'millennium', 'honorary', 'eternity', 'held', 'bombay', 'beethoven', 'stands', 'future', 'native', 'marx', 'bears', 'inventor', 'birthstone', 'closing', '24', 'alley', 'began', 'lord', 'silver', 'angels', 'iii', 'vbp', 'match', 'station', 'follow', 'individuals', 'sing', 'wonders', 'force', 'could', 'paso', 'maiden', 'zip', 'electricity', 'christopher', 'inches', 'enter', 'superstar', 'database', 'chairman', 'eruption', 'recorded', 'cage', 'strong', 'best-seller', 'dna', 'tourists', 'bulls', 'j.', 'lose', 'range', 'chief', 'hamblen', 'ranger', 'lobster', 'alice', 'valley', 'jerry', 'benny', 'shuttle', 'contemptible', 'creeps', 'estate', 'numerals', 'spelling', 'vermouth', 'ioc', 'clark', 'anthony', 'mammal', 'bush', 'establish', 'genesis', 'dr.', 'begins', 'handle', 'rich', 'dumpty', 'forces', 'protein', 'sodium', 'barbados', 'no.', 'gods', 'lauren', 'owned', 'flavor', 'belgium', 'havoc', 'clay', 'stadium', '1982', 'teddy', 'attempts', 'requirement', 'determine', 'gained', 'universe', 'butler', 'mckinley', 'scoundrel', 'wings', 'causing', 'marlowe', 'well', 'caesar', 'ended', 'approximate', 'colony', 'total', 'pan', 'grand', 'pairs', 'theory', 'develop', '16', 'bronze', 'performer', 'artificial', 'williams', 'architecture', 'debut', 'efficient', '197', 'toes', 'unaccounted', '18', 'involved', 'private', 'plants', 'bacall', 'met', 'equipment', 'dentist', 'floor', 'housewife', 'f', 'broadcast', 'constellation', 'fourth', 'movement', 'paul', 'worms', 'whip', 'conrad', 'animated', 'garry', 'sisters', 'razor', 'gene', 'betting', 'value', 'billy', 'judson', 'scarlett', 'rome', 'gave', 'singles', 'deer', 'spain', 'carl', 'tend', 'rolling', 'alphabet', 'j.r.r.', 'february', 'possession', 'dealt', 'era', '<bos>', '<eos>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seeding_random()\n",
    "\n",
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "data = []\n",
    "for line in file.readlines():\n",
    "    data.append(line)\n",
    "train_data = data[:round(len(data)*devLine)]\n",
    "dev_data = data[round(len(data)*devLine):]\n",
    "\n",
    "print(\"Création training set...\")\n",
    "training_set = QuestionDataset(train_data, nb_input-3)\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création dev set...\")\n",
    "dev_set = QuestionDataset(dev_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Création test set...\")\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)\n",
    "test_set = QuestionDataset(test_data, training_set.word_list)\n",
    "seeding_random()\n",
    "\n",
    "# Création du DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "seeding_random()\n",
    "\n",
    "train_loader = dataloader.DataLoader(training_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "dataloader_args_notshuffle = dict(shuffle=False, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)\n",
    "\n",
    "dev_loader = dataloader.DataLoader(dev_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "test_loader = dataloader.DataLoader(test_set, **dataloader_args_notshuffle)\n",
    "seeding_random()\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"List of word used:\")\n",
    "print(training_set.word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repartition per classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "    classes = [0,0,0,0,0,0]\n",
    "    for data, target in train_loader:\n",
    "        for t in list(target):\n",
    "            t = t.item()\n",
    "            classes[t] += 1\n",
    "\n",
    "    total = sum(classes)\n",
    "    rep_classes = [c/total*100 for c in classes]\n",
    "    print(\"Répartitions des données dans les classes:\")\n",
    "    for i in range(len(rep_classes)):\n",
    "        print(\"Classe numéro \" + str(i+1) + \": \" + str(rep_classes[i]) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word occurence repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if great_analysis:\n",
    "\n",
    "    word_occ = training_set.reparti_word\n",
    "    word_occ = dict(word_occ)\n",
    "    \n",
    "    total = sum([value for key, value in training_set.reparti_word.most_common(len(training_set.reparti_word))])\n",
    "    \n",
    "    values = [sum([value for key, value in training_set.reparti_word.most_common(i+1)])/total*100 for i in range(len(training_set.reparti_word))]\n",
    "\n",
    "    x = np.linspace(0, len(values), len(values))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    cnn_line, = ax.plot(x, values)\n",
    "\n",
    "    ax.set(xlabel=\"Vocabulaire unique\", ylabel=\"Couverture en %\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0., batch_first=True, nonlinearity='relu')\n",
    "        self.inter = nn.Linear(nb_neurons, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = torch.zeros(self.nb_layers, inputs.size(0), self.nb_neurons)\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        x, hn = self.rnn(inputs, h0)\n",
    "        \n",
    "        x = self.inter(hn[0])\n",
    "        #print(x)\n",
    "        #x = tensor([list(i[-1]) for i in x])\n",
    "        #print(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "#TODO\n",
    "#Entropy mean might be near to zero\n",
    "def getEntropies(rnn, batch_list):\n",
    "    entropy_list = []\n",
    "    #value, counts = np.unique(out, return_counts=True)\n",
    "    #entropy_list.append(entropy(out, base=None))\n",
    "    return [-1]\n",
    "\n",
    "\n",
    "# return correct_percent\n",
    "def getEfficience(rnn, batch_list) :\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        out = rnn(data).data\n",
    "        \n",
    "        _, predicted = torch.max(out.data, dim=1)\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "    return total_correct / total\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, data_loader, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses_train = []\n",
    "    losses_dev = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    best_rnn = rnn\n",
    "    max_acc_dev = -1\n",
    "    pos_best_rnn = 0;\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_correct = 0\n",
    "        total_target = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            #rnn.train()\n",
    "            seeding_random()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            rnn.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Get the Accuracy\n",
    "            \n",
    "            _, predicted = torch.max(output.data, dim=1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total_correct += correct\n",
    "            total_target += target.size(0)\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 500 == 0 or batch_idx % 500 == 1 or batch_idx == len(data_loader)-1:\n",
    "                print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}\\t Accuracy: {}'.format(\n",
    "                    epoch+1,\n",
    "                    num_epochs,\n",
    "                    batch_idx * len(data), \n",
    "                    len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader), \n",
    "                    loss.data.item(),\n",
    "                    (total_correct / total_target) * 100),\n",
    "                    end='')\n",
    "                losses_train.append(loss.data.item())\n",
    "                if great_analysis:\n",
    "                    dev_data, dev_target = next(iter(dev_loader))\n",
    "                    dev_data, dev_target = dev_data.to(device), dev_target.to(device)\n",
    "                    output = rnn(dev_data)\n",
    "                    loss = criterion(output, dev_target)\n",
    "                    losses_dev.append(loss.data.item())\n",
    "                    \n",
    "                    \n",
    "        print()\n",
    "        acc_dev = getEfficience(rnn, dev_loader)*100\n",
    "        if acc_dev > max_acc_dev:\n",
    "            max_acc_dev = acc_dev\n",
    "            best_rnn = rnn\n",
    "            pos_best_rnn = epoch\n",
    "        \n",
    "        print(\"Dev set: accuracy: \" + str(acc_dev) + \"% | max acc: \" + str(max_acc_dev)+\"%\")\n",
    "        print()\n",
    "    rnn = best_rnn\n",
    "    # Return losses list, you can print them later if you want\n",
    "    return {\"losses_train\":losses_train, \"losses_dev\":losses_dev, \"pos_best\":pos_best_rnn+1, \"best_rnn\":best_rnn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 1/80 [10800/10816 (100%)]\t Loss: 1.550068\t Accuracy: 26.479289940828405\n",
      "Dev set: accuracy: 30.97497842968076% | max acc: 30.97497842968076%\n",
      "\n",
      " Train Epoch: 2/80 [10800/10816 (100%)]\t Loss: 1.563617\t Accuracy: 41.05954142011834\n",
      "Dev set: accuracy: 49.13718723037101% | max acc: 49.13718723037101%\n",
      "\n",
      " Train Epoch: 3/80 [10800/10816 (100%)]\t Loss: 1.383327\t Accuracy: 59.94822485207101\n",
      "Dev set: accuracy: 59.98705780845557% | max acc: 59.98705780845557%\n",
      "\n",
      " Train Epoch: 4/80 [10800/10816 (100%)]\t Loss: 1.337113\t Accuracy: 67.15051775147928\n",
      "Dev set: accuracy: 63.114754098360656% | max acc: 63.114754098360656%\n",
      "\n",
      " Train Epoch: 5/80 [10800/10816 (100%)]\t Loss: 1.327922\t Accuracy: 71.78254437869822\n",
      "Dev set: accuracy: 70.66436583261432% | max acc: 70.66436583261432%\n",
      "\n",
      " Train Epoch: 6/80 [10800/10816 (100%)]\t Loss: 1.269686\t Accuracy: 80.35318047337277\n",
      "Dev set: accuracy: 74.5685936151855% | max acc: 74.5685936151855%\n",
      "\n",
      " Train Epoch: 7/80 [10800/10816 (100%)]\t Loss: 1.158052\t Accuracy: 82.79400887573965\n",
      "Dev set: accuracy: 77.24331320103538% | max acc: 77.24331320103538%\n",
      "\n",
      " Train Epoch: 8/80 [10800/10816 (100%)]\t Loss: 1.102246\t Accuracy: 84.72633136094674\n",
      "Dev set: accuracy: 76.03537532355479% | max acc: 77.24331320103538%\n",
      "\n",
      " Train Epoch: 9/80 [10800/10816 (100%)]\t Loss: 1.112041\t Accuracy: 85.82655325443787\n",
      "Dev set: accuracy: 76.33735979292493% | max acc: 77.24331320103538%\n",
      "\n",
      " Train Epoch: 10/80 [10800/10816 (100%)]\t Loss: 1.115998\t Accuracy: 86.96375739644971\n",
      "Dev set: accuracy: 79.05522001725626% | max acc: 79.05522001725626%\n",
      "\n",
      " Train Epoch: 11/80 [10800/10816 (100%)]\t Loss: 1.090686\t Accuracy: 87.20414201183432\n",
      "Dev set: accuracy: 75.69025021570319% | max acc: 79.05522001725626%\n",
      "\n",
      " Train Epoch: 12/80 [10800/10816 (100%)]\t Loss: 1.083239\t Accuracy: 87.53698224852072\n",
      "Dev set: accuracy: 76.79033649698016% | max acc: 79.05522001725626%\n",
      "\n",
      " Train Epoch: 13/80 [10800/10816 (100%)]\t Loss: 1.109825\t Accuracy: 88.4060650887574\n",
      "Dev set: accuracy: 80.88869715271785% | max acc: 80.88869715271785%\n",
      "\n",
      " Train Epoch: 14/80 [10800/10816 (100%)]\t Loss: 1.049374\t Accuracy: 88.62795857988166\n",
      "Dev set: accuracy: 81.4495254529767% | max acc: 81.4495254529767%\n",
      "\n",
      " Train Epoch: 15/80 [10800/10816 (100%)]\t Loss: 1.068215\t Accuracy: 88.58173076923077\n",
      "Dev set: accuracy: 80.52200172562553% | max acc: 81.4495254529767%\n",
      "\n",
      " Train Epoch: 16/80 [10800/10816 (100%)]\t Loss: 1.046571\t Accuracy: 88.3228550295858\n",
      "Dev set: accuracy: 81.7946505608283% | max acc: 81.7946505608283%\n",
      "\n",
      " Train Epoch: 17/80 [10800/10816 (100%)]\t Loss: 1.047888\t Accuracy: 89.52477810650888\n",
      "Dev set: accuracy: 80.86712683347713% | max acc: 81.7946505608283%\n",
      "\n",
      " Train Epoch: 18/80 [10800/10816 (100%)]\t Loss: 1.046780\t Accuracy: 89.21967455621301\n",
      "Dev set: accuracy: 82.44176013805004% | max acc: 82.44176013805004%\n",
      "\n",
      " Train Epoch: 19/80 [10800/10816 (100%)]\t Loss: 1.167609\t Accuracy: 89.2289201183432\n",
      "Dev set: accuracy: 70.51337359792925% | max acc: 82.44176013805004%\n",
      "\n",
      " Train Epoch: 20/80 [10800/10816 (100%)]\t Loss: 1.050620\t Accuracy: 88.87758875739645\n",
      "Dev set: accuracy: 81.94564279551338% | max acc: 82.44176013805004%\n",
      "\n",
      " Train Epoch: 21/80 [10800/10816 (100%)]\t Loss: 1.107619\t Accuracy: 89.5525147928994\n",
      "Dev set: accuracy: 81.85936151855047% | max acc: 82.44176013805004%\n",
      "\n",
      " Train Epoch: 22/80 [10800/10816 (100%)]\t Loss: 1.106718\t Accuracy: 90.57877218934911\n",
      "Dev set: accuracy: 80.80241587575496% | max acc: 82.44176013805004%\n",
      "\n",
      " Train Epoch: 23/80 [10800/10816 (100%)]\t Loss: 1.047209\t Accuracy: 89.86686390532545\n",
      "Dev set: accuracy: 82.5711820534944% | max acc: 82.5711820534944%\n",
      "\n",
      " Train Epoch: 24/80 [10800/10816 (100%)]\t Loss: 1.044712\t Accuracy: 91.20747041420118\n",
      "Dev set: accuracy: 83.08886971527178% | max acc: 83.08886971527178%\n",
      "\n",
      " Train Epoch: 25/80 [10800/10816 (100%)]\t Loss: 1.045729\t Accuracy: 89.2289201183432\n",
      "Dev set: accuracy: 83.26143226919758% | max acc: 83.26143226919758%\n",
      "\n",
      " Train Epoch: 26/80 [10800/10816 (100%)]\t Loss: 1.045519\t Accuracy: 91.0780325443787\n",
      "Dev set: accuracy: 83.75754961173425% | max acc: 83.75754961173425%\n",
      "\n",
      " Train Epoch: 27/80 [10800/10816 (100%)]\t Loss: 1.067606\t Accuracy: 87.3798076923077\n",
      "Dev set: accuracy: 83.7144089732528% | max acc: 83.75754961173425%\n",
      "\n",
      " Train Epoch: 28/80 [10800/10816 (100%)]\t Loss: 1.044814\t Accuracy: 91.5680473372781\n",
      "Dev set: accuracy: 83.95168248490079% | max acc: 83.95168248490079%\n",
      "\n",
      " Train Epoch: 29/80 [10800/10816 (100%)]\t Loss: 1.044734\t Accuracy: 91.70673076923077\n",
      "Dev set: accuracy: 83.84383088869716% | max acc: 83.95168248490079%\n",
      "\n",
      " Train Epoch: 30/80 [10800/10816 (100%)]\t Loss: 1.044738\t Accuracy: 90.89312130177515\n",
      "Dev set: accuracy: 83.64969801553063% | max acc: 83.95168248490079%\n",
      "\n",
      " Train Epoch: 31/80 [10800/10816 (100%)]\t Loss: 1.044291\t Accuracy: 91.42936390532545\n",
      "Dev set: accuracy: 83.7144089732528% | max acc: 83.95168248490079%\n",
      "\n",
      " Train Epoch: 32/80 [10800/10816 (100%)]\t Loss: 1.045637\t Accuracy: 87.8328402366864\n",
      "Dev set: accuracy: 83.64969801553063% | max acc: 83.95168248490079%\n",
      "\n",
      " Train Epoch: 33/80 [10800/10816 (100%)]\t Loss: 1.044542\t Accuracy: 91.49408284023669\n",
      "Dev set: accuracy: 83.64969801553063% | max acc: 83.95168248490079%\n",
      "\n",
      " Train Epoch: 34/80 [10800/10816 (100%)]\t Loss: 1.125065\t Accuracy: 90.20894970414201\n",
      "Dev set: accuracy: 66.39344262295081% | max acc: 83.95168248490079%\n",
      "\n",
      " Train Epoch: 35/80 [10800/10816 (100%)]\t Loss: 1.106563\t Accuracy: 91.20747041420118\n",
      "Dev set: accuracy: 77.0060396893874% | max acc: 83.95168248490079%\n",
      "\n",
      " Train Epoch: 36/80 [10800/10816 (100%)]\t Loss: 1.044091\t Accuracy: 91.84541420118343\n",
      "Dev set: accuracy: 84.21052631578947% | max acc: 84.21052631578947%\n",
      "\n",
      " Train Epoch: 37/80 [10800/10816 (100%)]\t Loss: 1.044161\t Accuracy: 91.4478550295858\n",
      "Dev set: accuracy: 84.36151855047454% | max acc: 84.36151855047454%\n",
      "\n",
      " Train Epoch: 38/80 [10800/10816 (100%)]\t Loss: 1.094904\t Accuracy: 92.0673076923077\n",
      "Dev set: accuracy: 84.36151855047454% | max acc: 84.36151855047454%\n",
      "\n",
      " Train Epoch: 39/80 [10800/10816 (100%)]\t Loss: 1.044270\t Accuracy: 91.74371301775149\n",
      "Dev set: accuracy: 83.15358067299397% | max acc: 84.36151855047454%\n",
      "\n",
      " Train Epoch: 40/80 [10800/10816 (100%)]\t Loss: 1.481558\t Accuracy: 85.53069526627219\n",
      "Dev set: accuracy: 62.14408973252804% | max acc: 84.36151855047454%\n",
      "\n",
      " Train Epoch: 41/80 [10800/10816 (100%)]\t Loss: 1.097160\t Accuracy: 86.81582840236686\n",
      "Dev set: accuracy: 83.13201035375324% | max acc: 84.36151855047454%\n",
      "\n",
      " Train Epoch: 42/80 [10800/10816 (100%)]\t Loss: 1.107918\t Accuracy: 92.3909023668639\n",
      "Dev set: accuracy: 73.74892148403796% | max acc: 84.36151855047454%\n",
      "\n",
      " Train Epoch: 43/80 [10800/10816 (100%)]\t Loss: 1.046161\t Accuracy: 90.71745562130178\n",
      "Dev set: accuracy: 83.26143226919758% | max acc: 84.36151855047454%\n",
      "\n",
      " Train Epoch: 44/80 [10800/10816 (100%)]\t Loss: 1.049107\t Accuracy: 92.59430473372781\n",
      "Dev set: accuracy: 71.76445211389128% | max acc: 84.36151855047454%\n",
      "\n",
      " Train Epoch: 45/80 [10800/10816 (100%)]\t Loss: 1.044244\t Accuracy: 92.69600591715977\n",
      "Dev set: accuracy: 84.46937014667817% | max acc: 84.46937014667817%\n",
      "\n",
      " Train Epoch: 46/80 [10800/10816 (100%)]\t Loss: 1.044519\t Accuracy: 92.40014792899409\n",
      "Dev set: accuracy: 84.81449525452976% | max acc: 84.81449525452976%\n",
      "\n",
      " Train Epoch: 47/80 [10800/10816 (100%)]\t Loss: 1.044105\t Accuracy: 92.89016272189349\n",
      "Dev set: accuracy: 85.09490940465919% | max acc: 85.09490940465919%\n",
      "\n",
      " Train Epoch: 48/80 [10800/10816 (100%)]\t Loss: 1.044660\t Accuracy: 92.57581360946746\n",
      "Dev set: accuracy: 83.9732528041415% | max acc: 85.09490940465919%\n",
      "\n",
      " Train Epoch: 49/80 [10800/10816 (100%)]\t Loss: 1.045170\t Accuracy: 91.31841715976331\n",
      "Dev set: accuracy: 75.69025021570319% | max acc: 85.09490940465919%\n",
      "\n",
      " Train Epoch: 50/80 [10800/10816 (100%)]\t Loss: 1.044239\t Accuracy: 92.63128698224851\n",
      "Dev set: accuracy: 84.87920621225194% | max acc: 85.09490940465919%\n",
      "\n",
      " Train Epoch: 51/80 [10800/10816 (100%)]\t Loss: 1.044406\t Accuracy: 91.15199704142012\n",
      "Dev set: accuracy: 84.90077653149267% | max acc: 85.09490940465919%\n",
      "\n",
      " Train Epoch: 53/80 [10800/10816 (100%)]\t Loss: 1.044124\t Accuracy: 93.38942307692307\n",
      "Dev set: accuracy: 84.70664365832614% | max acc: 85.09490940465919%\n",
      "\n",
      " Train Epoch: 54/80 [10800/10816 (100%)]\t Loss: 1.043954\t Accuracy: 92.90865384615384\n",
      "Dev set: accuracy: 85.35375323554788% | max acc: 85.35375323554788%\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 55/80 [10800/10816 (100%)]\t Loss: 1.045024\t Accuracy: 93.27847633136095\n",
      "Dev set: accuracy: 76.74719585849871% | max acc: 85.35375323554788%\n",
      "\n",
      " Train Epoch: 56/80 [10800/10816 (100%)]\t Loss: 1.044058\t Accuracy: 91.89164201183432\n",
      "Dev set: accuracy: 84.01639344262296% | max acc: 85.35375323554788%\n",
      "\n",
      " Train Epoch: 57/80 [10800/10816 (100%)]\t Loss: 1.044221\t Accuracy: 93.40791420118343\n",
      "Dev set: accuracy: 84.55565142364107% | max acc: 85.35375323554788%\n",
      "\n",
      " Train Epoch: 58/80 [10800/10816 (100%)]\t Loss: 1.044283\t Accuracy: 90.14423076923077\n",
      "Dev set: accuracy: 84.59879206212251% | max acc: 85.35375323554788%\n",
      "\n",
      " Train Epoch: 59/80 [10800/10816 (100%)]\t Loss: 1.044214\t Accuracy: 92.63128698224851\n",
      "Dev set: accuracy: 85.22433132010354% | max acc: 85.35375323554788%\n",
      "\n",
      " Train Epoch: 60/80 [10800/10816 (100%)]\t Loss: 1.044342\t Accuracy: 93.40791420118343\n",
      "Dev set: accuracy: 84.4909404659189% | max acc: 85.35375323554788%\n",
      "\n",
      " Train Epoch: 61/80 [10800/10816 (100%)]\t Loss: 1.043853\t Accuracy: 93.99963017751479\n",
      "Dev set: accuracy: 85.46160483175152% | max acc: 85.46160483175152%\n",
      "\n",
      " Train Epoch: 62/80 [10800/10816 (100%)]\t Loss: 1.046815\t Accuracy: 93.10281065088756\n",
      "Dev set: accuracy: 82.87316652286454% | max acc: 85.46160483175152%\n",
      "\n",
      " Train Epoch: 63/80 [10800/10816 (100%)]\t Loss: 1.296782\t Accuracy: 93.4633875739645\n",
      "Dev set: accuracy: 54.68075927523728% | max acc: 85.46160483175152%\n",
      "\n",
      " Train Epoch: 64/80 [10800/10816 (100%)]\t Loss: 1.043787\t Accuracy: 90.68047337278107\n",
      "Dev set: accuracy: 85.67730802415876% | max acc: 85.67730802415876%\n",
      "\n",
      " Train Epoch: 65/80 [10800/10816 (100%)]\t Loss: 1.043978\t Accuracy: 93.47263313609467\n",
      "Dev set: accuracy: 84.68507333908542% | max acc: 85.67730802415876%\n",
      "\n",
      " Train Epoch: 66/80 [10800/10816 (100%)]\t Loss: 1.043870\t Accuracy: 92.83468934911244\n",
      "Dev set: accuracy: 84.8360655737705% | max acc: 85.67730802415876%\n",
      "\n",
      " Train Epoch: 67/80 [10800/10816 (100%)]\t Loss: 1.083470\t Accuracy: 92.45562130177515\n",
      "Dev set: accuracy: 84.94391716997411% | max acc: 85.67730802415876%\n",
      "\n",
      " Train Epoch: 68/80 [10800/10816 (100%)]\t Loss: 1.043828\t Accuracy: 92.87167159763314\n",
      "Dev set: accuracy: 85.15962036238136% | max acc: 85.67730802415876%\n",
      "\n",
      " Train Epoch: 69/80 [10800/10816 (100%)]\t Loss: 1.043875\t Accuracy: 93.68528106508876\n",
      "Dev set: accuracy: 85.31061259706644% | max acc: 85.67730802415876%\n",
      "\n",
      " Train Epoch: 70/80 [10800/10816 (100%)]\t Loss: 1.043886\t Accuracy: 94.47115384615384\n",
      "Dev set: accuracy: 85.87144089732529% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 71/80 [10800/10816 (100%)]\t Loss: 1.043887\t Accuracy: 94.34171597633136\n",
      "Dev set: accuracy: 84.4909404659189% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 72/80 [10800/10816 (100%)]\t Loss: 1.043845\t Accuracy: 94.17529585798816\n",
      "Dev set: accuracy: 85.48317515099222% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 73/80 [10800/10816 (100%)]\t Loss: 1.043752\t Accuracy: 94.47115384615384\n",
      "Dev set: accuracy: 85.6341673856773% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 74/80 [10800/10816 (100%)]\t Loss: 1.124205\t Accuracy: 88.8590976331361\n",
      "Dev set: accuracy: 62.359792924935284% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 75/80 [10800/10816 (100%)]\t Loss: 1.047636\t Accuracy: 82.22078402366864\n",
      "Dev set: accuracy: 80.41415012942191% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 76/80 [10800/10816 (100%)]\t Loss: 1.044193\t Accuracy: 93.05658284023669\n",
      "Dev set: accuracy: 84.81449525452976% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 77/80 [10800/10816 (100%)]\t Loss: 1.043895\t Accuracy: 93.82396449704143\n",
      "Dev set: accuracy: 85.20276100086281% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 78/80 [10800/10816 (100%)]\t Loss: 1.044005\t Accuracy: 92.99186390532545\n",
      "Dev set: accuracy: 85.11647972389991% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 79/80 [10800/10816 (100%)]\t Loss: 1.043816\t Accuracy: 94.35096153846155\n",
      "Dev set: accuracy: 85.37532355478861% | max acc: 85.87144089732529%\n",
      "\n",
      " Train Epoch: 80/80 [10800/10816 (100%)]\t Loss: 1.043758\t Accuracy: 94.54511834319527\n",
      "Dev set: accuracy: 85.61259706643658% | max acc: 85.87144089732529%\n",
      "\n",
      "Done :)\n",
      "Learned in 0:52:15.676312\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "rnn = RNN(nb_inputs = len(training_set.word_list), nb_layers=nb_hidd_lay,\n",
    "          nb_neurons=hidden_size, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "with torch.enable_grad():\n",
    "    job = learn(rnn, train_loader, nb_epochs)\n",
    "    losses_train = job[\"losses_train\"]\n",
    "    losses_dev = job[\"losses_dev\"]\n",
    "    pos_best_rnn = job[\"pos_best\"]\n",
    "    rnn = job[\"best_rnn\"]\n",
    "    print(\"Done :)\")\n",
    "    \n",
    "end_time = datetime.datetime.now()\n",
    "print(\"Learned in \" + str(end_time - begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a126db24b3434f935ea37f8a114ddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='smooth', max=500, min=5, step=2), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def update_losses(smooth=1):\n",
    "    x_train = np.linspace(0, len(losses_train), len(losses_train))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax_train = fig.add_subplot(1,1,1)\n",
    "    cnn_line_train, = ax_train.plot(x_train, losses_train)\n",
    "    cnn_line_train.set_ydata(savgol_filter(losses_train, smooth, 3))\n",
    "    \n",
    "    if great_analysis:\n",
    "        x_dev = np.linspace(0, len(losses_dev), len(losses_dev))\n",
    "        ax_dev = fig.add_subplot(1,1,1)\n",
    "        cnn_line_dev, = ax_dev.plot(x_dev, losses_dev)\n",
    "        cnn_line_dev.set_ydata(savgol_filter(losses_dev, smooth, 3))\n",
    "    \n",
    "interact(update_losses, smooth=(5, 500, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n",
      "On the training set:\n",
      "Corrects: 94.50813609467455%\n",
      "\n",
      "On the dev set:\n",
      "Corrects: 85.61259706643658%\n",
      "\n",
      "On the test set:\n",
      "Moyenne des entropies: -1\n",
      "Corrects: 79.4%\n",
      "\n",
      "A présent, tu peux copier-coller ça dans le doc sur le drive :)\n",
      "1700\t0.001\t80\t8\t10\t16\t\t-1\t70\t94.50813609467455%\t85.61259706643658%\t79.4%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Congratulations!\")\n",
    "\n",
    "rnn.eval()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_train = getEfficience(rnn, train_loader)*100\n",
    "\n",
    "print(\"On the training set:\")\n",
    "print(\"Corrects: \" + str(correct_train) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_dev = getEfficience(rnn, dev_loader)*100\n",
    "\n",
    "print(\"On the dev set:\")\n",
    "print(\"Corrects: \" + str(correct_dev) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "correct_test = getEfficience(rnn, test_loader)*100\n",
    "\n",
    "mean_entropies = -1\n",
    "\n",
    "print(\"On the test set:\")\n",
    "print(\"Moyenne des entropies: \" + str(mean_entropies))\n",
    "print(\"Corrects: \" + str(correct_test) + \"%\")\n",
    "\n",
    "print()\n",
    "\n",
    "inputs = nb_input\n",
    "if inputs == -1:\n",
    "    inputs = len(training_set.word_list)-3\n",
    "\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(inputs)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(nb_hidd_lay)\n",
    "      +\"\\t\"+str(hidden_size)+\"\\t\"+str(nb_batchs)+\"\\t\\t\"+str(mean_entropies)+\"\\t\"+str(pos_best_rnn)\n",
    "      +\"\\t\"+str(correct_train)+\"%\\t\"+str(correct_dev)+\"%\\t\"+str(correct_test)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
