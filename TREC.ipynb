{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 1000 #8414 max\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 2\n",
    "hidden_size = 3\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 10\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "train_data = []\n",
    "for line in file.readlines():\n",
    "    train_data.append(line)\n",
    "\n",
    "# Now test set\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "# Working on list_of_words\n",
    "# TODO: it doesn't work\n",
    "def lemm(phrase):\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lemmed = []\n",
    "    for word in phrase:\n",
    "        lemmed.append(lemmer.lemmatize(word, pos=\"v\"))\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(questions) = 15452\n",
      "Exemples of questions:\n",
      "[['how', 'do', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?'], ['what', 'film', 'feature', 'the', 'character', 'popeye', 'doyle', '?']]\n",
      "\n",
      "And in the test set:\n",
      "len(questions_test) = 500\n",
      "Exemples of questions:\n",
      "[['how', 'far', 'be', 'it', 'from', 'denver', 'to', 'aspen', '?'], ['what', 'county', 'be', 'modesto', ',', 'california', 'in', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Divided file into 2 list:\n",
    "# questions = list of questions \n",
    "# labels = list of labels\n",
    "\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "# Black list\n",
    "regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "\n",
    "(train_data[0]).split()[0]\n",
    "for string in train_data:\n",
    "    question_str = []\n",
    "    for x in lemm(string.split()[1:]):\n",
    "        question_str.append(x.lower())\n",
    "    labels.append(string.split()[0])\n",
    "    questions.append(question_str)\n",
    "print(\"len(questions) = \" + str(len(questions)))\n",
    "print(\"Exemples of questions:\")\n",
    "print(questions[:2])\n",
    "\n",
    "\n",
    "\n",
    "questions_test = []\n",
    "labels_test = []\n",
    "\n",
    "(test_data[0]).split()[0]\n",
    "for string in test_data:\n",
    "    question_str = []\n",
    "    for x in lemm(string.split()[1:]):\n",
    "        question_str.append(x.lower())\n",
    "    labels_test.append(string.split()[0])\n",
    "    questions_test.append(question_str)\n",
    "print()\n",
    "print(\"And in the test set:\")\n",
    "print(\"len(questions_test) = \" + str(len(questions_test)))\n",
    "print(\"Exemples of questions:\")\n",
    "print(questions_test[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to add tags begin and end to phrases list.\n",
    "# /!\\ \n",
    "# WARNING : this method need to be executed only ONE time.\n",
    "# /!\\\n",
    "def add_tag(question_list):\n",
    "    for i in range(0, len(question_list)):\n",
    "        if question_list[i][0] != '<bos>' :\n",
    "            question_list[i].insert(0, '<bos>')\n",
    "            question_list[i].append('<eos>')\n",
    "add_tag(questions)\n",
    "add_tag(questions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188317\n",
      "CPU times: user 52 ms, sys: 4 ms, total: 56 ms\n",
      "Wall time: 55 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vocabulary of unique words\n",
    "data = []\n",
    "data.append('<unk>')\n",
    "for q in questions:\n",
    "    for word in q:\n",
    "        data.append(word)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv = np.array(data)\n",
    "unik, counts = np.unique(scv,return_counts=True)\n",
    "vocabulary = {}\n",
    "for i in range(0, len(unik)):\n",
    "    vocabulary[unik[i]] = counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ill-fated': 4,\n",
       " 'elizabethian': 2,\n",
       " '1964': 8,\n",
       " 'surf': 3,\n",
       " '1832': 5,\n",
       " 'archery': 2,\n",
       " 'bars': 3,\n",
       " 'backstreet': 3,\n",
       " 'octopus': 5,\n",
       " 'leia': 4,\n",
       " 'felt': 7,\n",
       " 'bog-dwellers': 4,\n",
       " 'edentulous': 2,\n",
       " 'doesn': 5,\n",
       " 'wars': 15,\n",
       " 'bomb': 26,\n",
       " 'stole': 3,\n",
       " 'mountainous': 4,\n",
       " 'westview': 1,\n",
       " '000th': 4,\n",
       " 'igor': 2,\n",
       " 'nazis': 10,\n",
       " 'ileana': 10,\n",
       " 'gosfield': 3,\n",
       " 'stern': 2,\n",
       " '111': 1,\n",
       " 'casper': 4,\n",
       " 'inescapable': 4,\n",
       " 'pyrotechnic': 4,\n",
       " 'collabrative': 5,\n",
       " 'oust': 2,\n",
       " 'drain': 1,\n",
       " 'room': 5,\n",
       " 'contestant': 5,\n",
       " 'busiest': 8,\n",
       " 'chicago-style': 4,\n",
       " '1960': 12,\n",
       " 'commission': 4,\n",
       " 'cult': 1,\n",
       " 'prism': 2,\n",
       " 'stairway': 3,\n",
       " 'constellation': 10,\n",
       " 'follow-up': 4,\n",
       " 'winner': 3,\n",
       " 'belong': 9,\n",
       " 'pentagon': 1,\n",
       " 'mosquitoes': 1,\n",
       " 'kalahari': 8,\n",
       " 'joan': 6,\n",
       " 'ejaculation': 5,\n",
       " 'lucky': 5,\n",
       " 'oceans': 19,\n",
       " 'holmes': 8,\n",
       " 'diplomacy': 5,\n",
       " 'natural': 14,\n",
       " 'buddy': 1,\n",
       " 'clark': 11,\n",
       " 'achieve': 1,\n",
       " 'malawi': 1,\n",
       " 'roman': 27,\n",
       " 'whole': 2,\n",
       " 'primate': 3,\n",
       " 'station': 25,\n",
       " 'indiglo': 6,\n",
       " 'seize': 10,\n",
       " 'hardest': 8,\n",
       " 'bull': 14,\n",
       " 'stallone': 1,\n",
       " 'peller': 3,\n",
       " 'scandinavian': 5,\n",
       " 'philanthropist': 2,\n",
       " 'arometherapy': 5,\n",
       " 'gregorian': 1,\n",
       " 'folklore': 4,\n",
       " 'fame': 31,\n",
       " 'fireplug': 2,\n",
       " 'msg': 2,\n",
       " 'arkansas': 5,\n",
       " 'web': 30,\n",
       " 'court-martial': 3,\n",
       " 'fraze': 8,\n",
       " 'jurassic': 3,\n",
       " '387': 3,\n",
       " 'walter': 4,\n",
       " 'export': 23,\n",
       " 'disaster': 5,\n",
       " 'mystique': 3,\n",
       " 'corgi': 4,\n",
       " 'humidity': 1,\n",
       " 'spokespeople': 4,\n",
       " 'song': 76,\n",
       " 'location': 8,\n",
       " 'preferably': 2,\n",
       " 'kyriakos': 1,\n",
       " 'typewriter': 10,\n",
       " 'popeye': 13,\n",
       " 'monkey': 12,\n",
       " 'transmit': 5,\n",
       " 'grade': 2,\n",
       " 'fury': 1,\n",
       " 'slam': 4,\n",
       " 'grinch': 3,\n",
       " 'tax': 23,\n",
       " 'plastic': 9,\n",
       " 'colombia': 6,\n",
       " 'hunt': 8,\n",
       " '1880': 4,\n",
       " 'charley': 3,\n",
       " 'fictional': 9,\n",
       " 'oceania': 1,\n",
       " 'kenya': 3,\n",
       " 'legendary': 5,\n",
       " 'peak': 14,\n",
       " 'christine': 2,\n",
       " 'treehouse': 5,\n",
       " 'toy': 15,\n",
       " 'sonny': 4,\n",
       " 'swear': 1,\n",
       " 'struggle': 3,\n",
       " 'announce': 8,\n",
       " 'heaven': 8,\n",
       " 'hibernia': 6,\n",
       " 'bros.': 3,\n",
       " 'professional': 9,\n",
       " 'thumb': 5,\n",
       " 'indian': 36,\n",
       " 'skater': 3,\n",
       " 'whisky': 6,\n",
       " 'purpose': 21,\n",
       " 'nns': 9,\n",
       " 'scar-faced': 5,\n",
       " 'scholar': 10,\n",
       " 'therapy': 5,\n",
       " 'injury': 5,\n",
       " 'vegetable': 9,\n",
       " 'nabokov': 5,\n",
       " 'mortgage': 4,\n",
       " 'diet': 3,\n",
       " 'oven': 8,\n",
       " 'chinese': 30,\n",
       " 'bloom': 6,\n",
       " 'sun-blasted': 5,\n",
       " 'potter': 5,\n",
       " 'flightless': 1,\n",
       " 'choo': 6,\n",
       " 'poorly': 5,\n",
       " 'accidents': 5,\n",
       " 'court': 19,\n",
       " 'microscope': 3,\n",
       " 'racoon': 5,\n",
       " 'grant': 10,\n",
       " 'dictator': 12,\n",
       " '1%': 1,\n",
       " 'speed': 21,\n",
       " 'robert': 15,\n",
       " 'neurological': 6,\n",
       " 'down': 26,\n",
       " 'lickin': 2,\n",
       " 'powerful': 14,\n",
       " 'pesth': 2,\n",
       " 'whist': 1,\n",
       " 'cuba': 8,\n",
       " 'seal': 4,\n",
       " 'shaken': 5,\n",
       " 'mont': 3,\n",
       " 'metabolize': 4,\n",
       " 'spectacle': 2,\n",
       " 'zodiac': 2,\n",
       " 'problem': 4,\n",
       " 'budapest': 3,\n",
       " 'alexandre': 4,\n",
       " 'tailors': 2,\n",
       " 'resemble': 2,\n",
       " 'svga': 3,\n",
       " 'cook': 9,\n",
       " 'lock': 4,\n",
       " 'randolph': 7,\n",
       " 'hydrogen': 15,\n",
       " 'marvin': 5,\n",
       " 'tramp': 1,\n",
       " 'cable': 8,\n",
       " 'suzy': 1,\n",
       " 'sir': 6,\n",
       " 'recovery': 2,\n",
       " 'europeans': 1,\n",
       " 'captain': 22,\n",
       " 'practical': 2,\n",
       " 'hostages': 5,\n",
       " 'victor': 7,\n",
       " 'boxer': 8,\n",
       " 'volleyball': 1,\n",
       " 'al-farri': 5,\n",
       " 'radiographer': 1,\n",
       " 'madonna': 14,\n",
       " 'portuguese': 1,\n",
       " 'recessed-filter': 4,\n",
       " 'gretzky': 2,\n",
       " 'cotrubas': 10,\n",
       " 'lend': 3,\n",
       " 'hungry': 3,\n",
       " 'desk': 2,\n",
       " 'inoperative': 4,\n",
       " 'deliver': 5,\n",
       " 'arthur': 11,\n",
       " 'campbell': 2,\n",
       " 'guadalcanal': 2,\n",
       " 'warren': 1,\n",
       " 'sword': 2,\n",
       " 'nine': 21,\n",
       " 'psychology': 1,\n",
       " 'substance': 15,\n",
       " 'benelux': 5,\n",
       " 'surveyor': 1,\n",
       " 'fiction': 7,\n",
       " 'venture': 4,\n",
       " 'tadeus': 4,\n",
       " 'naacp': 3,\n",
       " '22': 10,\n",
       " 'roaring': 1,\n",
       " 'thickness': 1,\n",
       " 'son-in-law': 2,\n",
       " 'agreement': 3,\n",
       " 'crown-winning': 3,\n",
       " 'sed': 3,\n",
       " 'gillette': 5,\n",
       " 'cool': 3,\n",
       " 'diminutive': 6,\n",
       " 'character': 121,\n",
       " 'spangled': 3,\n",
       " 'social': 21,\n",
       " 'nature': 16,\n",
       " 'heliologist': 3,\n",
       " '14-mile': 5,\n",
       " 'hugo': 10,\n",
       " 'protestant': 4,\n",
       " 'smothers': 1,\n",
       " 'pops': 4,\n",
       " 'televise': 2,\n",
       " 't.v.': 5,\n",
       " 'visitors': 4,\n",
       " 'wolverine': 1,\n",
       " 'basilica': 3,\n",
       " 'cid': 1,\n",
       " 'genome': 1,\n",
       " 'wingspan': 5,\n",
       " 'concorde': 7,\n",
       " 'palmer': 5,\n",
       " 'rams': 3,\n",
       " 'plains': 1,\n",
       " 'hirsch': 4,\n",
       " 'prehistoric': 5,\n",
       " 'creatures': 2,\n",
       " 'sinclair': 1,\n",
       " 'repeal': 5,\n",
       " 'snap': 6,\n",
       " 'holstrum': 4,\n",
       " 'p-32': 4,\n",
       " 'approval': 4,\n",
       " 'heroine': 3,\n",
       " 'major': 39,\n",
       " 'expectancy': 9,\n",
       " 'bone': 23,\n",
       " 'stuck-out': 1,\n",
       " 'diphallic': 1,\n",
       " 'pangaea': 1,\n",
       " 'esperanto': 3,\n",
       " 'terata': 1,\n",
       " 'characteristics': 1,\n",
       " 'suffrage': 5,\n",
       " 'warner': 3,\n",
       " 'morris': 1,\n",
       " 'cinderslut': 5,\n",
       " 'me': 58,\n",
       " 'tenant': 4,\n",
       " 'taiwanese': 7,\n",
       " 'countinghouse': 1,\n",
       " 'tee': 5,\n",
       " 'astronauts': 20,\n",
       " 'detectors': 1,\n",
       " 'olympic': 35,\n",
       " 'sort': 11,\n",
       " 'various': 13,\n",
       " 'dissolve': 1,\n",
       " 'complex': 1,\n",
       " 'lifesaver': 5,\n",
       " 'gogh': 10,\n",
       " 'middle': 23,\n",
       " 'general': 59,\n",
       " 'counties': 3,\n",
       " 'carta': 9,\n",
       " 'defect': 9,\n",
       " 'tablespoon': 1,\n",
       " 'beanie': 6,\n",
       " 'snow': 16,\n",
       " 'breed': 18,\n",
       " 'footwear': 2,\n",
       " 'sisley': 5,\n",
       " 'abilities': 2,\n",
       " 'gift': 3,\n",
       " 'trophy': 6,\n",
       " 'february': 11,\n",
       " 'death': 52,\n",
       " 'lethal': 8,\n",
       " 'souls': 3,\n",
       " 'innocent': 4,\n",
       " '1.76': 2,\n",
       " 'eternity': 11,\n",
       " 'boulevard': 4,\n",
       " 'propaganda': 2,\n",
       " 'filthiest': 5,\n",
       " 'madilyn': 5,\n",
       " 'ado': 1,\n",
       " 'shetland': 1,\n",
       " 'daily': 16,\n",
       " 'source': 13,\n",
       " 'businesses': 7,\n",
       " 'spritsail': 3,\n",
       " 'cheerios': 1,\n",
       " 'grab': 8,\n",
       " 'housewarming': 2,\n",
       " 'spawn': 5,\n",
       " 'espionage': 7,\n",
       " 'fruit': 12,\n",
       " 'chile': 3,\n",
       " 'baseemen': 4,\n",
       " 'machinery': 1,\n",
       " 'socioeconomic': 1,\n",
       " 'coleman': 1,\n",
       " 'pea-soup': 5,\n",
       " 'ark': 1,\n",
       " 'overflow': 5,\n",
       " 'take': 174,\n",
       " 'infamous': 9,\n",
       " 'spears': 2,\n",
       " 'pugilist': 1,\n",
       " 'fred': 14,\n",
       " 'maria': 7,\n",
       " 'feel': 6,\n",
       " 'luis': 5,\n",
       " 'boomer': 1,\n",
       " 'category': 5,\n",
       " 'newspaper': 37,\n",
       " 'ibm': 4,\n",
       " 'tokyo': 20,\n",
       " 'scratch': 2,\n",
       " 'wordsworth': 1,\n",
       " 'johnson': 14,\n",
       " 'chino': 1,\n",
       " 'j.d.': 4,\n",
       " '2.5': 2,\n",
       " '26': 5,\n",
       " 'baseball': 105,\n",
       " 'dostoevski': 4,\n",
       " 'organs': 4,\n",
       " 'theta': 4,\n",
       " 'popcorn': 5,\n",
       " 'tracy': 7,\n",
       " 'law': 53,\n",
       " 'honor': 10,\n",
       " 'gran': 1,\n",
       " 'sondheim': 3,\n",
       " 'electoral': 6,\n",
       " 'mutombo': 11,\n",
       " 'uruguay': 1,\n",
       " 'erich': 2,\n",
       " 'qualifications': 1,\n",
       " 'expel': 4,\n",
       " 'pile': 1,\n",
       " 'architect': 6,\n",
       " 'further': 3,\n",
       " 'researches': 1,\n",
       " 'expert': 2,\n",
       " 'sinn': 13,\n",
       " 'pookie': 1,\n",
       " 'underwater': 5,\n",
       " 'win': 146,\n",
       " 'richthofen': 3,\n",
       " 'shelve': 2,\n",
       " 'iraq': 6,\n",
       " 'fathom': 8,\n",
       " 'fairy': 6,\n",
       " 'plain': 4,\n",
       " '000': 7,\n",
       " \"'70\": 1,\n",
       " 'leg': 9,\n",
       " 'creeps': 8,\n",
       " 'galaxy': 3,\n",
       " 'braille': 4,\n",
       " 'hear': 24,\n",
       " 'cosmology': 13,\n",
       " 'dem': 4,\n",
       " 'kamchatka': 5,\n",
       " 'thank-yous': 2,\n",
       " 'rubens': 4,\n",
       " 'carcinogens': 4,\n",
       " 'atmosphere': 1,\n",
       " 'infant': 4,\n",
       " 'poems': 3,\n",
       " 'crucifixion': 5,\n",
       " 'violent': 2,\n",
       " 'liverpool': 4,\n",
       " 'stagecoach': 5,\n",
       " 'liz': 3,\n",
       " 'legs': 7,\n",
       " 'shipwreck': 4,\n",
       " 'capitalism': 2,\n",
       " 'denote': 1,\n",
       " 'villain': 8,\n",
       " 'collect': 12,\n",
       " 'isosceles': 5,\n",
       " 'beverly': 1,\n",
       " 'church': 19,\n",
       " 'fort': 6,\n",
       " 'napsylate': 2,\n",
       " 'fastest-growing': 1,\n",
       " 'seattle': 10,\n",
       " 'grocer': 2,\n",
       " 'permutations': 2,\n",
       " 'ants': 2,\n",
       " 'clause': 3,\n",
       " 'consortium': 3,\n",
       " 'pomegranate': 3,\n",
       " 'ejaculate': 4,\n",
       " 'gravity': 6,\n",
       " 'tubetti': 2,\n",
       " 'shoulder': 5,\n",
       " 'disc': 11,\n",
       " 'nylon': 3,\n",
       " 'lake': 35,\n",
       " 'sullivan': 8,\n",
       " 'fossils': 5,\n",
       " 'extend': 6,\n",
       " 'stonewall': 1,\n",
       " 'syringe': 1,\n",
       " 'divers': 4,\n",
       " 'cabarnet': 3,\n",
       " 'wreckage': 4,\n",
       " 'edgar': 8,\n",
       " 'joel': 5,\n",
       " 'political': 6,\n",
       " 'gills': 1,\n",
       " 'unfamiliar': 4,\n",
       " 'philatelist': 4,\n",
       " 'stickers': 3,\n",
       " 'earthquakes': 1,\n",
       " 'bucher': 1,\n",
       " 'individuals': 10,\n",
       " 'everett': 1,\n",
       " 'minimum': 21,\n",
       " 'venus': 11,\n",
       " 'michagin': 4,\n",
       " '1912': 2,\n",
       " 'chronicle': 7,\n",
       " 'angela': 2,\n",
       " 'microcontrollers': 4,\n",
       " 'cut': 8,\n",
       " 'derby': 4,\n",
       " 'carson': 3,\n",
       " 'clockwise': 5,\n",
       " 'help': 7,\n",
       " '1951': 7,\n",
       " '1849': 2,\n",
       " 'larger': 4,\n",
       " 'maurizio': 19,\n",
       " 'release': 23,\n",
       " 'calypso': 3,\n",
       " 'dirty': 4,\n",
       " 'dirkson': 1,\n",
       " 'personas': 5,\n",
       " 'dialect': 4,\n",
       " 'settle': 3,\n",
       " 'peanut': 13,\n",
       " 'fubu': 2,\n",
       " 'reunite': 2,\n",
       " 'pictorial': 5,\n",
       " 'let': 12,\n",
       " 'bounty': 18,\n",
       " 'ready-to-eat': 1,\n",
       " 'appropriate': 2,\n",
       " 'irl': 2,\n",
       " 'bog': 10,\n",
       " 'amendment': 21,\n",
       " 'ethnological': 3,\n",
       " 'gandhi': 13,\n",
       " 'euchre': 5,\n",
       " 'luke': 7,\n",
       " 'youngman': 2,\n",
       " 'gilbert': 3,\n",
       " 'recomended': 2,\n",
       " 'hero': 25,\n",
       " 'august': 14,\n",
       " 'a_tisket': 1,\n",
       " 'mvp': 2,\n",
       " 'wilder': 5,\n",
       " 'chickens': 5,\n",
       " 'ukulele': 1,\n",
       " '600': 3,\n",
       " 'lombardi': 2,\n",
       " 'sound': 18,\n",
       " 'brown': 21,\n",
       " 'lynn': 1,\n",
       " 'serial': 4,\n",
       " 'kisser': 3,\n",
       " 'bare': 4,\n",
       " 'heineken': 1,\n",
       " 'cawdor': 1,\n",
       " \"'50s\": 1,\n",
       " 'witness': 6,\n",
       " 'bowler': 2,\n",
       " 'nostradamus': 4,\n",
       " 'ouija': 7,\n",
       " '1975': 6,\n",
       " 'weigh': 11,\n",
       " 'instructor': 2,\n",
       " 'widmark': 3,\n",
       " 'cockroaches': 5,\n",
       " 'konigsberg': 3,\n",
       " 'stinger': 1,\n",
       " 'annual': 4,\n",
       " 'rosa': 3,\n",
       " 'plantar': 5,\n",
       " 'exercise': 11,\n",
       " 'last': 85,\n",
       " 'painter': 12,\n",
       " 'batman': 1,\n",
       " 'quilt': 1,\n",
       " 'british': 43,\n",
       " 'springfield': 3,\n",
       " 'populous': 4,\n",
       " 'pajamas': 3,\n",
       " 'acquaintance': 2,\n",
       " 'kansas': 9,\n",
       " 'quantity': 5,\n",
       " 'imam': 4,\n",
       " 'prevent': 13,\n",
       " 'per': 8,\n",
       " 'sting': 2,\n",
       " 'roommates': 3,\n",
       " 'cherokee': 4,\n",
       " 'crackle': 1,\n",
       " 'relax': 4,\n",
       " 'finally': 3,\n",
       " 'j.r.r.': 8,\n",
       " 'iberian': 1,\n",
       " 'bic': 5,\n",
       " 'geckos': 1,\n",
       " 'enthalpy': 2,\n",
       " 'carpenter': 2,\n",
       " 'butch': 5,\n",
       " 'plc': 4,\n",
       " 'brave': 6,\n",
       " 'rigati': 2,\n",
       " 'assassinate': 35,\n",
       " 'self-defense': 2,\n",
       " 'register': 9,\n",
       " 'callus': 5,\n",
       " 'majority': 5,\n",
       " 'trinitrotoluene': 4,\n",
       " 'moral': 6,\n",
       " 'acne': 2,\n",
       " 'zionism': 3,\n",
       " 'tonne': 2,\n",
       " 'prosecutor': 3,\n",
       " 'nato': 5,\n",
       " 'rainstorm': 4,\n",
       " 'geoscientist': 2,\n",
       " 'dixville': 4,\n",
       " 'reinstate': 2,\n",
       " 'detail': 10,\n",
       " 'ukraine': 2,\n",
       " 'bell': 7,\n",
       " '175': 4,\n",
       " 'duel': 3,\n",
       " 'compete': 1,\n",
       " 'ancients': 2,\n",
       " '42.3': 5,\n",
       " 'strip': 37,\n",
       " 'cystic': 4,\n",
       " 'peters': 3,\n",
       " 'windshield': 1,\n",
       " 'tap': 3,\n",
       " 'missile': 4,\n",
       " 'contest': 12,\n",
       " 'kiss': 9,\n",
       " 'something': 13,\n",
       " 'devour': 4,\n",
       " 'aztecs': 1,\n",
       " 'ozone': 1,\n",
       " 'primal': 5,\n",
       " 'kai-shek': 2,\n",
       " 'rogers': 16,\n",
       " 'grunt': 1,\n",
       " 'dime': 6,\n",
       " 'argentine': 3,\n",
       " 'braun': 5,\n",
       " 'handle': 8,\n",
       " 'lindbergh': 9,\n",
       " 'wield': 1,\n",
       " 'tampa': 3,\n",
       " 'northeast': 12,\n",
       " 'diagnose': 5,\n",
       " 'vera': 1,\n",
       " 'bombay': 10,\n",
       " 'aztec': 4,\n",
       " 'rann': 2,\n",
       " 'hayden': 3,\n",
       " 'contract': 15,\n",
       " 'alvin': 5,\n",
       " 'abolish': 2,\n",
       " 'bishop': 1,\n",
       " 'post': 10,\n",
       " 'website': 34,\n",
       " 'syzygy': 2,\n",
       " 'mendelevium': 2,\n",
       " 'trail': 10,\n",
       " 'goldfinger': 2,\n",
       " 'ignore': 1,\n",
       " 'unarm': 1,\n",
       " 'immortals': 5,\n",
       " '1954': 3,\n",
       " 'colony': 10,\n",
       " 'nanometer': 5,\n",
       " 'involve': 14,\n",
       " 'romania': 3,\n",
       " 'experts': 5,\n",
       " 'bullfighting': 3,\n",
       " 'verification': 1,\n",
       " 'co-star': 2,\n",
       " 'mystical': 4,\n",
       " 'historically': 1,\n",
       " 'bagdad': 3,\n",
       " 'matter': 11,\n",
       " 'erase': 2,\n",
       " 'several': 9,\n",
       " 'proliferation': 1,\n",
       " 'dealer': 6,\n",
       " 'tammy': 7,\n",
       " 'ferry': 3,\n",
       " 'creek': 5,\n",
       " 'dogsledding': 5,\n",
       " 'famous': 129,\n",
       " 'fiji': 1,\n",
       " 'lack': 8,\n",
       " 'arm': 10,\n",
       " 'maya': 2,\n",
       " 'components': 2,\n",
       " 'show-biz': 3,\n",
       " 'cyclone': 4,\n",
       " \"'ll\": 16,\n",
       " 'chilean': 4,\n",
       " 'a.g.': 5,\n",
       " 'brightest': 5,\n",
       " 'gunpowder': 4,\n",
       " '&': 36,\n",
       " 'bestow': 1,\n",
       " 'wiper': 1,\n",
       " 'harden': 2,\n",
       " 'wonder': 14,\n",
       " 'score': 18,\n",
       " 'offices': 6,\n",
       " 'lai': 1,\n",
       " '1984': 29,\n",
       " 'robbins': 1,\n",
       " 'bullwinkle': 2,\n",
       " 'pennsylvania': 17,\n",
       " 'commanders': 4,\n",
       " 'hungary': 4,\n",
       " 'firestorm': 1,\n",
       " 'billy': 10,\n",
       " 'some': 86,\n",
       " 'japanese': 41,\n",
       " 'eunice': 5,\n",
       " 'smallest': 9,\n",
       " 'chiropodist': 2,\n",
       " 'wang': 2,\n",
       " 'bernadette': 3,\n",
       " 'base': 23,\n",
       " 'bolivia': 6,\n",
       " 'circumorbital': 4,\n",
       " 'tallest': 26,\n",
       " 'motorcycle': 5,\n",
       " '16': 10,\n",
       " 'peruvian': 1,\n",
       " 'advantage': 3,\n",
       " 'socialism': 1,\n",
       " 'development': 4,\n",
       " 'write': 195,\n",
       " '43rd': 2,\n",
       " 'specimen': 1,\n",
       " 'dollar': 11,\n",
       " 'viii': 5,\n",
       " 'spicey': 2,\n",
       " 'block': 3,\n",
       " 'amazon': 4,\n",
       " 'theo': 3,\n",
       " 'niagara': 3,\n",
       " 'canine': 6,\n",
       " 'louse': 1,\n",
       " 'naked': 5,\n",
       " 'skate': 1,\n",
       " 'alabama': 1,\n",
       " 'hernando': 1,\n",
       " 'singer': 29,\n",
       " 'laureate': 4,\n",
       " 'available': 7,\n",
       " 'browns': 10,\n",
       " 'wild': 6,\n",
       " 'need': 36,\n",
       " 'mortem': 5,\n",
       " 'fifth': 15,\n",
       " 'shady': 2,\n",
       " 'turnkey': 1,\n",
       " 'nj': 6,\n",
       " 'bentos': 4,\n",
       " 'supplement': 2,\n",
       " 'parachute': 1,\n",
       " 'monster': 8,\n",
       " 'mentor': 3,\n",
       " 'waugh': 3,\n",
       " 'traditions': 2,\n",
       " 'packers': 3,\n",
       " 'sweaty': 1,\n",
       " 'pellegrin': 19,\n",
       " 'muppets': 17,\n",
       " 'win-': 5,\n",
       " 'ballet': 5,\n",
       " 'for': 1250,\n",
       " 'engineer': 8,\n",
       " 'alaska': 23,\n",
       " 'action': 5,\n",
       " 'abacus': 2,\n",
       " 'coin': 12,\n",
       " 'almost': 7,\n",
       " 'shop': 4,\n",
       " 'crane': 3,\n",
       " 'sport': 102,\n",
       " 'manchukuo': 3,\n",
       " 'example': 14,\n",
       " 'dolphins': 5,\n",
       " 'anne': 4,\n",
       " 'desire': 3,\n",
       " 'dream': 12,\n",
       " 'flag': 42,\n",
       " 'aykroyd': 5,\n",
       " 'pilot': 9,\n",
       " 'replica': 1,\n",
       " 'jews': 6,\n",
       " 'nuclear': 15,\n",
       " 'james': 47,\n",
       " 'weird': 3,\n",
       " 'net': 4,\n",
       " 'triangle': 16,\n",
       " '1955': 1,\n",
       " 'absolute': 4,\n",
       " 'palate': 5,\n",
       " 'exist': 13,\n",
       " 'devo': 4,\n",
       " 'confucius': 1,\n",
       " 'styloid': 5,\n",
       " 'enola': 4,\n",
       " 'cricketer': 1,\n",
       " 'champions': 5,\n",
       " 'chappellet': 6,\n",
       " 'trainer': 1,\n",
       " 'aspartame': 16,\n",
       " 'contributions': 4,\n",
       " 'require': 8,\n",
       " 'deere': 1,\n",
       " 'tape': 1,\n",
       " 'currents': 1,\n",
       " 'lewis': 3,\n",
       " 'bloodhound': 3,\n",
       " 'stalker': 4,\n",
       " 'ethel': 6,\n",
       " 'origin': 166,\n",
       " 'indira': 2,\n",
       " 'chocolate': 21,\n",
       " 'incubate': 3,\n",
       " 'cisalpine': 4,\n",
       " 'glamorous': 2,\n",
       " 'most-used': 3,\n",
       " 'trillion': 2,\n",
       " 'her': 63,\n",
       " 'code': 26,\n",
       " 'ram': 6,\n",
       " 'principal': 3,\n",
       " 'shia': 4,\n",
       " 'swamp': 3,\n",
       " 'satirize': 1,\n",
       " 'tungsten': 1,\n",
       " 'webpage': 1,\n",
       " 'tufts': 4,\n",
       " 'litmus': 1,\n",
       " 'martyr': 5,\n",
       " 'cabinet': 3,\n",
       " 'approximate': 10,\n",
       " 'formula': 6,\n",
       " 'gore': 8,\n",
       " 'us': 78,\n",
       " 'iron': 13,\n",
       " 'step': 9,\n",
       " 'yacht': 7,\n",
       " 'sidney': 2,\n",
       " 'costner': 17,\n",
       " 'badge': 3,\n",
       " 'wilbur': 5,\n",
       " 'puppy': 2,\n",
       " 'cascade': 5,\n",
       " 'ancestral': 1,\n",
       " 'laugh-in': 9,\n",
       " 'haifa': 3,\n",
       " 'normans': 1,\n",
       " 'you': 561,\n",
       " 'pregnancy': 15,\n",
       " 'rex': 8,\n",
       " 'break': 22,\n",
       " 'detect': 9,\n",
       " 'sailor': 6,\n",
       " 'rupee': 5,\n",
       " 'cigar-chewing': 3,\n",
       " 'skittle': 1,\n",
       " 'yearly': 1,\n",
       " 'pen': 11,\n",
       " 'bulls': 7,\n",
       " 'unsuccessful': 4,\n",
       " 'whose': 55,\n",
       " 'wear': 40,\n",
       " 'dickens': 16,\n",
       " 'vats': 2,\n",
       " 'quality': 1,\n",
       " 'gram': 4,\n",
       " 'too': 3,\n",
       " '2-sided': 2,\n",
       " 'rhinestone': 1,\n",
       " '1929': 7,\n",
       " 'ibm-compatible': 14,\n",
       " 'peter': 28,\n",
       " 'ybarra': 5,\n",
       " 'corsica': 1,\n",
       " 'liquid': 2,\n",
       " 'capture': 2,\n",
       " 'vlaja': 4,\n",
       " 'inventors': 4,\n",
       " 'varieties': 7,\n",
       " 'boil': 6,\n",
       " 'directors': 1,\n",
       " 'heimlich': 1,\n",
       " '95': 1,\n",
       " 'widely-used': 1,\n",
       " 'corporation': 2,\n",
       " 'pushy': 1,\n",
       " 'fray': 4,\n",
       " 'super': 38,\n",
       " 'ernie': 5,\n",
       " 'jiggy': 8,\n",
       " 'zoo': 2,\n",
       " 'aurora': 1,\n",
       " 'abuse': 7,\n",
       " 'strasbourg': 2,\n",
       " 'forces': 1,\n",
       " 'kahn': 5,\n",
       " 'reactivity': 9,\n",
       " 'later': 3,\n",
       " 'rolls-royce': 5,\n",
       " 'scooby-doo': 4,\n",
       " 'antigua': 2,\n",
       " 'farmer': 3,\n",
       " 'debt': 1,\n",
       " 'basque': 5,\n",
       " 'proud': 5,\n",
       " 'khrushchev': 4,\n",
       " 'miles': 44,\n",
       " 'abominable': 5,\n",
       " 'a-tasket': 1,\n",
       " 'guam': 4,\n",
       " 'processor': 7,\n",
       " 'dondi': 2,\n",
       " 'player': 35,\n",
       " 'airliners': 1,\n",
       " 'barr': 4,\n",
       " 'toothpaste': 1,\n",
       " 'stuart': 12,\n",
       " 'enrol': 3,\n",
       " 'atom': 4,\n",
       " 'attractions': 11,\n",
       " 'bernoulli': 2,\n",
       " 'enemy': 4,\n",
       " 'yogi': 8,\n",
       " 'ip': 8,\n",
       " 'polka': 1,\n",
       " 'spout': 3,\n",
       " 'vince': 2,\n",
       " 'often': 25,\n",
       " 'audio': 3,\n",
       " 'well-known': 9,\n",
       " 'doria': 4,\n",
       " 'multilingual': 1,\n",
       " 'dakota': 12,\n",
       " 'biscuits': 4,\n",
       " 'fondness': 5,\n",
       " 'products': 24,\n",
       " 'surgeon': 3,\n",
       " 'butcher': 1,\n",
       " 'pencey': 4,\n",
       " '2th-century': 5,\n",
       " 'boston': 6,\n",
       " 'mormons': 11,\n",
       " 'vienna': 8,\n",
       " 'needle': 9,\n",
       " 'poodle': 3,\n",
       " 'lcd': 3,\n",
       " 'reims': 23,\n",
       " 'cardinal': 6,\n",
       " 'forerunner': 2,\n",
       " 'havlicek': 4,\n",
       " 'fall': 8,\n",
       " 'glacier': 7,\n",
       " 'politics': 2,\n",
       " 'hog': 3,\n",
       " 'holy': 9,\n",
       " 'bernard': 3,\n",
       " 'bank': 15,\n",
       " 'ocho': 4,\n",
       " 'strike': 7,\n",
       " 'zealand': 10,\n",
       " 'farmers': 1,\n",
       " 'limbo': 2,\n",
       " 'mutiny': 12,\n",
       " 'sunset': 1,\n",
       " 'herb': 7,\n",
       " 'foot-pounds': 3,\n",
       " 'deadrise': 3,\n",
       " 'chancellor': 6,\n",
       " 'sleeve': 2,\n",
       " 'specific': 3,\n",
       " 'play': 225,\n",
       " 'numerals': 9,\n",
       " 'key': 2,\n",
       " 'barbershop': 1,\n",
       " 'brief': 3,\n",
       " 'garfield': 4,\n",
       " 'intestines': 4,\n",
       " 'card': 63,\n",
       " 'reason': 24,\n",
       " 'noodle': 1,\n",
       " 'members': 16,\n",
       " 'dudley': 5,\n",
       " '1926': 5,\n",
       " '1963': 18,\n",
       " '1779': 5,\n",
       " 'huston': 5,\n",
       " 'shirtwaist': 4,\n",
       " 'st.': 38,\n",
       " 'exxon': 5,\n",
       " 'server': 5,\n",
       " ':': 198,\n",
       " 'philby': 2,\n",
       " 'hood': 3,\n",
       " 'rathaus': 3,\n",
       " 'ostrich': 1,\n",
       " 'jpeg': 3,\n",
       " 'konrad': 2,\n",
       " 'ahead': 5,\n",
       " 'electronics': 4,\n",
       " 'filenes': 1,\n",
       " 'piracy': 5,\n",
       " 'chop': 7,\n",
       " 'softest': 1,\n",
       " 'dustbowl': 3,\n",
       " 'tour': 18,\n",
       " 'ringo': 5,\n",
       " 'nebraska': 8,\n",
       " 'producers': 5,\n",
       " 'truck': 2,\n",
       " '280': 5,\n",
       " 'hussain': 4,\n",
       " 'reb': 2,\n",
       " 'mcbeal': 2,\n",
       " 'victoria': 15,\n",
       " 'range': 15,\n",
       " 'alcohol': 2,\n",
       " 'aftra': 1,\n",
       " 'postage': 4,\n",
       " 'dubliners': 2,\n",
       " 'fish': 21,\n",
       " 'daughters': 12,\n",
       " 'hate': 12,\n",
       " 'cowboy': 22,\n",
       " 'wet': 3,\n",
       " 'marvelous': 3,\n",
       " '1985': 6,\n",
       " 'rico': 1,\n",
       " 'pitcher': 12,\n",
       " 'metalious': 2,\n",
       " 'gestapo': 4,\n",
       " 'martin': 13,\n",
       " 'calories': 13,\n",
       " 'rice': 4,\n",
       " 'malaysia': 6,\n",
       " 'feature': 78,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pop', 15),\n",
       " ('upon', 15),\n",
       " ('ways', 15),\n",
       " ('away', 15),\n",
       " ('championship', 15),\n",
       " ('poker', 15),\n",
       " ('move', 15),\n",
       " ('sons', 15),\n",
       " ('citizen', 15),\n",
       " ('root', 15)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Less commons words in your vocabulary\n",
    "Counter(vocabulary).most_common(nb_input-1)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 1000 words.\n"
     ]
    }
   ],
   "source": [
    "word_list = list([x[0] for x in Counter(vocabulary).most_common(nb_input-1)])\n",
    "\n",
    "# We add the unk word for future purpose.\n",
    "word_list.append('<unk>')\n",
    "words_array = np.array(word_list)\n",
    "print(\"Vocabulary contains\", len(words_array), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Integer encoding with OneHotEncoder\n",
    "words_tre = words_array.reshape(len(words_array),1)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionnary of word and its one hot array\n",
    "words_onehoted = {}\n",
    "for i in range(0, len(words_array)):\n",
    "    words_onehoted[word_list[i]] = onehot_encoded[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the corresponding one hot list for a word.\n",
    "def get_onehot_word(word):\n",
    "    if word in words_onehoted:\n",
    "        return list(words_onehoted[word])\n",
    "    else:\n",
    "        return list(words_onehoted['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_word('<unk>')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if an unknown word is transformed into a <unk>\n",
    "get_onehot_word('obviously_an_unknown_word').index(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot categories\n",
    "\n",
    "categories_onehoted = {}\n",
    "categories_onehoted['ABBR'] = [1, 0, 0, 0, 0, 0] # Abbreviation\n",
    "categories_onehoted['ENTY'] = [0, 1, 0, 0, 0, 0] # Entity\n",
    "categories_onehoted['DESC'] = [0, 0, 1, 0, 0, 0] # Description\n",
    "categories_onehoted['HUM']  = [0, 0, 0, 1, 0, 0] # Human\n",
    "categories_onehoted['LOC']  = [0, 0, 0, 0, 1, 0] # Location\n",
    "categories_onehoted['NUM']  = [0, 0, 0, 0, 0, 1] # Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the corresponding one hot list for a category.\n",
    "def get_onehot_category(category):\n",
    "    return categories_onehoted[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_category('HUM')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 372 ms, total: 14.8 s\n",
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating training set\n",
    "\n",
    "batch_data = []\n",
    "for num_question in range(len(questions)):\n",
    "    # Construction of question_onehot list.\n",
    "    question_onehot = [get_onehot_word(word) for word in questions[num_question]]\n",
    "    \n",
    "    # Construction of category_onehot.\n",
    "    category = labels[num_question].partition(':')[0]\n",
    "    category_onehot = get_onehot_category(category)\n",
    "    batch_data.append([tensor([question_onehot]), tensor([category_onehot])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 416 ms, sys: 20 ms, total: 436 ms\n",
      "Wall time: 431 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating test set\n",
    "\n",
    "batch_data_test = []\n",
    "for num_question in range(len(questions_test)):\n",
    "    \n",
    "    # Construction of question_onehot list.\n",
    "    question_onehot = [get_onehot_word(word) for word in questions[num_question]]\n",
    "    \n",
    "    # Construction of category_onehot.\n",
    "    category = labels_test[num_question].partition(':')[0]\n",
    "    category_onehot = get_onehot_category(category)\n",
    "    batch_data_test.append([tensor([question_onehot]), tensor([category_onehot])])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0.5, batch_first=True, nonlinearity='relu')\n",
    "        self.inter = nn.Linear(nb_neurons, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        #self.synapses = Variable(torch.zeros(self.nb_layers, 1, self.nb_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = Variable(torch.zeros(self.nb_layers, inputs.size(0), self.nb_neurons))\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        \n",
    "        x, hn = self.rnn(inputs, h0)\n",
    "        \n",
    "        x = self.inter(x)\n",
    "        x = nn.functional.softmax(x, dim=2)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, batch_list, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Shuffling batch_list\n",
    "    shuffle(batch_list)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(batch_list):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            #data, target = Variable(data), Variable(target)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "            rnn.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 100 == 0 or batch_idx % 100 == 1 or batch_idx == len(batch_list)-1:\n",
    "                print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                        epoch, \n",
    "                        (batch_idx+1) * len(data), \n",
    "                        len(batch_list),\n",
    "                        100. * (batch_idx+1) / len(batch_list), \n",
    "                        loss.data.item()), \n",
    "                        end='')\n",
    "        print()\n",
    "        \n",
    "    # Return losses list, you can print them later if you want\n",
    "    return losses\n",
    "\n",
    "\n",
    "# return (rightAnswer, ignored, falseAnswer)\n",
    "def getEfficience(rnn, batch_list, tresh=0) :\n",
    "    rightAnswer = 0\n",
    "    ignored = 0\n",
    "    falseAnswer = 0\n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        predicted = rnn(data).detach().cpu().numpy()[-1][-1]\n",
    "        #print(\"predicted: \"+str(np.argmax(predicted)))\n",
    "        #print(\"target: \"+str(np.argmax(target.detach().cpu().numpy()[-1])))\n",
    "        if max(predicted) < tresh :\n",
    "            ignored += 1\n",
    "        else:\n",
    "            if np.argmax(predicted) == np.argmax(target.detach().cpu().numpy()[-1]):\n",
    "                rightAnswer += 1\n",
    "            else:\n",
    "                falseAnswer += 1\n",
    "    return (rightAnswer, ignored, falseAnswer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 0 [15452/15452 (100%)]\t Loss: 2.463336\n",
      " Train Epoch: 1 [15452/15452 (100%)]\t Loss: 2.406374\n",
      " Train Epoch: 2 [15452/15452 (100%)]\t Loss: 2.405602\n",
      " Train Epoch: 3 [15452/15452 (100%)]\t Loss: 2.406030\n",
      " Train Epoch: 4 [15452/15452 (100%)]\t Loss: 2.405616\n",
      " Train Epoch: 5 [15452/15452 (100%)]\t Loss: 2.408392\n",
      " Train Epoch: 6 [15452/15452 (100%)]\t Loss: 2.405265\n",
      " Train Epoch: 7 [15452/15452 (100%)]\t Loss: 2.405372\n",
      " Train Epoch: 8 [15452/15452 (100%)]\t Loss: 2.405301\n",
      " Train Epoch: 9 [15452/15452 (100%)]\t Loss: 2.414414\n",
      "Done :)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(nb_inputs = nb_input, nb_layers=nb_hidd_lay,\n",
    "          nb_neurons=hidden_size, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "losses = learn(rnn, batch_data, nb_epochs)\n",
    "print(\"Done :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fee2067ecc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d670049b4a5b4f1d9d24efe2bb68dda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=51, description='smooth', max=201, min=5, step=2), Output()), _dom_class…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "x = np.linspace(0, len(losses), len(losses))\n",
    "fig = plt.figure(figsize=(13, 8)) \n",
    "ax = fig.add_subplot(1,1,1)\n",
    "cnn_line, = ax.plot(x, losses)\n",
    "\n",
    "def update_losses(smooth=51):\n",
    "    cnn_line.set_ydata(savgol_filter(losses, smooth, 3))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(update_losses, smooth=(5, 201, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! On the test set:\n",
      "Corrects: 1.5596686513072742%\n",
      "False:    98.44033134869272%\n",
      "\n",
      "A présent, tu peux copier-coller ça dans le doc sur le drive :)\n",
      "1000\t0.001\t10\t2\t3\t\t1.5596686513072742%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_results = getEfficience(rnn, batch_data_test)\n",
    "total = sum(final_results)\n",
    "\n",
    "correct = final_results[0]/total*100\n",
    "ignored = final_results[1]/total*100\n",
    "false = final_results[2]/total*100\n",
    "\n",
    "\n",
    "\n",
    "print(\"Congratulations! On the test set:\")\n",
    "print(\"Corrects: \" + str(correct) + \"%\")\n",
    "#print(\"Ignored:  \" + str(ignored) + \"%\")\n",
    "print(\"False:    \" + str(false) + \"%\")\n",
    "\n",
    "print()\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(nb_input)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(nb_hidd_lay)+\"\\t\"+str(hidden_size)+\"\\t\\t\"+str(correct)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
