{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from scipy.stats import entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 100 #8414 max\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 5\n",
    "hidden_size = 10\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.0001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 10\n",
    "\n",
    "# Random seed, don't change it if you don't know what it is\n",
    "random_seed = 42\n",
    "\n",
    "nb_batchs = 4\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding_random():\n",
    "    random.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed_all(random_seed)\n",
    "        torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "# Working on list_of_words\n",
    "# TODO: it doesn't work\n",
    "def lemm(phrase):\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lemmed = []\n",
    "    for word in phrase:\n",
    "        lemmed.append(lemmer.lemmatize(word, pos=\"v\"))\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDataset(Dataset):\n",
    "    def __init__(self, file, nb_most_commons):\n",
    "        # Encoding in windows-1252, utf-8 generate error on some char\n",
    "        file = codecs.open(file, \"r+\",\"windows-1252\")\n",
    "        train_data = []\n",
    "        for line in file.readlines():\n",
    "            train_data.append(line)\n",
    "\n",
    "        # Divided file into 2 list:\n",
    "        # questions = list of questions \n",
    "        # labels = list of labels\n",
    "\n",
    "        questions = []\n",
    "        labels = []\n",
    "\n",
    "        # Black list\n",
    "        regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "\n",
    "        (train_data[0]).split()[0]\n",
    "        for string in train_data:\n",
    "            question_str = []\n",
    "            for x in lemm(string.split()[1:]):\n",
    "                question_str.append(x.lower())\n",
    "            labels.append(string.split()[0])\n",
    "            questions.append(question_str)\n",
    "\n",
    "        self.add_tag(questions)\n",
    "        \n",
    "        # Vocabulary of unique words\n",
    "        data = []\n",
    "        data.append('<unk>')\n",
    "        for q in questions:\n",
    "            for word in q:\n",
    "                data.append(word)\n",
    "        #print(len(data))\n",
    "        scv = np.array(data)\n",
    "        unik, counts = np.unique(scv,return_counts=True)\n",
    "        vocabulary = {}\n",
    "        for i in range(0, len(unik)):\n",
    "            vocabulary[unik[i]] = counts[i]\n",
    "    \n",
    "        word_list = list([x[0] for x in Counter(vocabulary).most_common(nb_most_commons+2)])\n",
    "        # We add the unk word for future purpose.\n",
    "        word_list.append('<unk>')\n",
    "        words_array = np.array(word_list)\n",
    "        \n",
    "        # Integer encoding with OneHotEncoder\n",
    "        words_tre = words_array.reshape(len(words_array),1)\n",
    "        one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "        onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "        # Creating a dictionnary of word and its one hot array\n",
    "        self.words_onehoted = {}\n",
    "        for i in range(0, len(words_array)):\n",
    "            self.words_onehoted[word_list[i]] = onehot_encoded[i]\n",
    "\n",
    "        # One hot categories\n",
    "        self.categories_onehoted = {}\n",
    "        self.categories_onehoted['ABBR'] = 0 # Abbreviation\n",
    "        self.categories_onehoted['ENTY'] = 1 # Entity\n",
    "        self.categories_onehoted['DESC'] = 2 # Description\n",
    "        self.categories_onehoted['HUM']  = 3 # Human\n",
    "        self.categories_onehoted['LOC']  = 4 # Location\n",
    "        self.categories_onehoted['NUM']  = 5 # Numeric\n",
    "        \n",
    "        self.batch_data = []\n",
    "        for num_question in range(len(questions)):\n",
    "            # Construction of question_onehot list.\n",
    "            question_onehot = [self.get_onehot_word(word) for word in questions[num_question]]\n",
    "\n",
    "            # Construction of category_onehot.\n",
    "            category = labels[num_question].partition(':')[0]\n",
    "            category_onehot = self.get_onehot_category(category)\n",
    "            self.batch_data.append([(question_onehot), (category_onehot)])\n",
    "    \n",
    "        \n",
    "    \n",
    "    # Function to get the corresponding one hot list for a category.\n",
    "    def get_onehot_category(self, category):\n",
    "        return self.categories_onehoted[category]\n",
    "\n",
    "\n",
    "    # Function to get the corresponding one hot list for a word.\n",
    "    def get_onehot_word(self, word):\n",
    "        if word in self.words_onehoted:\n",
    "            return list(self.words_onehoted[word])\n",
    "        else:\n",
    "            return list(self.words_onehoted['<unk>'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Method to add tags begin and end to phrases list.\n",
    "    # /!\\ \n",
    "    # WARNING : this method need to be executed only ONE time.\n",
    "    # /!\\\n",
    "    def add_tag(self, question_list):\n",
    "        for i in range(0, len(question_list)):\n",
    "            if question_list[i][0] != '<bos>' :\n",
    "                question_list[i].insert(0, '<bos>')\n",
    "                question_list[i].append('<eos>')\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.batch_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seeding_random()\n",
    "        return self.batch_data[idx]\n",
    "    \n",
    "def pad_collate(batch):\n",
    "    max_length = max([len(q[0]) for q in batch])\n",
    "\n",
    "    inputs = torch.FloatTensor([[[0. for _ in range(len(x[0][0]))] for i in range(max_length-len(x[0]))]+x[0] for x in batch])\n",
    "    outputs = torch.LongTensor([x[1] for x in batch])\n",
    "    \n",
    "    return inputs, outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seeding_random()\n",
    "training_set = QuestionDataset(\"train_all.label\", nb_input-3)\n",
    "seeding_random()\n",
    "\n",
    "test_set = QuestionDataset(\"TREC_test.label\", nb_input-3)\n",
    "seeding_random()\n",
    "\n",
    "# Création du DataLoader\n",
    "dataloader_args = dict(shuffle=True, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)# if use_cuda else dict(collate_fn=pad_collate)\n",
    "seeding_random()\n",
    "\n",
    "train_loader = dataloader.DataLoader(training_set, **dataloader_args)\n",
    "seeding_random()\n",
    "\n",
    "dataloader_args_test = dict(shuffle=False, batch_size=nb_batchs, num_workers=1,\n",
    "                       pin_memory=True, worker_init_fn=seeding_random(), collate_fn=pad_collate)# if use_cuda else dict(collate_fn=pad_collate)\n",
    "seeding_random()\n",
    "\n",
    "test_loader = dataloader.DataLoader(test_set, **dataloader_args_test)\n",
    "seeding_random()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0., batch_first=True, nonlinearity='relu')\n",
    "        self.inter = nn.Linear(nb_neurons, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = torch.zeros(self.nb_layers, inputs.size(0), self.nb_neurons)\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        x, hn = self.rnn(inputs, h0)\n",
    "        \n",
    "        x = self.inter(hn[0])\n",
    "        #print(x)\n",
    "        #x = tensor([list(i[-1]) for i in x])\n",
    "        #print(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, data_loader, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_correct = 0\n",
    "        total_target = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            #print(output)\n",
    "            #print(target)\n",
    "            loss = criterion(output, target)\n",
    "            rnn.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            losses.append(loss.data.item())\n",
    "            \n",
    "            # Get the Accuracy\n",
    "            \n",
    "            _, predicted = torch.max(output.data, dim=1)\n",
    "            correct = (predicted == target).sum().item()\n",
    "            total_correct += correct\n",
    "            total_target += target.size(0)\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 100 == 0 or batch_idx % 100 == 1 or batch_idx == len(data_loader)-1:\n",
    "                  print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}\\t Accuracy: {}'.format(\n",
    "                    epoch, \n",
    "                    batch_idx * len(data), \n",
    "                    len(data_loader.dataset),\n",
    "                    100. * batch_idx / len(data_loader), \n",
    "                    loss.data.item(),\n",
    "                    (total_correct / total_target) * 100),\n",
    "                    end='')\n",
    "        print()\n",
    "        \n",
    "    # Return losses list, you can print them later if you want\n",
    "    return losses\n",
    "\n",
    "# return (rightAnswer, falseAnswer, entropy_list)\n",
    "def getEfficience(rnn, batch_list) :\n",
    "    entropy_list = []\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        out = rnn(data).data\n",
    "        \n",
    "        _, predicted = torch.max(out.data, dim=1)\n",
    "        correct = (predicted == target).sum().item()\n",
    "        total_correct += correct\n",
    "        total += target.size(0)\n",
    "\n",
    "        value, counts = np.unique(out, return_counts=True)\n",
    "        entropy_list.append(entropy(out, base=None))\n",
    "\n",
    "    return (total_correct, total_target-total_correct, entropy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 0 [15448/15452 (100%)]\t Loss: 1.784005\t Accuracy: 18.91664509448615\n",
      " Train Epoch: 1 [15448/15452 (100%)]\t Loss: 1.672840\t Accuracy: 23.265596686513074\n",
      " Train Epoch: 2 [15448/15452 (100%)]\t Loss: 1.625977\t Accuracy: 23.084390370178618\n",
      " Train Epoch: 3 [15448/15452 (100%)]\t Loss: 1.576590\t Accuracy: 25.23297954957287\n",
      " Train Epoch: 4 [15448/15452 (100%)]\t Loss: 1.612147\t Accuracy: 32.96660626456122\n",
      " Train Epoch: 5 [15448/15452 (100%)]\t Loss: 1.706352\t Accuracy: 37.943308309603935\n",
      " Train Epoch: 6 [15448/15452 (100%)]\t Loss: 1.758542\t Accuracy: 42.577012684442145\n",
      " Train Epoch: 7 [15448/15452 (100%)]\t Loss: 1.667837\t Accuracy: 44.84209163862283\n",
      " Train Epoch: 8 [15448/15452 (100%)]\t Loss: 1.513853\t Accuracy: 45.94874449909397\n",
      " Train Epoch: 9 [15448/15452 (100%)]\t Loss: 1.698738\t Accuracy: 46.440590214858915\n",
      "Done :)\n"
     ]
    }
   ],
   "source": [
    "seeding_random()\n",
    "\n",
    "rnn = RNN(nb_inputs = nb_input, nb_layers=nb_hidd_lay,\n",
    "          nb_neurons=hidden_size, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "\n",
    "with torch.enable_grad():\n",
    "    losses = learn(rnn, train_loader, nb_epochs)\n",
    "    print(\"Done :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1876a7b3f7a448f6a1d0cdec5bf786ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='smooth', max=201, min=5, step=2), Output()), _dom_classe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "\n",
    "def update_losses(smooth=1):\n",
    "    x = np.linspace(0, len(losses), len(losses))\n",
    "    fig = plt.figure(figsize=(13, 8)) \n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    cnn_line, = ax.plot(x, losses)\n",
    "    cnn_line.set_ydata(savgol_filter(losses, smooth, 3))\n",
    "    \n",
    "interact(update_losses, smooth=(5, 201, 2));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations!\n",
      "On the training set:\n",
      "Corrects: 46.3888169816205%\n",
      "False:    53.61118301837949%\n",
      "\n",
      "[array([ 0.02737719,  1.08859992,  0.58416933,  1.07360196,  0.58113724,\n",
      "        0.56308007], dtype=float32), array([ 0.05083909,  1.18554378,  0.6013732 ,  1.10491848,  0.5934025 ,\n",
      "        0.56890309], dtype=float32), array([  2.29224914e-23,   7.55336821e-01,   5.29539597e-04,\n",
      "         1.28820717e+00,   1.47581259e-05,   1.59848496e-05], dtype=float32), array([ 0.01424396,  0.76633573,  0.60259211,  1.19096816,  0.50275183,\n",
      "        0.50325227], dtype=float32), array([  4.91164986e-30,   1.05162013e+00,   6.90678353e-05,\n",
      "         1.23126507e+00,   6.11834821e-07,   5.81006873e-07], dtype=float32), array([  0.00000000e+00,   1.09110403e+00,   5.30077500e-07,\n",
      "         1.23554718e+00,   5.81069970e-10,   5.01343467e-10], dtype=float32), array([  3.15853940e-30,   1.08839643e+00,   1.12433918e-04,\n",
      "         1.20230818e+00,   1.00351338e-06,   9.31600880e-07], dtype=float32), array([  2.20162081e-04,   8.54336381e-01,   4.08039719e-01,\n",
      "         1.19561887e+00,   2.69301564e-01,   2.66825527e-01], dtype=float32), array([  4.18176473e-31,   1.06062937e+00,   5.10044410e-05,\n",
      "         1.18447471e+00,   4.07927899e-07,   3.69432399e-07], dtype=float32), array([ 0.69578856,  1.33933997,  1.34609759,  1.31278992,  1.33395755,\n",
      "        1.32954574], dtype=float32), array([  1.79344701e-04,   1.37033474e+00,   6.06596887e-01,\n",
      "         1.37799335e+00,   3.74805987e-01,   3.51699919e-01], dtype=float32), array([  2.32628405e-09,   7.72964001e-01,   1.14248097e-01,\n",
      "         1.33287072e+00,   2.71099620e-02,   2.86475271e-02], dtype=float32), array([ 0.57161832,  1.06001365,  0.69286621,  0.97583872,  0.69035035,\n",
      "        0.689358  ], dtype=float32), array([  8.38106345e-17,   8.01513255e-01,   6.54611550e-03,\n",
      "         1.32111764e+00,   5.48833515e-04,   5.68573712e-04], dtype=float32), array([  5.08535322e-06,   1.04247868e+00,   2.43554801e-01,\n",
      "         1.17230809e+00,   1.38073862e-01,   1.37868032e-01], dtype=float32), array([             nan,   2.59519428e-01,   7.27208398e-12,\n",
      "         1.38234413e+00,   2.15156930e-16,   1.77054781e-16], dtype=float32), array([  0.00000000e+00,   7.45105982e-01,   1.57234396e-07,\n",
      "         1.20298100e+00,   1.85093454e-10,   1.61836128e-10], dtype=float32), array([  0.00000000e+00,   7.59255528e-01,   4.42773768e-14,\n",
      "         1.19487739e+00,   1.47494084e-19,   1.12907763e-19], dtype=float32), array([  0.00000000e+00,   6.92983270e-01,   2.40679449e-10,\n",
      "         1.21968699e+00,   2.06681020e-14,   1.75510411e-14], dtype=float32), array([  1.77192589e-04,   1.04920125e+00,   4.42565441e-01,\n",
      "         1.09999824e+00,   3.32878441e-01,   2.97730595e-01], dtype=float32), array([  2.37741013e-04,   1.19430149e+00,   4.05593932e-01,\n",
      "         1.14107704e+00,   3.13631862e-01,   2.84974754e-01], dtype=float32), array([ 0.93264997,  1.36577272,  1.08816874,  1.06056261,  1.09505248,\n",
      "        1.0956558 ], dtype=float32), array([  7.76833106e-31,   1.07401514e+00,   4.85842720e-05,\n",
      "         1.18782783e+00,   4.53577485e-07,   4.16656547e-07], dtype=float32), array([  0.00000000e+00,   6.80036992e-02,   6.79378413e-27,\n",
      "         1.29079771e+00,   2.69498326e-37,   2.09112496e-37], dtype=float32), array([ 0.        ,  1.04303837,  0.01503968,  1.36307859,  0.00245772,\n",
      "        0.00235608], dtype=float32), array([ 0.57968545,  1.31462598,  0.81194627,  1.12217295,  0.75219703,\n",
      "        0.74611187], dtype=float32), array([        nan,  1.06240499,  0.20682362,  1.37606013,  0.11031893,\n",
      "        0.10931024], dtype=float32), array([  2.35092731e-28,   7.42267072e-01,   1.10560853e-04,\n",
      "         1.25211287e+00,   1.34624042e-06,   1.22653694e-06], dtype=float32), array([  0.00000000e+00,   5.33028960e-01,   8.62390129e-14,\n",
      "         1.20324659e+00,   2.90562054e-19,   2.20456325e-19], dtype=float32), array([  1.15713032e-04,   9.49983597e-01,   3.43284816e-01,\n",
      "         1.15622950e+00,   2.50681162e-01,   2.38163158e-01], dtype=float32), array([  0.00000000e+00,   1.07057810e+00,   3.23717330e-07,\n",
      "         1.18608999e+00,   3.86059018e-10,   3.22000260e-10], dtype=float32), array([ 0.12226298,  1.24666846,  0.65866065,  1.13694823,  0.64331728,\n",
      "        0.63238597], dtype=float32), array([  2.24311293e-07,   1.07885778e+00,   1.69324592e-01,\n",
      "         1.17080021e+00,   8.07950795e-02,   7.25469887e-02], dtype=float32), array([  0.00000000e+00,   2.52226114e-01,   3.02011732e-14,\n",
      "         1.35986364e+00,   1.75675559e-19,   1.44310111e-19], dtype=float32), array([ 0.00272917,  1.09191751,  0.56813186,  1.20909953,  0.41004506,\n",
      "        0.40582624], dtype=float32), array([  0.00000000e+00,   8.27862978e-01,   8.10467327e-07,\n",
      "         1.29004288e+00,   1.51363733e-09,   1.44781909e-09], dtype=float32), array([  1.60987111e-30,   7.57791221e-01,   5.58914981e-05,\n",
      "         1.19707549e+00,   5.00712019e-07,   4.63132523e-07], dtype=float32), array([  2.71522268e-31,   8.94118011e-01,   4.67270293e-05,\n",
      "         1.16874790e+00,   3.84202536e-07,   3.48477982e-07], dtype=float32), array([  9.91663115e-31,   1.01110530e+00,   5.35068248e-05,\n",
      "         1.19525695e+00,   4.65528359e-07,   4.26651212e-07], dtype=float32), array([  4.46936397e-18,   8.70343387e-01,   4.99431230e-03,\n",
      "         1.31277764e+00,   2.77496350e-04,   2.93714838e-04], dtype=float32), array([ 0.00472508,  1.28705454,  0.510607  ,  1.15013397,  0.47014499,\n",
      "        0.44665277], dtype=float32), array([  7.22508330e-30,   8.84043217e-01,   8.60569417e-05,\n",
      "         1.23586380e+00,   6.70754901e-07,   6.14774308e-07], dtype=float32), array([ 0.66434085,  0.702259  ,  0.69238377,  1.02126181,  0.69236779,\n",
      "        0.69237709], dtype=float32), array([  1.87859519e-06,   1.04285240e+00,   2.35007793e-01,\n",
      "         1.13255548e+00,   1.35316998e-01,   1.26173273e-01], dtype=float32), array([ 0.71575773,  1.07382035,  1.08434737,  0.82348913,  1.07636416,\n",
      "        1.0712955 ], dtype=float32), array([  3.35305487e-29,   1.09953892e+00,   1.92847045e-04,\n",
      "         1.25483823e+00,   1.75167293e-06,   1.54810550e-06], dtype=float32), array([  1.13077643e-31,   8.55805755e-01,   5.39519169e-05,\n",
      "         1.17052603e+00,   3.48541903e-07,   3.12536741e-07], dtype=float32), array([  0.00000000e+00,   5.01022339e-01,   3.50927169e-11,\n",
      "         1.34147358e+00,   1.84301664e-15,   1.53958286e-15], dtype=float32), array([  0.00000000e+00,   8.23508382e-01,   1.74540290e-07,\n",
      "         1.22073662e+00,   2.12169060e-10,   1.89323834e-10], dtype=float32), array([  0.00000000e+00,   1.07985902e+00,   5.45472834e-10,\n",
      "         1.21351302e+00,   5.79756241e-14,   4.59495723e-14], dtype=float32), array([  0.00000000e+00,   7.91779545e-04,   0.00000000e+00,\n",
      "         1.18795204e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32), array([  8.64616686e-06,   9.45197940e-01,   3.30647767e-01,\n",
      "         1.15624475e+00,   1.69599459e-01,   1.51344687e-01], dtype=float32), array([  0.00000000e+00,   7.61262715e-01,   7.16812304e-15,\n",
      "         1.18923497e+00,   2.19987488e-20,   2.02686618e-20], dtype=float32), array([  0.00000000e+00,   5.70329189e-01,   1.19963543e-05,\n",
      "         1.36061001e+00,   1.22033271e-07,   1.13040805e-07], dtype=float32), array([  7.74179451e-31,   1.14700043e+00,   4.91146529e-05,\n",
      "         1.19583023e+00,   4.43286353e-07,   4.11659414e-07], dtype=float32), array([             nan,   3.96987259e-01,   3.75869336e-10,\n",
      "         1.37682045e+00,   5.05100993e-14,   4.28496993e-14], dtype=float32), array([  0.00000000e+00,   1.04482245e+00,   3.27188054e-10,\n",
      "         1.27647233e+00,   2.61279086e-14,   2.38679756e-14], dtype=float32), array([ 0.02959029,  0.70274061,  0.60981882,  1.05248535,  0.58543849,\n",
      "        0.56548798], dtype=float32), array([  4.07863438e-30,   1.09240603e+00,   1.16567717e-04,\n",
      "         1.20759201e+00,   1.04662627e-06,   9.70607630e-07], dtype=float32), array([  1.95301565e-27,   1.06287742e+00,   1.77967144e-04,\n",
      "         1.16948903e+00,   3.12175280e-06,   2.67470796e-06], dtype=float32), array([  0.00000000e+00,   1.08969069e+00,   8.38280749e-03,\n",
      "         1.36430931e+00,   1.06050645e-03,   9.94979171e-04], dtype=float32), array([ 0.        ,  1.07993925,  0.03843518,  1.3621732 ,  0.0063381 ,\n",
      "        0.0061079 ], dtype=float32), array([  0.00000000e+00,   1.03092158e+00,   4.76073514e-10,\n",
      "         1.23317742e+00,   2.92098024e-14,   2.54345834e-14], dtype=float32), array([  0.00000000e+00,   3.99133228e-02,   1.93579319e-23,\n",
      "         1.35919380e+00,   3.05504605e-32,   2.29222559e-32], dtype=float32), array([        nan,  0.74554092,  0.15096208,  1.37437248,  0.05904454,\n",
      "        0.05877888], dtype=float32), array([ 0.3752912 ,  1.18928254,  1.09590459,  0.83478808,  1.07468176,\n",
      "        1.06994128], dtype=float32), array([  3.17605996e-31,   1.03590572e+00,   5.00720344e-05,\n",
      "         1.17762995e+00,   3.92150923e-07,   3.54675961e-07], dtype=float32), array([  3.48982837e-07,   7.27717459e-01,   2.02517271e-01,\n",
      "         1.15191698e+00,   9.16167796e-02,   8.87085944e-02], dtype=float32), array([  6.97322151e-31,   1.25101984e+00,   9.92802670e-05,\n",
      "         1.19040668e+00,   7.93945787e-07,   7.15211286e-07], dtype=float32), array([  0.00000000e+00,   6.77712321e-01,   6.40416564e-09,\n",
      "         1.36184812e+00,   3.37368588e-12,   2.84419992e-12], dtype=float32), array([  0.00000000e+00,   9.70681369e-01,   2.81538145e-04,\n",
      "         1.34179151e+00,   5.52573511e-06,   5.42064208e-06], dtype=float32), array([  3.47108067e-28,   1.36320102e+00,   2.61818874e-04,\n",
      "         1.31401229e+00,   2.73951946e-06,   2.70062787e-06], dtype=float32), array([ 0.        ,  0.93080401,  0.01500467,  1.36009753,  0.0024569 ,\n",
      "        0.00235528], dtype=float32), array([ 0.69314718,  0.70020199,  0.69314718,  1.34285569,  0.69314718,\n",
      "        0.69314718], dtype=float32), array([  0.00000000e+00,   9.90342557e-01,   2.64346763e-03,\n",
      "         1.36423409e+00,   1.14506052e-04,   1.19775781e-04], dtype=float32), array([             nan,   5.74336469e-01,   6.27098370e-06,\n",
      "         1.37708795e+00,   2.77593557e-08,   2.76567746e-08], dtype=float32), array([ 0.76321131,  1.07717514,  1.0651356 ,  0.91324854,  1.06994843,\n",
      "        1.06694996], dtype=float32), array([ 0.01804963,  0.87016714,  0.61546147,  1.05682778,  0.57314658,\n",
      "        0.55081445], dtype=float32), array([  2.53647969e-25,   9.82656717e-01,   6.59903977e-04,\n",
      "         1.29932106e+00,   7.81679501e-06,   7.92544142e-06], dtype=float32), array([ 0.03362469,  1.03450656,  0.59293103,  1.23319662,  0.53058255,\n",
      "        0.53347534], dtype=float32), array([  1.80793454e-29,   1.28311241e+00,   1.52219145e-04,\n",
      "         1.25930929e+00,   1.38262726e-06,   1.30463832e-06], dtype=float32), array([  0.00000000e+00,   3.14076275e-01,   1.89988756e-22,\n",
      "         1.19043434e+00,   4.71008314e-31,   3.58397672e-31], dtype=float32), array([  1.59168104e-30,   1.30592847e+00,   6.43271705e-05,\n",
      "         1.23399651e+00,   4.95040979e-07,   4.44545975e-07], dtype=float32), array([        nan,  0.73559815,  0.08662122,  1.38622212,  0.04217257,\n",
      "        0.0428614 ], dtype=float32), array([ 0.        ,  0.74574786,  0.01529203,  1.35720122,  0.00252471,\n",
      "        0.00242041], dtype=float32), array([  1.53271973e-09,   6.92766726e-01,   7.89037645e-02,\n",
      "         1.21957338e+00,   2.29507685e-02,   2.21409462e-02], dtype=float32), array([ 0.68146849,  0.96005625,  0.68268603,  1.03208101,  0.69274998,\n",
      "        0.69270617], dtype=float32), array([             nan,   3.01739663e-01,   1.18529264e-12,\n",
      "         1.37394953e+00,   2.13524913e-17,   1.75833220e-17], dtype=float32), array([  9.89124956e-26,   1.07760668e+00,   2.57501466e-04,\n",
      "         1.30098414e+00,   5.04250738e-06,   5.06215156e-06], dtype=float32), array([  0.00000000e+00,   6.38215125e-01,   8.62720472e-14,\n",
      "         1.18617630e+00,   3.92312016e-19,   2.95197031e-19], dtype=float32), array([        nan,  1.08494675,  0.1694795 ,  1.37676525,  0.09755527,\n",
      "        0.10227188], dtype=float32), array([             nan,   4.66950536e-02,   1.27364130e-22,\n",
      "         1.37745309e+00,   1.65522535e-31,   1.23191126e-31], dtype=float32), array([             nan,   6.98886931e-01,   2.01472581e-08,\n",
      "         1.38355482e+00,   1.21291909e-11,   1.05017219e-11], dtype=float32), array([  2.62494609e-10,   1.29380858e+00,   8.15665573e-02,\n",
      "         1.25649536e+00,   1.79463718e-02,   1.63601134e-02], dtype=float32), array([  1.12424023e-14,   1.23549068e+00,   2.04232112e-02,\n",
      "         1.34399378e+00,   1.79598411e-03,   1.92705193e-03], dtype=float32), array([ 0.06863433,  1.35663271,  0.62145019,  1.18021536,  0.60655433,\n",
      "        0.58906543], dtype=float32), array([ 0.84858489,  1.36673403,  1.07081234,  1.0336206 ,  1.08188057,\n",
      "        1.08142018], dtype=float32), array([  0.00000000e+00,   1.34641099e+00,   4.92614618e-07,\n",
      "         1.25375557e+00,   5.90065663e-10,   5.33410816e-10], dtype=float32), array([ 0.69314718,  1.07592463,  0.69314718,  1.35185838,  0.69314718,\n",
      "        0.69314718], dtype=float32), array([  0.00000000e+00,   6.61086068e-02,   3.53684093e-21,\n",
      "         1.35925245e+00,   5.02721461e-29,   4.15005343e-29], dtype=float32), array([  0.00000000e+00,   8.61587048e-01,   7.42733444e-11,\n",
      "         1.19139719e+00,   4.60397093e-15,   3.67330322e-15], dtype=float32), array([ 0.41235638,  1.08451867,  0.67891133,  1.0365721 ,  0.68213916,\n",
      "        0.68242908], dtype=float32), array([  1.18910849e-13,   1.30705011e+00,   2.62981839e-02,\n",
      "         1.23096538e+00,   3.60360136e-03,   3.40529904e-03], dtype=float32), array([  0.00000000e+00,   9.45539594e-01,   2.08060662e-08,\n",
      "         1.36432981e+00,   1.19395466e-11,   1.12903029e-11], dtype=float32), array([ 0.24270678,  1.36380649,  0.66132975,  1.15178096,  0.66115969,\n",
      "        0.65450478], dtype=float32), array([  8.42427713e-31,   9.60636318e-01,   5.21124675e-05,\n",
      "         1.18922055e+00,   4.54898100e-07,   4.17574427e-07], dtype=float32), array([  0.00000000e+00,   9.38796878e-01,   4.02515843e-06,\n",
      "         1.30988777e+00,   1.18426442e-08,   1.15917471e-08], dtype=float32), array([  3.21435333e-31,   1.02287400e+00,   4.79824739e-05,\n",
      "         1.17524076e+00,   3.99612276e-07,   3.62299232e-07], dtype=float32), array([  9.00675223e-05,   1.35267508e+00,   3.29855263e-01,\n",
      "         1.26057804e+00,   2.26496860e-01,   2.17517436e-01], dtype=float32), array([  0.00000000e+00,   1.03813338e+00,   1.90795491e-10,\n",
      "         1.18887568e+00,   1.55197120e-14,   1.24586815e-14], dtype=float32), array([ 0.37375972,  1.04173326,  0.68716788,  0.99904418,  0.68230915,\n",
      "        0.68028414], dtype=float32), array([  3.00817308e-04,   7.60986328e-01,   4.72654343e-01,\n",
      "         1.21158671e+00,   2.74666578e-01,   2.85073042e-01], dtype=float32), array([  0.00000000e+00,   2.08974227e-01,   1.72519503e-21,\n",
      "         1.22403193e+00,   1.07240594e-29,   8.36459737e-30], dtype=float32), array([ 0.82595593,  1.10160744,  1.08677638,  0.8872962 ,  1.0864135 ,\n",
      "        1.08501863], dtype=float32), array([  1.43208357e-27,   1.12390113e+00,   4.59039118e-04,\n",
      "         1.28510404e+00,   4.01958050e-06,   3.82966800e-06], dtype=float32), array([             nan,   1.03393984e+00,   1.55473717e-05,\n",
      "         1.38570380e+00,   8.76323085e-08,   7.97228097e-08], dtype=float32), array([  9.57117217e-30,   6.93890393e-01,   7.25431892e-05,\n",
      "         1.22529995e+00,   6.90657544e-07,   6.62108619e-07], dtype=float32), array([ 0.        ,  1.3682735 ,  0.05860979,  1.38093925,  0.01070145,\n",
      "        0.01014933], dtype=float32), array([        nan,  1.04979658,  0.69341123,  1.38266838,  0.69315302,\n",
      "        0.69315231], dtype=float32), array([  0.00000000e+00,   1.18440437e+00,   5.35272022e-07,\n",
      "         1.27613592e+00,   6.40230202e-10,   6.22354279e-10], dtype=float32), array([             nan,   6.66353572e-03,   7.55943600e-35,\n",
      "         1.37620592e+00,   0.00000000e+00,   0.00000000e+00], dtype=float32), array([  0.00000000e+00,   7.83565640e-01,   1.27780410e-07,\n",
      "         1.17625165e+00,   1.71245726e-10,   1.43894507e-10], dtype=float32), array([ 0.69314718,  0.77068698,  0.69314718,  1.34383571,  0.69314718,\n",
      "        0.69314718], dtype=float32), array([ 0.17994076,  0.68004721,  0.68426943,  1.00359738,  0.66596496,\n",
      "        0.6696887 ], dtype=float32), array([  8.30599888e-31,   9.43723440e-01,   4.94787309e-05,\n",
      "         1.18607855e+00,   4.50411989e-07,   4.18160624e-07], dtype=float32)]\n",
      "On the test set:\n",
      "Moyenne des entropies: [        nan  0.91178995  0.21968855  1.22662997  0.19738838  0.19494857]\n",
      "Corrects: 4.0%\n",
      "False:    96.0%\n",
      "\n",
      "A présent, tu peux copier-coller ça dans le doc sur le drive :)\n",
      "100\t0.0001\t10\t5\t10\t4\t\t-1\t46.3888169816205%\t4.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn.eval()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "final_results = getEfficience(rnn, train_loader)\n",
    "total = sum(final_results[:2])\n",
    "\n",
    "correct_train = final_results[0]/total*100\n",
    "false_train = final_results[1]/total*100\n",
    "\n",
    "\n",
    "print(\"Congratulations!\")\n",
    "print(\"On the training set:\")\n",
    "print(\"Corrects: \" + str(correct_train) + \"%\")\n",
    "print(\"False:    \" + str(false_train) + \"%\")\n",
    "print()\n",
    "\n",
    "seeding_random()\n",
    "\n",
    "final_results = getEfficience(rnn, test_loader)\n",
    "total = sum(final_results[:2])\n",
    "\n",
    "correct = final_results[0]/total*100\n",
    "false = final_results[1]/total*100\n",
    "mean_entropies = sum(final_results[2])/len(final_results[2])\n",
    "print(final_results[2])\n",
    "#Entropy mean might be near to zero\n",
    "print(\"On the test set:\")\n",
    "print(\"Moyenne des entropies: \" + str(mean_entropies))\n",
    "print(\"Corrects: \" + str(correct) + \"%\")\n",
    "print(\"False:    \" + str(false) + \"%\")\n",
    "\n",
    "mean_entropies = -1\n",
    "print()\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(nb_input)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(nb_hidd_lay)+\"\\t\"+str(hidden_size)+\"\\t\"+str(nb_batchs)+\"\\t\\t\"+str(mean_entropies)+\"\\t\"+str(correct_train)+\"%\\t\"+str(correct)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
