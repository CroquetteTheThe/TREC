{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, cross_val_score\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 100 #8414 max\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 1\n",
    "hidden_size = 1\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 10\n",
    "\n",
    "# Random seed, don't change it if you don't know what it is\n",
    "random_seed = 42\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "train_data = []\n",
    "for line in file.readlines():\n",
    "    train_data.append(line)\n",
    "\n",
    "# Now test set\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "# Working on list_of_words\n",
    "# TODO: it doesn't work\n",
    "def lemm(phrase):\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lemmed = []\n",
    "    for word in phrase:\n",
    "        lemmed.append(lemmer.lemmatize(word, pos=\"v\"))\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(questions) = 15452\n",
      "Exemples of questions:\n",
      "[['how', 'do', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?'], ['what', 'film', 'feature', 'the', 'character', 'popeye', 'doyle', '?']]\n",
      "\n",
      "And in the test set:\n",
      "len(questions_test) = 500\n",
      "Exemples of questions:\n",
      "[['how', 'far', 'be', 'it', 'from', 'denver', 'to', 'aspen', '?'], ['what', 'county', 'be', 'modesto', ',', 'california', 'in', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Divided file into 2 list:\n",
    "# questions = list of questions \n",
    "# labels = list of labels\n",
    "\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "# Black list\n",
    "regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "\n",
    "(train_data[0]).split()[0]\n",
    "for string in train_data:\n",
    "    question_str = []\n",
    "    for x in lemm(string.split()[1:]):\n",
    "        question_str.append(x.lower())\n",
    "    labels.append(string.split()[0])\n",
    "    questions.append(question_str)\n",
    "print(\"len(questions) = \" + str(len(questions)))\n",
    "print(\"Exemples of questions:\")\n",
    "print(questions[:2])\n",
    "\n",
    "\n",
    "\n",
    "questions_test = []\n",
    "labels_test = []\n",
    "\n",
    "(test_data[0]).split()[0]\n",
    "for string in test_data:\n",
    "    question_str = []\n",
    "    for x in lemm(string.split()[1:]):\n",
    "        question_str.append(x.lower())\n",
    "    labels_test.append(string.split()[0])\n",
    "    questions_test.append(question_str)\n",
    "print()\n",
    "print(\"And in the test set:\")\n",
    "print(\"len(questions_test) = \" + str(len(questions_test)))\n",
    "print(\"Exemples of questions:\")\n",
    "print(questions_test[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to add tags begin and end to phrases list.\n",
    "# /!\\ \n",
    "# WARNING : this method need to be executed only ONE time.\n",
    "# /!\\\n",
    "def add_tag(question_list):\n",
    "    for i in range(0, len(question_list)):\n",
    "        if question_list[i][0] != '<bos>' :\n",
    "            question_list[i].insert(0, '<bos>')\n",
    "            question_list[i].append('<eos>')\n",
    "add_tag(questions)\n",
    "add_tag(questions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188317\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary of unique words\n",
    "data = []\n",
    "data.append('<unk>')\n",
    "for q in questions:\n",
    "    for word in q:\n",
    "        data.append(word)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv = np.array(data)\n",
    "unik, counts = np.unique(scv,return_counts=True)\n",
    "vocabulary = {}\n",
    "for i in range(0, len(unik)):\n",
    "    vocabulary[unik[i]] = counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 100 words.\n"
     ]
    }
   ],
   "source": [
    "word_list = list([x[0] for x in Counter(vocabulary).most_common(nb_input-1)])\n",
    "\n",
    "# We add the unk word for future purpose.\n",
    "word_list.append('<unk>')\n",
    "words_array = np.array(word_list)\n",
    "print(\"Vocabulary contains\", len(words_array), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer encoding with OneHotEncoder\n",
    "words_tre = words_array.reshape(len(words_array),1)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "#print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionnary of word and its one hot array\n",
    "words_onehoted = {}\n",
    "for i in range(0, len(words_array)):\n",
    "    words_onehoted[word_list[i]] = onehot_encoded[i]\n",
    "\n",
    "# Function to get the corresponding one hot list for a word.\n",
    "def get_onehot_word(word):\n",
    "    if word in words_onehoted:\n",
    "        return list(words_onehoted[word])\n",
    "    else:\n",
    "        return list(words_onehoted['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_word('<unk>')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if an unknown word is transformed into a <unk>\n",
    "get_onehot_word('obviously_an_unknown_word').index(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot categories\n",
    "categories_onehoted = {}\n",
    "categories_onehoted['ABBR'] = [1, 0, 0, 0, 0, 0] # Abbreviation\n",
    "categories_onehoted['ENTY'] = [0, 1, 0, 0, 0, 0] # Entity\n",
    "categories_onehoted['DESC'] = [0, 0, 1, 0, 0, 0] # Description\n",
    "categories_onehoted['HUM']  = [0, 0, 0, 1, 0, 0] # Human\n",
    "categories_onehoted['LOC']  = [0, 0, 0, 0, 1, 0] # Location\n",
    "categories_onehoted['NUM']  = [0, 0, 0, 0, 0, 1] # Numeric\n",
    "\n",
    "# Function to get the corresponding one hot list for a category.\n",
    "def get_onehot_category(category):\n",
    "    return categories_onehoted[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_category('HUM')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15452\n",
      "500\n",
      "CPU times: user 1.61 s, sys: 34.5 ms, total: 1.64 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating training set\n",
    "\n",
    "batch_data = []\n",
    "for num_question in range(len(questions)):\n",
    "    # Construction of question_onehot list.\n",
    "    question_onehot = [get_onehot_word(word) for word in questions[num_question]]\n",
    "    \n",
    "    # Construction of category_onehot.\n",
    "    category = labels[num_question].partition(':')[0]\n",
    "    category_onehot = get_onehot_category(category)\n",
    "    batch_data.append([tensor([question_onehot]), tensor([category_onehot])])    \n",
    "print(len(batch_data))\n",
    "# Creating test set\n",
    "\n",
    "batch_data_test = []\n",
    "for num_question in range(len(questions_test)):\n",
    "    \n",
    "    # Construction of question_onehot list.\n",
    "    question_onehot = [get_onehot_word(word) for word in questions[num_question]]\n",
    "    \n",
    "    # Construction of category_onehot.\n",
    "    category = labels_test[num_question].partition(':')[0]\n",
    "    category_onehot = get_onehot_category(category)\n",
    "    batch_data_test.append([tensor([question_onehot]), tensor([category_onehot])])\n",
    "print(len(batch_data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dev / Train\n",
    "batch_dev = batch_data[10000:]\n",
    "batch_train = batch_data[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0., batch_first=True, nonlinearity='relu')\n",
    "        self.inter = nn.Linear(nb_neurons, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = Variable(torch.zeros(self.nb_layers, inputs.size(0), self.nb_neurons))\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        x, hn = self.rnn(inputs, h0)\n",
    "        \n",
    "        x = self.inter(x)\n",
    "        x = self.sm(x)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, batch_list, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffling batch_list\n",
    "        shuffle(batch_list)\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(batch_list):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = rnn(data)\n",
    "            loss = criterion(output, target)\n",
    "            losses.append(loss.data.item())\n",
    "            rnn.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 100 == 0 or batch_idx % 100 == 1 or batch_idx == len(batch_list)-1:\n",
    "                print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                        epoch, \n",
    "                        (batch_idx+1) * len(data), \n",
    "                        len(batch_list),\n",
    "                        100. * (batch_idx+1) / len(batch_list), \n",
    "                        loss.data.item()), \n",
    "                        end='')\n",
    "        print()\n",
    "        \n",
    "    # Return losses list, you can print them later if you want\n",
    "    return losses\n",
    "\n",
    "\n",
    "# return (rightAnswer, ignored, falseAnswer)\n",
    "def getEfficience(rnn, batch_list, tresh=0) :\n",
    "    rightAnswer = 0\n",
    "    ignored = 0\n",
    "    falseAnswer = 0\n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        predicted = rnn(data).detach().cpu().numpy()[-1][-1]\n",
    "        #print(\"predicted: \"+str(np.argmax(predicted)))\n",
    "        #print(\"target: \"+str(np.argmax(target.detach().cpu().numpy()[-1])))\n",
    "        if max(predicted) < tresh :\n",
    "            ignored += 1\n",
    "        else:\n",
    "            if np.argmax(predicted) == np.argmax(target.detach().cpu().numpy()[-1]):\n",
    "                rightAnswer += 1\n",
    "            else:\n",
    "                falseAnswer += 1\n",
    "    return (rightAnswer, ignored, falseAnswer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, nb_input, nb_output, nb_hidd_lay, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = nb_hidd_lay\n",
    "        self.lstm = nn.LSTM(nb_input,hidden_size,nb_hidd_lay, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, nb_output)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        h0 = Variable(torch.zeros(self.nb_hidd_lay, inputs.size(0), self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.nb_hidd_lay, inputs.size(0), self.hidden_size))\n",
    "        \n",
    "        out, _ = self.lstm(inputs,(h0,c0))\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 0 [5452/5452 (100%)]\t Loss: 2.496161\n",
      " Train Epoch: 1 [5452/5452 (100%)]\t Loss: 2.382797\n",
      " Train Epoch: 2 [5452/5452 (100%)]\t Loss: 2.624322\n",
      " Train Epoch: 3 [5452/5452 (100%)]\t Loss: 2.619894\n",
      " Train Epoch: 4 [5452/5452 (100%)]\t Loss: 2.260918\n",
      " Train Epoch: 5 [5452/5452 (100%)]\t Loss: 2.209827\n",
      " Train Epoch: 6 [5452/5452 (100%)]\t Loss: 3.138693\n",
      " Train Epoch: 7 [5452/5452 (100%)]\t Loss: 2.853784\n",
      " Train Epoch: 8 [5452/5452 (100%)]\t Loss: 2.438035\n",
      " Train Epoch: 9 [5452/5452 (100%)]\t Loss: 2.410218\n",
      "Done :)\n",
      "-------------------\n",
      " Train Epoch: 0 [10000/10000 (100%)]\t Loss: 2.565804\n",
      " Train Epoch: 1 [10000/10000 (100%)]\t Loss: 2.053995\n",
      " Train Epoch: 2 [10000/10000 (100%)]\t Loss: 2.708793\n",
      " Train Epoch: 3 [10000/10000 (100%)]\t Loss: 2.152769\n",
      " Train Epoch: 4 [10000/10000 (100%)]\t Loss: 2.652014\n",
      " Train Epoch: 5 [10000/10000 (100%)]\t Loss: 2.048925\n",
      " Train Epoch: 6 [10000/10000 (100%)]\t Loss: 2.789239\n",
      " Train Epoch: 7 [10000/10000 (100%)]\t Loss: 2.492803\n",
      " Train Epoch: 8 [10000/10000 (100%)]\t Loss: 2.483818\n",
      " Train Epoch: 9 [10000/10000 (100%)]\t Loss: 2.329366\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "rnn = RNN(nb_inputs = nb_input, nb_layers=nb_hidd_lay,\n",
    "          nb_neurons=hidden_size,nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "losses = learn(rnn, batch_dev, nb_epochs)\n",
    "print(\"Done :)\")\n",
    "print('-------------------')\n",
    "losses_train = learn(rnn, batch_train, nb_epochs)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1300x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651f76a3831c4c24840f1f7bfa68a037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=51, description='smooth', max=201, min=5, step=2), Output()), _dom_class…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "x = np.linspace(0, len(losses), len(losses))\n",
    "fig = plt.figure(figsize=(13, 8)) \n",
    "ax = fig.add_subplot(1,1,1)\n",
    "cnn_line, = ax.plot(x, losses)\n",
    "\n",
    "def update_losses(smooth=51):\n",
    "    cnn_line.set_ydata(savgol_filter(losses, smooth, 3))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(update_losses, smooth=(5, 201, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! On the training set:\n",
      "Corrects: 1.5596686513072742%\n",
      "False:    98.44033134869272%\n",
      "\n",
      "Congratulations! On the test set:\n",
      "Corrects: 1.7999999999999998%\n",
      "False:    98.2%\n",
      "\n",
      "A présent, tu peux copier-coller ça dans le doc sur le drive :)\n",
      "100\t0.001\t10\t1\t1\t\t1.7999999999999998%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "\n",
    "rnn.eval()\n",
    "\n",
    "final_results = getEfficience(rnn, batch_data)\n",
    "total = sum(final_results)\n",
    "\n",
    "correct = final_results[0]/total*100\n",
    "ignored = final_results[1]/total*100\n",
    "false = final_results[2]/total*100\n",
    "\n",
    "\n",
    "\n",
    "print(\"Congratulations! On the training set:\")\n",
    "print(\"Corrects: \" + str(correct) + \"%\")\n",
    "#print(\"Ignored:  \" + str(ignored) + \"%\")\n",
    "print(\"False:    \" + str(false) + \"%\")\n",
    "print()\n",
    "\n",
    "final_results = getEfficience(rnn, batch_data_test)\n",
    "total = sum(final_results)\n",
    "\n",
    "correct = final_results[0]/total*100\n",
    "ignored = final_results[1]/total*100\n",
    "false = final_results[2]/total*100\n",
    "\n",
    "\n",
    "\n",
    "print(\"Congratulations! On the test set:\")\n",
    "print(\"Corrects: \" + str(correct) + \"%\")\n",
    "#print(\"Ignored:  \" + str(ignored) + \"%\")\n",
    "print(\"False:    \" + str(false) + \"%\")\n",
    "\n",
    "print()\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(nb_input)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(nb_hidd_lay)+\"\\t\"+str(hidden_size)+\"\\t\\t\"+str(correct)+\"%\")\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
