{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 500 #8414 max\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 2\n",
    "hidden_size = 3\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 10\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "train_data = []\n",
    "for line in file.readlines():\n",
    "    train_data.append(line)\n",
    "\n",
    "# Now test set\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "# Working on list_of_words\n",
    "# TODO: it doesn't work\n",
    "def lemm(phrase):\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lemmed = []\n",
    "    for word in phrase:\n",
    "        lemmed.append(lemmer.lemmatize(word, pos=\"v\"))\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(questions) = 15452\n",
      "Exemples of questions:\n",
      "[['How', 'do', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'Russia', '?'], ['What', 'film', 'feature', 'the', 'character', 'Popeye', 'Doyle', '?']]\n",
      "\n",
      "And in the test set:\n",
      "len(questions_test) = 500\n",
      "Exemples of questions:\n",
      "[['How', 'far', 'be', 'it', 'from', 'Denver', 'to', 'Aspen', '?'], ['What', 'county', 'be', 'Modesto', ',', 'California', 'in', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Divided file into 2 list:\n",
    "# questions = list of questions \n",
    "# labels = list of labels\n",
    "\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "(train_data[0]).split()[0]\n",
    "for string in train_data:\n",
    "    labels.append(string.split()[0])\n",
    "    questions.append(lemm(string.split()[1:]))\n",
    "print(\"len(questions) = \" + str(len(questions)))\n",
    "print(\"Exemples of questions:\")\n",
    "print(questions[:2])\n",
    "\n",
    "\n",
    "\n",
    "questions_test = []\n",
    "labels_test = []\n",
    "\n",
    "(test_data[0]).split()[0]\n",
    "for string in test_data:\n",
    "    labels_test.append(string.split()[0])\n",
    "    questions_test.append(lemm(string.split()[1:]))\n",
    "print()\n",
    "print(\"And in the test set:\")\n",
    "print(\"len(questions_test) = \" + str(len(questions_test)))\n",
    "print(\"Exemples of questions:\")\n",
    "print(questions_test[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to add tags begin and end to phrases list.\n",
    "# /!\\ \n",
    "# WARNING : this method need to be executed only ONE time.\n",
    "# /!\\\n",
    "def add_tag(question_list):\n",
    "    for i in range(0, len(question_list)):\n",
    "        if question_list[i][0] != '<bos>' :\n",
    "            question_list[i].insert(0, '<bos>')\n",
    "            question_list[i].append('<eos>')\n",
    "add_tag(questions)\n",
    "add_tag(questions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT\n",
    "# # Using sklearn to get the vocabulary (only on the training set)\n",
    "# vectorizer = CountVectorizer(token_pattern=\"(<\\\\w\\\\w\\\\w>|(?u)\\\\b\\\\w\\\\w+\\\\b)\")\n",
    "# str_questions = []\n",
    "# for quest in questions:\n",
    "#     str_questions.append(' '.join(quest))\n",
    "# output = vectorizer.fit(str_questions)\n",
    "\n",
    "# # This is the vocabulary dict.\n",
    "# vocabulary = output.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188317\n",
      "CPU times: user 81.9 ms, sys: 0 ns, total: 81.9 ms\n",
      "Wall time: 80.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vocabulary of unique words\n",
    "data = []\n",
    "data.append('<unk>')\n",
    "regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "for i in range(0, len(questions)):\n",
    "    q = questions[i]\n",
    "    for word in q:\n",
    "        if regex.search(word) == None:\n",
    "            data.append(word.lower())\n",
    "        else:\n",
    "            data.append(word)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv = np.array(data)\n",
    "unik, counts = np.unique(scv,return_counts=True)\n",
    "vocabulary = {}\n",
    "for i in range(0, len(unik)):\n",
    "    vocabulary[unik[i]] = counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 23,\n",
       " '#1': 5,\n",
       " '$': 3,\n",
       " '$1': 8,\n",
       " '$15': 3,\n",
       " '$200': 4,\n",
       " '$28': 4,\n",
       " '$5': 9,\n",
       " '$836': 3,\n",
       " '$85': 5,\n",
       " '&': 36,\n",
       " \"'\": 206,\n",
       " \"''\": 1201,\n",
       " \"'-lantern\": 3,\n",
       " \"'50s\": 1,\n",
       " \"'70\": 1,\n",
       " \"'clock\": 4,\n",
       " \"'d\": 2,\n",
       " \"'em\": 4,\n",
       " \"'etat\": 3,\n",
       " \"'hara\": 5,\n",
       " \"'l\": 6,\n",
       " \"'ll\": 16,\n",
       " \"'m\": 1,\n",
       " \"'n\": 8,\n",
       " \"'neal\": 2,\n",
       " \"'re\": 9,\n",
       " \"'s\": 2051,\n",
       " \"'t\": 61,\n",
       " \"'the\": 3,\n",
       " \"'ve\": 7,\n",
       " ',': 1572,\n",
       " '-': 35,\n",
       " '.': 333,\n",
       " '...': 3,\n",
       " '...the': 4,\n",
       " '.com': 5,\n",
       " '.dbf': 5,\n",
       " '.tbk': 1,\n",
       " '0': 29,\n",
       " '000': 7,\n",
       " '000-a-year': 5,\n",
       " '000th': 4,\n",
       " '007': 1,\n",
       " '1': 58,\n",
       " '1%': 1,\n",
       " '1-cent': 7,\n",
       " '1-dollar': 4,\n",
       " '1-millionth': 3,\n",
       " '1.76': 2,\n",
       " '10': 26,\n",
       " '10%': 1,\n",
       " '10-??': 5,\n",
       " '100': 3,\n",
       " '100-page': 1,\n",
       " '103': 5,\n",
       " '11': 22,\n",
       " '111': 1,\n",
       " '118': 5,\n",
       " '11th-century': 3,\n",
       " '12': 16,\n",
       " '12-hour': 6,\n",
       " '12-year-old': 5,\n",
       " '123': 3,\n",
       " '126': 5,\n",
       " '12601': 2,\n",
       " '13': 14,\n",
       " '134': 5,\n",
       " '137': 1,\n",
       " '139': 1,\n",
       " '14': 6,\n",
       " '14-mile': 5,\n",
       " '142': 5,\n",
       " '15': 18,\n",
       " '15-minute': 3,\n",
       " '15-year-old': 3,\n",
       " '155': 2,\n",
       " '1562': 2,\n",
       " '158': 5,\n",
       " '15th': 4,\n",
       " '16': 10,\n",
       " '16-year': 4,\n",
       " '1603': 2,\n",
       " '1642-1649': 6,\n",
       " '165': 4,\n",
       " '1669': 1,\n",
       " '167': 5,\n",
       " '168': 2,\n",
       " '1699-172': 2,\n",
       " '16th': 13,\n",
       " '16th-century': 4,\n",
       " '17': 6,\n",
       " '173': 2,\n",
       " '175': 4,\n",
       " '1767-1834': 2,\n",
       " '177': 5,\n",
       " '1779': 5,\n",
       " '1781': 2,\n",
       " '1787': 3,\n",
       " '1789': 1,\n",
       " '1797-185': 9,\n",
       " '18': 11,\n",
       " '1800s': 1,\n",
       " '1812': 5,\n",
       " '1815': 1,\n",
       " '1830': 4,\n",
       " '1832': 5,\n",
       " '1835': 3,\n",
       " '1842': 1,\n",
       " '1847': 6,\n",
       " '1849': 2,\n",
       " '1853': 3,\n",
       " '1857': 1,\n",
       " '1863': 2,\n",
       " '1865': 3,\n",
       " '1866': 5,\n",
       " '1870': 2,\n",
       " '1872': 5,\n",
       " '1873': 4,\n",
       " '1879': 4,\n",
       " '187s': 5,\n",
       " '188': 2,\n",
       " '1880': 4,\n",
       " '1881': 4,\n",
       " '1885': 4,\n",
       " '1886': 2,\n",
       " '1891': 4,\n",
       " '1892': 1,\n",
       " '1893': 5,\n",
       " '1895': 1,\n",
       " '1896': 3,\n",
       " '1898': 1,\n",
       " '1899': 23,\n",
       " '19': 17,\n",
       " '1900s': 2,\n",
       " '191': 1,\n",
       " '1912': 2,\n",
       " '1913': 4,\n",
       " '1915': 4,\n",
       " '1916': 1,\n",
       " '1919': 1,\n",
       " '192': 3,\n",
       " '1920s': 10,\n",
       " '1922': 3,\n",
       " '1923': 9,\n",
       " '1925': 1,\n",
       " '1926': 5,\n",
       " '1927': 1,\n",
       " '1928': 2,\n",
       " '1929': 7,\n",
       " '193': 5,\n",
       " '1930s': 4,\n",
       " '1931': 3,\n",
       " '1932': 5,\n",
       " '1933': 6,\n",
       " '1935': 6,\n",
       " '1936': 7,\n",
       " '1937': 2,\n",
       " '1939': 17,\n",
       " '194': 2,\n",
       " '1940': 8,\n",
       " '1940s': 3,\n",
       " '1941': 7,\n",
       " '1942': 12,\n",
       " '1943': 2,\n",
       " '1945': 7,\n",
       " '1946': 4,\n",
       " '1948': 8,\n",
       " '195': 5,\n",
       " '1950': 8,\n",
       " '1951': 7,\n",
       " '1952': 7,\n",
       " '1953': 11,\n",
       " '1954': 3,\n",
       " '1955': 1,\n",
       " '1956': 7,\n",
       " '1956-1971': 3,\n",
       " '1957': 4,\n",
       " '1958': 6,\n",
       " '1959': 2,\n",
       " '196': 5,\n",
       " '1960': 12,\n",
       " \"1960's\": 4,\n",
       " '1960s': 1,\n",
       " '1961': 4,\n",
       " '1962': 5,\n",
       " '1963': 18,\n",
       " '1964': 8,\n",
       " '1965': 16,\n",
       " '1966': 8,\n",
       " '1967': 16,\n",
       " '1968': 7,\n",
       " '1969': 16,\n",
       " '197': 8,\n",
       " '1970s': 1,\n",
       " '1971': 7,\n",
       " '1972': 15,\n",
       " '1973': 11,\n",
       " '1974': 5,\n",
       " '1975': 6,\n",
       " '1976': 5,\n",
       " '1977': 5,\n",
       " '1978': 13,\n",
       " '1979': 6,\n",
       " '198': 3,\n",
       " '1980': 16,\n",
       " '1980s': 1,\n",
       " '1981': 11,\n",
       " '1982': 9,\n",
       " '1983': 18,\n",
       " '1984': 29,\n",
       " '1985': 6,\n",
       " '1988': 4,\n",
       " '1989': 12,\n",
       " '199': 6,\n",
       " '1990': 9,\n",
       " '1990s': 5,\n",
       " '1991': 24,\n",
       " '1992': 8,\n",
       " '1993': 7,\n",
       " '1994': 16,\n",
       " '1996': 3,\n",
       " '1997': 4,\n",
       " '1998': 15,\n",
       " '1999': 9,\n",
       " '19th': 7,\n",
       " '19th-century': 10,\n",
       " '1st': 1,\n",
       " '2': 48,\n",
       " '2-foot-square': 5,\n",
       " '2-sided': 2,\n",
       " '2.5': 2,\n",
       " '20-game': 1,\n",
       " '2000': 10,\n",
       " '2001': 1,\n",
       " '2010': 2,\n",
       " '2020': 2,\n",
       " '20th': 2,\n",
       " '21': 18,\n",
       " '21-27': 4,\n",
       " '2112': 2,\n",
       " '219': 5,\n",
       " '22': 10,\n",
       " '22-287-5': 1,\n",
       " '227': 4,\n",
       " '23': 5,\n",
       " '239': 2,\n",
       " '24': 12,\n",
       " '24-disc': 2,\n",
       " '24-hour': 5,\n",
       " '25': 10,\n",
       " '25th': 4,\n",
       " '26': 5,\n",
       " '26-year': 3,\n",
       " '27': 11,\n",
       " '28': 1,\n",
       " '280': 5,\n",
       " '2th': 3,\n",
       " '2th-century': 5,\n",
       " '3': 27,\n",
       " '3-pin': 2,\n",
       " '31': 4,\n",
       " '313': 2,\n",
       " '327': 2,\n",
       " '33': 3,\n",
       " '35': 3,\n",
       " '35-millimeter': 4,\n",
       " '36893': 1,\n",
       " '37803': 2,\n",
       " '387': 3,\n",
       " '39': 6,\n",
       " '3rd': 1,\n",
       " '4': 9,\n",
       " '4-foot-9': 2,\n",
       " '401': 5,\n",
       " '42.3': 5,\n",
       " '43rd': 2,\n",
       " '455-yard': 4,\n",
       " '45mhz': 4,\n",
       " '46': 4,\n",
       " '468-pound': 5,\n",
       " '47': 1,\n",
       " '48': 2,\n",
       " '48th': 2,\n",
       " '49': 9,\n",
       " '4th': 4,\n",
       " '5': 27,\n",
       " '5%': 1,\n",
       " '5.9': 2,\n",
       " '500': 3,\n",
       " '52': 4,\n",
       " '528': 1,\n",
       " '55': 6,\n",
       " '56-game': 1,\n",
       " '576': 4,\n",
       " '5th': 9,\n",
       " '6': 22,\n",
       " '600': 3,\n",
       " '61': 2,\n",
       " '64': 3,\n",
       " '69': 2,\n",
       " '6th': 13,\n",
       " '7': 19,\n",
       " '7-eleven': 2,\n",
       " '7-minute': 5,\n",
       " '72': 4,\n",
       " '732': 2,\n",
       " '737': 4,\n",
       " '740-tree': 6,\n",
       " '755': 3,\n",
       " '7847+5943': 3,\n",
       " '7th': 2,\n",
       " '8': 23,\n",
       " '8/28/1941': 4,\n",
       " '80': 5,\n",
       " '80%': 1,\n",
       " '84': 5,\n",
       " '86': 4,\n",
       " '864': 2,\n",
       " '86ed': 5,\n",
       " '8th': 1,\n",
       " '9': 5,\n",
       " '9-8-98': 2,\n",
       " '90': 1,\n",
       " '900': 6,\n",
       " '924': 1,\n",
       " '95': 1,\n",
       " '98': 8,\n",
       " '985': 4,\n",
       " '9971': 3,\n",
       " '999': 2,\n",
       " '9th': 4,\n",
       " ':': 198,\n",
       " ';': 12,\n",
       " '<bos>': 15452,\n",
       " '<eos>': 15452,\n",
       " '<unk>': 1,\n",
       " '?': 15131,\n",
       " 'A&W': 3,\n",
       " 'A_Tisket': 1,\n",
       " 'M&Ms': 1,\n",
       " 'R&B': 2,\n",
       " 'S&P': 1,\n",
       " '`': 121,\n",
       " '``': 1199,\n",
       " 'a': 2898,\n",
       " 'a-tasket': 1,\n",
       " 'a.': 1,\n",
       " 'a.g.': 5,\n",
       " 'a.m.': 4,\n",
       " 'aaa': 1,\n",
       " 'aaron': 17,\n",
       " 'abacus': 2,\n",
       " 'abandon': 2,\n",
       " 'abbey': 3,\n",
       " 'abbie': 2,\n",
       " 'abbreviate': 12,\n",
       " 'abbreviation': 54,\n",
       " 'abby': 4,\n",
       " 'abdication': 4,\n",
       " 'abdominal': 1,\n",
       " 'abigail': 3,\n",
       " 'abilities': 2,\n",
       " 'ability': 4,\n",
       " 'abner': 6,\n",
       " 'abolish': 2,\n",
       " 'abolitionists': 1,\n",
       " 'abominable': 5,\n",
       " 'aborigines': 1,\n",
       " 'about': 158,\n",
       " 'abraham': 7,\n",
       " 'absolute': 4,\n",
       " 'abstract': 4,\n",
       " 'abundant': 4,\n",
       " 'abuse': 7,\n",
       " 'abzug': 2,\n",
       " 'academy': 12,\n",
       " 'academy-award-winning': 1,\n",
       " 'accept': 2,\n",
       " 'acceptance': 2,\n",
       " 'access': 7,\n",
       " 'accessory': 5,\n",
       " 'accident': 2,\n",
       " 'accidents': 5,\n",
       " 'accommodate': 1,\n",
       " 'accompany': 7,\n",
       " 'accord': 9,\n",
       " 'account': 7,\n",
       " 'accuse': 1,\n",
       " 'ace': 3,\n",
       " 'acetylsalicylic': 5,\n",
       " 'acheive': 2,\n",
       " 'achieve': 1,\n",
       " 'achievement': 1,\n",
       " 'achievements': 3,\n",
       " 'acid': 21,\n",
       " 'acid-base': 3,\n",
       " 'acidic': 3,\n",
       " 'aclu': 3,\n",
       " 'acne': 2,\n",
       " 'acoustic': 2,\n",
       " 'acquaintance': 2,\n",
       " 'acquit': 3,\n",
       " 'acreage': 5,\n",
       " 'acres': 4,\n",
       " 'acronym': 7,\n",
       " 'across': 10,\n",
       " 'act': 35,\n",
       " 'action': 5,\n",
       " 'activate': 2,\n",
       " 'active': 8,\n",
       " 'activity': 1,\n",
       " 'actor': 87,\n",
       " 'actors': 4,\n",
       " 'actress': 67,\n",
       " 'actual': 5,\n",
       " 'actually': 5,\n",
       " 'ad': 15,\n",
       " 'adam': 2,\n",
       " 'adapter': 3,\n",
       " 'add': 13,\n",
       " 'address': 66,\n",
       " 'adenauer': 2,\n",
       " 'adjacent': 1,\n",
       " 'adjoin': 4,\n",
       " 'adjournment': 4,\n",
       " 'administration': 5,\n",
       " 'admiral': 5,\n",
       " 'admit': 9,\n",
       " 'admonition': 5,\n",
       " 'ado': 1,\n",
       " 'adopt': 11,\n",
       " 'adoptive': 2,\n",
       " 'adorn': 4,\n",
       " 'ads': 12,\n",
       " 'adult': 12,\n",
       " 'adults': 1,\n",
       " 'advance': 5,\n",
       " 'advantage': 3,\n",
       " 'adventours': 1,\n",
       " 'adventure': 8,\n",
       " 'adventures': 5,\n",
       " 'advertise': 25,\n",
       " 'advertize': 5,\n",
       " 'advise': 4,\n",
       " 'advisory': 1,\n",
       " 'advocate': 1,\n",
       " 'aerodynamics': 2,\n",
       " 'aesop': 3,\n",
       " 'aeul': 4,\n",
       " 'affair': 4,\n",
       " 'affairs': 2,\n",
       " 'affect': 18,\n",
       " 'affectionate': 5,\n",
       " 'affiant': 2,\n",
       " 'afflict': 1,\n",
       " 'afghanistan': 7,\n",
       " 'afoot': 2,\n",
       " 'afraid': 2,\n",
       " 'africa': 34,\n",
       " 'african': 24,\n",
       " 'afs': 3,\n",
       " 'after': 88,\n",
       " 'after-dinner': 1,\n",
       " 'afternoon': 6,\n",
       " 'aftra': 1,\n",
       " 'again': 1,\n",
       " 'against': 16,\n",
       " 'age': 66,\n",
       " 'agencies': 1,\n",
       " 'agency': 6,\n",
       " 'agent': 6,\n",
       " 'agents': 4,\n",
       " 'ago': 13,\n",
       " 'agra': 1,\n",
       " 'agree': 5,\n",
       " 'agreement': 3,\n",
       " 'agricultural': 3,\n",
       " 'ahead': 5,\n",
       " 'aid': 6,\n",
       " 'aids': 16,\n",
       " 'ailment': 4,\n",
       " 'aim': 1,\n",
       " 'aim-54c': 5,\n",
       " 'air': 48,\n",
       " 'airborne': 5,\n",
       " 'aircraft': 4,\n",
       " 'airforce': 1,\n",
       " 'airline': 5,\n",
       " 'airliners': 1,\n",
       " 'airman': 4,\n",
       " 'airplane': 11,\n",
       " 'airplanes': 3,\n",
       " 'airport': 43,\n",
       " 'airports': 9,\n",
       " 'airwaves': 5,\n",
       " 'akita': 5,\n",
       " 'al': 12,\n",
       " 'al-farri': 5,\n",
       " 'alabama': 1,\n",
       " 'aladdin': 3,\n",
       " 'alamein': 4,\n",
       " 'alan': 3,\n",
       " 'alaska': 23,\n",
       " 'albee': 1,\n",
       " 'albert': 6,\n",
       " 'album': 14,\n",
       " 'albums': 3,\n",
       " 'alcatraz': 1,\n",
       " 'alcohol': 2,\n",
       " 'alcoholic': 3,\n",
       " 'alda': 3,\n",
       " 'aldous': 4,\n",
       " 'aldrin': 9,\n",
       " 'alexander': 3,\n",
       " 'alexandra': 7,\n",
       " 'alexandre': 4,\n",
       " 'algeria': 3,\n",
       " 'algiers': 4,\n",
       " 'ali': 9,\n",
       " 'alice': 11,\n",
       " 'alive': 5,\n",
       " 'all': 126,\n",
       " 'all-star': 4,\n",
       " 'all-time': 6,\n",
       " 'allah': 5,\n",
       " 'allan': 4,\n",
       " 'allege': 3,\n",
       " 'allegedly': 4,\n",
       " 'allen': 9,\n",
       " 'alley': 14,\n",
       " 'alliance': 5,\n",
       " 'allies': 1,\n",
       " 'allow': 18,\n",
       " 'alloy': 1,\n",
       " 'allsburg': 2,\n",
       " 'ally': 2,\n",
       " 'almanac': 3,\n",
       " 'almost': 7,\n",
       " 'alone': 5,\n",
       " 'along': 14,\n",
       " 'alpert': 2,\n",
       " 'alpha': 4,\n",
       " 'alphabet': 14,\n",
       " 'alphabetical': 2,\n",
       " 'alphabetically': 4,\n",
       " 'also': 20,\n",
       " 'alter': 3,\n",
       " 'alternate': 3,\n",
       " 'alternative': 3,\n",
       " 'alternator': 5,\n",
       " 'alveoli': 3,\n",
       " 'alvin': 5,\n",
       " 'always': 22,\n",
       " 'alyssa': 5,\n",
       " 'am': 5,\n",
       " 'amaretto': 4,\n",
       " 'amateur': 6,\n",
       " 'amazing': 4,\n",
       " 'amazon': 4,\n",
       " 'amazonis': 3,\n",
       " 'amazons': 1,\n",
       " 'ambassador': 5,\n",
       " 'ambassadorial': 3,\n",
       " 'amelia': 3,\n",
       " 'amen': 2,\n",
       " 'amend': 3,\n",
       " 'amendements': 4,\n",
       " 'amendment': 21,\n",
       " 'america': 103,\n",
       " 'american': 217,\n",
       " 'americans': 28,\n",
       " 'americas': 7,\n",
       " 'amezaiku': 3,\n",
       " 'amherst': 6,\n",
       " 'amicable': 4,\n",
       " 'amish': 5,\n",
       " 'among': 5,\n",
       " 'amount': 13,\n",
       " 'amphibians': 5,\n",
       " 'amsterdam': 4,\n",
       " 'amtrak': 3,\n",
       " 'an': 334,\n",
       " 'analysis': 8,\n",
       " 'analyst': 2,\n",
       " 'ancestral': 1,\n",
       " 'ancient': 10,\n",
       " 'ancients': 2,\n",
       " 'and': 1204,\n",
       " 'anderson': 4,\n",
       " 'andie': 2,\n",
       " 'andorra': 1,\n",
       " 'andrea': 4,\n",
       " 'andress': 4,\n",
       " 'andrew': 13,\n",
       " 'andrews': 7,\n",
       " 'andy': 12,\n",
       " 'anesthetic': 2,\n",
       " 'aneurysm': 1,\n",
       " 'angel': 8,\n",
       " 'angela': 2,\n",
       " 'angeles': 25,\n",
       " 'angelica': 4,\n",
       " 'angels': 10,\n",
       " 'angelus': 1,\n",
       " 'angle': 5,\n",
       " 'anglican': 7,\n",
       " 'anglicans': 1,\n",
       " 'animal': 66,\n",
       " 'animals': 49,\n",
       " 'animate': 13,\n",
       " 'anita': 1,\n",
       " 'anka': 4,\n",
       " 'ankle': 2,\n",
       " 'anna': 4,\n",
       " 'anne': 4,\n",
       " 'annie': 1,\n",
       " 'anniversary': 12,\n",
       " 'annotate': 5,\n",
       " 'announce': 8,\n",
       " 'annual': 4,\n",
       " 'anopheles': 5,\n",
       " 'anorexia': 4,\n",
       " 'another': 38,\n",
       " 'answer': 15,\n",
       " 'answers.com': 16,\n",
       " 'antarctica': 5,\n",
       " 'ante': 5,\n",
       " 'anteater': 1,\n",
       " 'anthem': 8,\n",
       " 'anthony': 9,\n",
       " 'anti-aids': 1,\n",
       " 'anti-locking': 1,\n",
       " 'antichrist': 3,\n",
       " 'antidisestablishmentarianism': 5,\n",
       " 'antigua': 2,\n",
       " 'antilles': 3,\n",
       " 'antonia': 1,\n",
       " 'antonio': 4,\n",
       " 'ants': 2,\n",
       " 'anus': 3,\n",
       " 'any': 17,\n",
       " 'anybody': 4,\n",
       " 'anymore': 1,\n",
       " 'anyone': 4,\n",
       " 'anything': 8,\n",
       " 'anzus': 5,\n",
       " 'aol': 2,\n",
       " 'aol.com': 4,\n",
       " 'aortic': 1,\n",
       " 'apache': 1,\n",
       " 'apart': 5,\n",
       " 'apartheid': 4,\n",
       " 'apartment': 5,\n",
       " 'apartments': 2,\n",
       " 'apocalypse': 1,\n",
       " 'apollo': 9,\n",
       " 'apostle': 2,\n",
       " 'appalachian': 3,\n",
       " 'apparel': 5,\n",
       " 'appear': 46,\n",
       " 'appearance': 14,\n",
       " 'appearances': 7,\n",
       " 'appendix': 4,\n",
       " 'apple': 6,\n",
       " 'apples': 16,\n",
       " 'application': 2,\n",
       " 'apply': 5,\n",
       " 'appoint': 6,\n",
       " 'appointments': 1,\n",
       " 'approach': 10,\n",
       " 'appropriate': 2,\n",
       " 'appropriately': 4,\n",
       " 'approval': 4,\n",
       " 'approximate': 10,\n",
       " 'approximately': 5,\n",
       " 'apricot': 5,\n",
       " 'april': 10,\n",
       " 'apso': 4,\n",
       " 'aquatic': 3,\n",
       " 'arab': 4,\n",
       " 'arabia': 3,\n",
       " 'arabian': 5,\n",
       " 'arabic': 4,\n",
       " 'arcadia': 4,\n",
       " 'arcane': 3,\n",
       " 'arch': 13,\n",
       " 'arch-enemy': 5,\n",
       " 'archenemy': 1,\n",
       " 'archer': 2,\n",
       " 'archery': 2,\n",
       " 'archie': 4,\n",
       " 'archimedes': 2,\n",
       " 'architect': 6,\n",
       " 'architecture': 8,\n",
       " 'archy': 5,\n",
       " 'arctic': 13,\n",
       " 'area': 35,\n",
       " 'areas': 10,\n",
       " 'argentina': 3,\n",
       " 'argentine': 3,\n",
       " 'argon': 4,\n",
       " 'argonauts': 1,\n",
       " 'aristotle': 6,\n",
       " 'ark': 1,\n",
       " 'arkansas': 5,\n",
       " 'arles': 2,\n",
       " 'arm': 10,\n",
       " 'arma': 10,\n",
       " 'armor': 2,\n",
       " 'armstrong': 4,\n",
       " 'army': 17,\n",
       " 'arnold': 12,\n",
       " 'arometherapy': 5,\n",
       " 'around': 28,\n",
       " 'arouse': 3,\n",
       " 'arrest': 1,\n",
       " 'arrive': 4,\n",
       " 'arrow': 1,\n",
       " 'arsenal': 3,\n",
       " 'arson': 2,\n",
       " 'art': 35,\n",
       " 'artemis': 4,\n",
       " 'arthur': 11,\n",
       " 'article': 13,\n",
       " 'artificial': 8,\n",
       " 'artist': 23,\n",
       " 'artists': 8,\n",
       " 'arts': 2,\n",
       " 'as': 375,\n",
       " 'ash': 6,\n",
       " 'ashen-faced': 1,\n",
       " 'asia': 10,\n",
       " 'asian': 28,\n",
       " 'asiento': 1,\n",
       " 'ask': 33,\n",
       " 'asleep': 3,\n",
       " 'aspartame': 16,\n",
       " 'assassin': 2,\n",
       " 'assassinate': 35,\n",
       " 'assassination': 12,\n",
       " 'assassinations': 3,\n",
       " 'assembly': 4,\n",
       " 'assent': 3,\n",
       " 'assign': 7,\n",
       " 'assisi': 3,\n",
       " 'associate': 13,\n",
       " 'associated': 2,\n",
       " 'associates': 6,\n",
       " 'association': 12,\n",
       " 'associaton': 4,\n",
       " 'assume': 4,\n",
       " 'astaire': 6,\n",
       " 'asthma': 3,\n",
       " 'astor': 2,\n",
       " 'astronaut': 14,\n",
       " 'astronauts': 20,\n",
       " 'astronomer-architect': 2,\n",
       " 'astronomical': 5,\n",
       " 'astroturf': 1,\n",
       " 'at': 215,\n",
       " 'atari': 5,\n",
       " 'athens': 4,\n",
       " 'athlete': 9,\n",
       " 'athletes': 3,\n",
       " 'athletic': 3,\n",
       " 'atlantic': 16,\n",
       " 'atlas': 1,\n",
       " 'atm': 7,\n",
       " 'atmosphere': 1,\n",
       " 'atom': 4,\n",
       " 'attack': 28,\n",
       " 'attempt': 16,\n",
       " 'attend': 17,\n",
       " 'attendance': 1,\n",
       " 'attendant': 5,\n",
       " 'attic': 5,\n",
       " 'attire': 4,\n",
       " 'attorney-general': 1,\n",
       " 'attorneys': 3,\n",
       " 'attract': 8,\n",
       " 'attraction': 5,\n",
       " 'attractions': 11,\n",
       " 'auberge': 5,\n",
       " 'audience': 1,\n",
       " 'audio': 3,\n",
       " 'audrey': 1,\n",
       " 'auerstadt': 1,\n",
       " 'august': 14,\n",
       " 'auh2o': 1,\n",
       " 'aunt': 1,\n",
       " 'aurora': 1,\n",
       " 'austerlitz': 2,\n",
       " 'australia': 19,\n",
       " 'australian': 8,\n",
       " 'austria': 5,\n",
       " 'author': 40,\n",
       " 'authorities': 2,\n",
       " 'auto': 10,\n",
       " 'auto-commentary': 1,\n",
       " 'autobiography': 12,\n",
       " 'autoimmune': 5,\n",
       " 'automation': 2,\n",
       " 'automobile': 15,\n",
       " 'autry': 1,\n",
       " 'available': 7,\n",
       " 'avalanche': 1,\n",
       " 'avenue': 6,\n",
       " 'average': 88,\n",
       " 'avery': 5,\n",
       " 'aviator': 2,\n",
       " 'aviv': 1,\n",
       " 'avoid': 2,\n",
       " 'awake': 3,\n",
       " 'award': 36,\n",
       " 'awards': 6,\n",
       " 'away': 15,\n",
       " 'ayer': 1,\n",
       " 'aykroyd': 5,\n",
       " 'aztec': 4,\n",
       " 'aztecs': 1,\n",
       " 'b': 4,\n",
       " 'b.y.o.b.': 1,\n",
       " 'b12': 1,\n",
       " 'babar': 5,\n",
       " 'babe': 4,\n",
       " 'baby': 42,\n",
       " 'bacall': 9,\n",
       " 'bachelor': 3,\n",
       " 'back': 32,\n",
       " 'backgammon': 3,\n",
       " 'background': 9,\n",
       " 'backstreet': 3,\n",
       " 'backup': 4,\n",
       " 'bacon': 3,\n",
       " 'bad': 3,\n",
       " 'badaling': 1,\n",
       " 'badge': 3,\n",
       " 'badgers': 4,\n",
       " 'badly': 2,\n",
       " 'badu': 1,\n",
       " 'baffin': 2,\n",
       " 'bagdad': 3,\n",
       " 'bagels': 1,\n",
       " 'baggins': 5,\n",
       " 'baghdad': 3,\n",
       " 'bail': 1,\n",
       " 'bailey': 1,\n",
       " 'baja': 3,\n",
       " 'bake': 11,\n",
       " 'baker': 4,\n",
       " 'bakery': 4,\n",
       " 'balance': 10,\n",
       " 'bald': 3,\n",
       " 'ball': 67,\n",
       " 'ballad': 3,\n",
       " 'ballcock': 5,\n",
       " 'ballet': 5,\n",
       " 'balloon': 4,\n",
       " 'ballot': 5,\n",
       " 'baltic': 5,\n",
       " 'ban': 11,\n",
       " 'banana': 4,\n",
       " 'band': 27,\n",
       " 'bandit': 4,\n",
       " 'bandleader': 1,\n",
       " 'bang': 1,\n",
       " 'bank': 15,\n",
       " 'bankruptcy': 1,\n",
       " 'banner': 7,\n",
       " 'bar': 21,\n",
       " 'bar-code': 1,\n",
       " 'barbados': 9,\n",
       " 'barbara': 5,\n",
       " 'barbary': 1,\n",
       " 'barbeque': 6,\n",
       " 'barbershop': 1,\n",
       " 'barbie': 7,\n",
       " 'bare': 4,\n",
       " 'baretta': 2,\n",
       " 'barkis': 4,\n",
       " 'barney': 5,\n",
       " 'barnstorm': 2,\n",
       " 'barnum': 4,\n",
       " 'barr': 4,\n",
       " 'barrett': 2,\n",
       " 'barrier': 3,\n",
       " 'barroom': 5,\n",
       " 'barrymore': 8,\n",
       " 'bars': 3,\n",
       " 'barton': 1,\n",
       " 'baryshnikov': 1,\n",
       " 'base': 23,\n",
       " 'baseball': 105,\n",
       " 'baseemen': 4,\n",
       " 'baseman': 2,\n",
       " 'basic': 1,\n",
       " 'basidiomycetes': 1,\n",
       " 'basilica': 3,\n",
       " 'basketball': 26,\n",
       " 'baskin': 1,\n",
       " 'basophobic': 5,\n",
       " 'basque': 5,\n",
       " 'bastille': 11,\n",
       " 'bat': 5,\n",
       " 'batcycle': 1,\n",
       " 'bateau-lavoir': 1,\n",
       " 'bath': 4,\n",
       " 'bathroom': 3,\n",
       " 'batman': 1,\n",
       " 'batmobile': 3,\n",
       " 'batteries': 4,\n",
       " 'battery': 16,\n",
       " 'battle': 72,\n",
       " 'battlefield': 3,\n",
       " 'battleship': 4,\n",
       " 'bavaria': 4,\n",
       " 'bavarian': 4,\n",
       " 'bay': 17,\n",
       " 'bayer': 5,\n",
       " 'be': 8511,\n",
       " 'beach': 28,\n",
       " 'beak': 8,\n",
       " 'bean': 3,\n",
       " 'beanie': 6,\n",
       " 'beany': 1,\n",
       " 'bear': 127,\n",
       " 'beard': 4,\n",
       " 'bearer': 3,\n",
       " 'bears': 3,\n",
       " 'beasley': 3,\n",
       " 'beast': 3,\n",
       " 'beat': 20,\n",
       " 'beata': 4,\n",
       " 'beatles': 18,\n",
       " 'beatrix': 4,\n",
       " 'beautiful': 6,\n",
       " 'beauty': 12,\n",
       " 'beaver': 10,\n",
       " 'bebrenia': 3,\n",
       " 'because': 23,\n",
       " 'becket': 1,\n",
       " 'become': 142,\n",
       " 'bed': 4,\n",
       " 'bedroom': 3,\n",
       " 'bee': 16,\n",
       " 'beef': 3,\n",
       " 'beer': 26,\n",
       " 'beer-producing': 5,\n",
       " 'beers': 14,\n",
       " 'beethoven': 10,\n",
       " 'beetle': 4,\n",
       " 'befall': 4,\n",
       " 'before': 45,\n",
       " 'begin': 69,\n",
       " 'behavior': 2,\n",
       " 'behind': 13,\n",
       " 'beholder': 3,\n",
       " 'belgium': 9,\n",
       " 'belgrade': 3,\n",
       " 'believe': 36,\n",
       " 'belize': 2,\n",
       " 'bell': 7,\n",
       " 'bella': 2,\n",
       " 'belle': 3,\n",
       " 'bellworts': 4,\n",
       " 'belly': 3,\n",
       " 'belmont': 3,\n",
       " 'belong': 9,\n",
       " 'below': 2,\n",
       " 'belt': 5,\n",
       " 'belushi': 8,\n",
       " 'ben': 15,\n",
       " 'bend': 6,\n",
       " 'bendix': 1,\n",
       " 'beneath': 3,\n",
       " 'benedict': 3,\n",
       " 'benefit': 3,\n",
       " 'benelux': 5,\n",
       " 'benjamin': 1,\n",
       " 'benny': 12,\n",
       " 'benson': 2,\n",
       " 'bentos': 4,\n",
       " 'bergen-jacqueline': 2,\n",
       " 'bergeres': 3,\n",
       " 'bergman': 3,\n",
       " 'bering': 2,\n",
       " 'berlin': 27,\n",
       " 'bermuda': 4,\n",
       " 'bernadette': 3,\n",
       " 'bernard': 3,\n",
       " 'bernardo': 1,\n",
       " 'bernini-bristol': 1,\n",
       " 'bernoulli': 2,\n",
       " 'bernstein': 3,\n",
       " 'berry': 1,\n",
       " 'bert': 9,\n",
       " 'berth': 1,\n",
       " 'beryl': 3,\n",
       " 'beside': 2,\n",
       " ...}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less commons words in your vocabulary\n",
    "#TODO: vocabulary ne correspond pas à la fréquence des mots, il faut créer un dictionnaire mot:frequence\n",
    "#Counter(vocabulary).most_common(nb_input-1)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 500 words.\n"
     ]
    }
   ],
   "source": [
    "word_list = list([x[0] for x in Counter(vocabulary).most_common(nb_input-1)])\n",
    "\n",
    "# We add the unk word for future purpose.\n",
    "word_list.append('<unk>')\n",
    "words_array = np.array(word_list)\n",
    "print(\"Vocabulary contains\", len(words_array), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Integer encoding with OneHotEncoder\n",
    "words_tre = words_array.reshape(len(words_array),1)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionnary of word and its one hot array\n",
    "words_onehoted = {}\n",
    "for i in range(0, len(words_array)):\n",
    "    words_onehoted[word_list[i]] = onehot_encoded[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the corresponding one hot list for a word.\n",
    "def get_onehot_word(word):\n",
    "    if word in words_onehoted:\n",
    "        return list(words_onehoted[word])\n",
    "    else:\n",
    "        return list(words_onehoted['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_word('<unk>')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if an unknown word is transformed into a <unk>\n",
    "get_onehot_word('obviously_an_unknown_word').index(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot categories\n",
    "\n",
    "categories_onehoted = {}\n",
    "categories_onehoted['ABBR'] = [1, 0, 0, 0, 0, 0] # Abbreviation\n",
    "categories_onehoted['ENTY'] = [0, 1, 0, 0, 0, 0] # Entity\n",
    "categories_onehoted['DESC'] = [0, 0, 1, 0, 0, 0] # Description\n",
    "categories_onehoted['HUM']  = [0, 0, 0, 1, 0, 0] # Human\n",
    "categories_onehoted['LOC']  = [0, 0, 0, 0, 1, 0] # Location\n",
    "categories_onehoted['NUM']  = [0, 0, 0, 0, 0, 1] # Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the corresponding one hot list for a category.\n",
    "def get_onehot_category(category):\n",
    "    return categories_onehoted[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_category('HUM')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creating training set\n",
    "\n",
    "batch_data = []\n",
    "for num_question in range(len(questions)):\n",
    "    # Even if this has already been done earlier, it will be useful for new questions.\n",
    "#     out = vectorizer.fit(questions[num_question])\n",
    "#     vect = out.vocabulary_\n",
    "    words = list(vocabulary.keys())\n",
    "    \n",
    "    # Construction of question_onehot list.\n",
    "    question_onehot = [get_onehot_word(word) for word in words]\n",
    "    \n",
    "    # Construction of category_onehot.\n",
    "    category = labels[num_question].partition(':')[0]\n",
    "    category_onehot = get_onehot_category(category)\n",
    "    batch_data.append([tensor([question_onehot]), tensor([category_onehot])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating test set\n",
    "\n",
    "batch_data_test = []\n",
    "for num_question in range(len(questions_test)):\n",
    "    # Even if this has already been done earlier, it will be useful for new questions.\n",
    "#     out = vectorizer.fit(questions_test[num_question])\n",
    "#     vect = out.vocabulary_\n",
    "    words = list(vocabulary.keys())\n",
    "    \n",
    "    # Construction of question_onehot list.\n",
    "    question_onehot = [get_onehot_word(word) for word in words]\n",
    "    \n",
    "    # Construction of category_onehot.\n",
    "    category = labels_test[num_question].partition(':')[0]\n",
    "    category_onehot = get_onehot_category(category)\n",
    "    batch_data_test.append([tensor([question_onehot]), tensor([category_onehot])])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0.5, batch_first=True, nonlinearity='relu')\n",
    "        self.inter = nn.Linear(nb_neurons, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        #self.synapses = Variable(torch.zeros(self.nb_layers, 1, self.nb_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = Variable(torch.zeros(self.nb_layers, inputs.size(0), self.nb_neurons))\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        \n",
    "        x, hn = self.rnn(inputs, h0)\n",
    "        \n",
    "        x = self.inter(x)\n",
    "        x = nn.functional.softmax(x, dim=2)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, batch_list, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Shuffling batch_list\n",
    "    shuffle(batch_list)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(batch_list):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            #data, target = Variable(data), Variable(target)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "            rnn.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 100 == 0 or batch_idx % 100 == 1 or batch_idx == len(batch_list)-1:\n",
    "                print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                        epoch, \n",
    "                        (batch_idx+1) * len(data), \n",
    "                        len(batch_list),\n",
    "                        100. * (batch_idx+1) / len(batch_list), \n",
    "                        loss.data.item()), \n",
    "                        end='')\n",
    "        print()\n",
    "        \n",
    "    # Return losses list, you can print them later if you want\n",
    "    return losses\n",
    "\n",
    "\n",
    "# return (rightAnswer, ignored, falseAnswer)\n",
    "def getEfficience(rnn, batch_list, tresh=0) :\n",
    "    rightAnswer = 0\n",
    "    ignored = 0\n",
    "    falseAnswer = 0\n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        predicted = rnn(data).detach().cpu().numpy()[-1][-1]\n",
    "        #print(\"predicted: \")\n",
    "        #print(predicted)\n",
    "        #print(\"target: \")\n",
    "        #print(target.detach().cpu().numpy()[-1])\n",
    "        if max(predicted) < tresh :\n",
    "            ignored += 1\n",
    "        else:\n",
    "            if np.argmax(predicted) == np.argmax(target.detach().cpu().numpy()[-1]):\n",
    "                rightAnswer += 1\n",
    "            else:\n",
    "                falseAnswer += 1\n",
    "    return (rightAnswer, ignored, falseAnswer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Done :)\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(nb_inputs = nb_input, nb_layers=nb_hidd_lay,\n",
    "          nb_neurons=hidden_size, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "losses = learn(rnn, batch_data, nb_epochs)\n",
    "print(\"Done :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1300x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5abcdfe642a468c849f395fba3f70fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=51, description='smooth', max=201, min=5, step=2), Output()), _dom_class…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "x = np.linspace(0, len(losses), len(losses))\n",
    "fig = plt.figure(figsize=(13, 8)) \n",
    "ax = fig.add_subplot(1,1,1)\n",
    "cnn_line, = ax.plot(x, losses)\n",
    "\n",
    "def update_losses(smooth=51):\n",
    "    cnn_line.set_ydata(savgol_filter(losses, smooth, 3))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(update_losses, smooth=(5, 201, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-619fbbd29b12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mignored\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfalse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "\n",
    "final_results = getEfficience(rnn, batch_data_test)\n",
    "total = sum(final_results)\n",
    "\n",
    "correct = final_results[0]/total*100\n",
    "ignored = final_results[1]/total*100\n",
    "false = final_results[2]/total*100\n",
    "\n",
    "\n",
    "\n",
    "print(\"Congratulations! On the test set:\")\n",
    "print(\"Corrects: \" + str(correct) + \"%\")\n",
    "#print(\"Ignored:  \" + str(ignored) + \"%\")\n",
    "print(\"False:    \" + str(false) + \"%\")\n",
    "\n",
    "print()\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(nb_input)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(nb_hidd_lay)+\"\\t\"+str(hidden_size)+\"\\t\\t\"+str(correct)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
