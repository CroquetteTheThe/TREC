{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 3\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "train_data = []\n",
    "list_of_words = []\n",
    "for line in file.readlines():\n",
    "    for word in line.split():\n",
    "        list_of_words.append(word)\n",
    "    train_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15452"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divided file into 2 list:\n",
    "# questions = list of questions \n",
    "# labels = list of labels\n",
    "\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "(train_data[0]).split()[0]\n",
    "for string in train_data:\n",
    "    labels.append(string.split()[0])\n",
    "    questions.append(string.split()[1:])\n",
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "# Working on list_of_words\n",
    "def lemm(phrase):\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lemmed = []\n",
    "    for word in phrase:\n",
    "        lemmed.append(lemmer.lemmatize(word, pos=\"v\"))\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lemms = lemm(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to add tags begin and end to phrases list.\n",
    "# /!\\ \n",
    "# WARNING : this method need to be executed only ONE time.\n",
    "# /!\\\n",
    "def add_tag():\n",
    "    for i in range(0, len(questions)):\n",
    "        if questions[i][0] != '<bos>' :\n",
    "            questions[i].insert(0, '<bos>')\n",
    "            questions[i].append('<eos>')\n",
    "add_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sklearn to get the vocabulary\n",
    "vectorizer = CountVectorizer()\n",
    "str_questions = []\n",
    "for quest in questions:\n",
    "    str_questions.append(' '.join(quest))\n",
    "output = vectorizer.fit(str_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 8414 words.\n"
     ]
    }
   ],
   "source": [
    "# This is the vocabulary dict.\n",
    "vocabulary = output.vocabulary_\n",
    "word_list = list(vocabulary.keys())\n",
    "# We add the unk word for future purpose.\n",
    "word_list.append('unk')\n",
    "words_array = np.array(word_list)\n",
    "print(\"Vocabulary contains\", len(words_array), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Integer encoding with OneHotEncoder\n",
    "words_tre = words_array.reshape(len(words_array),1)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionnary of word and its one hot array\n",
    "words_onehoted = {}\n",
    "for i in range(0, len(words_array)):\n",
    "    words_onehoted[word_list[i]] = onehot_encoded[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the corresponding one hot list for a word.\n",
    "def get_onehot_word(word):\n",
    "    out = []\n",
    "    for key, value in words_onehoted.items():\n",
    "        if key == word:\n",
    "            out = list(value)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7898"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_word('unk')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot categories\n",
    "\n",
    "categories_onehoted = {}\n",
    "categories_onehoted['ABBR'] = [1, 0, 0, 0, 0, 0] # Abbreviation\n",
    "categories_onehoted['ENTY'] = [0, 1, 0, 0, 0, 0] # Entity\n",
    "categories_onehoted['DESC'] = [0, 0, 1, 0, 0, 0] # Description\n",
    "categories_onehoted['HUM']  = [0, 0, 0, 1, 0, 0] # Human\n",
    "categories_onehoted['LOC']  = [0, 0, 0, 0, 1, 0] # Location\n",
    "categories_onehoted['NUM']  = [0, 0, 0, 0, 0, 1] # Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the corresponding one hot list for a category.\n",
    "def get_onehot_category(category):\n",
    "    out = []\n",
    "    for key, value in categories_onehoted.items():\n",
    "        if key == category:\n",
    "            out = list(value)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_category('HUM')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_data = []\n",
    "for num_question in range(len(questions)):\n",
    "    # Construction of question_onehot list.\n",
    "    question_onehot = []\n",
    "    # Even if this has already been done earlier, it will be useful for new questions.\n",
    "    out = vectorizer.fit(questions[num_question])\n",
    "    vect = out.vocabulary_\n",
    "    words = list(vect.keys())\n",
    "    for word in words:\n",
    "        question_onehot.append(get_onehot_word(word))\n",
    "    \n",
    "    # Construction of category_onehot.\n",
    "    category = labels[num_question].partition(':')[0]\n",
    "    category_onehot = get_onehot_category(category)\n",
    "    batch_data.append([tensor([question_onehot]), tensor([category_onehot])])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0.5, batch_first=True, nonlinearity='relu')\n",
    "        self.sm = nn.Softmax(nb_outputs)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x, self.synapses = self.rnn(inputs)\n",
    "        #print(\"[osef de ce 1; taille de la phrase; nb de neurones en sortie de la couche rnn]\")\n",
    "        #print(x.size())\n",
    "        #x = self.sm(x)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, batch_list, num_epochs=1):\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(batch_list):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "            rnn.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 100 == 0 or batch_idx % 100 == 1 or batch_idx == len(batch_list)-1:\n",
    "                print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                        epoch, \n",
    "                        (batch_idx+1) * len(data), \n",
    "                        len(batch_list),\n",
    "                        100. * (batch_idx+1) / len(batch_list), \n",
    "                        loss.data.item()), \n",
    "                        end='')\n",
    "        print()\n",
    "        \n",
    "    # Return losses list, you can print them later if you want\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Let's try our RNN\n",
    "Dans cet exemple, nous avons un dataset très basique. Nous avons 2 outputs. Si dans la séquence donnée, une entrée est égale à 1, alors on attendra que la première sortie soit activée. Autrement, nous nous attendrons à ce que la seconde sortie soit activée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/info/etu/m2/i140302/venv/lib/python3.5/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 0 [15452/15452 (100%)]\t Loss: 0.816804\n",
      " Train Epoch: 1 [15452/15452 (100%)]\t Loss: 0.882083\n",
      " Train Epoch: 2 [15452/15452 (100%)]\t Loss: 0.937897\n",
      " Train Epoch: 3 [7602/15452 (49%)]\t Loss: 0.944598"
     ]
    }
   ],
   "source": [
    "# Basically, meet a 1 and activate the first output; the second one if only 0 have been encountered\n",
    "batch_list = [\n",
    "    [tensor([[ [0., 0.], [0., 0.] ]]), tensor([ [0, 1] ])],\n",
    "    [tensor([[ [0., 0.], [0., 0.], [0., 0.] ]]), tensor([ [0, 1] ])],\n",
    "    [tensor([[ [0., 0.], [0., 0.], [0., 0.], [0., 0.] ]]), tensor([ [0, 1] ])],\n",
    "    [tensor([[ [0., 0.], [0., 0.], [0., 0.], [0., 0.], [0., 0.] ]]), tensor([ [0, 1] ])],\n",
    "    [tensor([[ [1., 0.], [0., 0.], [0., 0.] ]]), tensor([ [1, 0] ])],\n",
    "    [tensor([[ [0., 0.], [1., 1.], [0., 0.] ]]), tensor([ [1, 0] ])],\n",
    "    [tensor([[ [0., 0.], [1., 0.], [1., 0.] ]]), tensor([ [1, 0] ])],\n",
    "    [tensor([[ [1., 0.], [1., 0.], [1., 0.] ]]), tensor([ [1, 0] ])]\n",
    "]\n",
    "rnn = RNN(nb_inputs = 8414, nb_layers=1, nb_neurons=6, nb_outputs=6, learning_rate=0.001)\n",
    "\n",
    "#losses = learn(rnn, batch_list, 100)\n",
    "losses = learn(rnn, batch_data, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous affichons notre courbe d'erreur\n",
    "\n",
    "Elle semble la même au fil du temps ici, parce que l'on a vraiment très peu de données\n",
    "\n",
    "(Code provenant du TP de Barrault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "x = np.linspace(0, len(losses), len(losses))\n",
    "fig = plt.figure(figsize=(13, 8)) \n",
    "ax = fig.add_subplot(1,1,1)\n",
    "cnn_line, = ax.plot(x, losses)\n",
    "\n",
    "def update_losses(smooth=51):\n",
    "    cnn_line.set_ydata(savgol_filter(losses, smooth, 3))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(update_losses, smooth=(5, 201, 2));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
