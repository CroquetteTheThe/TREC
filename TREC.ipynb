{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import codecs\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from numpy import array\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of recognized words you put in input\n",
    "nb_input = 1000 #8414 max\n",
    "\n",
    "# Number of classe, constant\n",
    "nb_output = 6\n",
    "\n",
    "# Number of hidden layers\n",
    "nb_hidd_lay = 2\n",
    "hidden_size = 3\n",
    "\n",
    "# Learning rate\n",
    "lr = 0.001\n",
    "\n",
    "# Number of epochs\n",
    "nb_epochs = 10\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding in windows-1252, utf-8 generate error on some char\n",
    "file = codecs.open(\"train_all.label\", \"r+\",\"windows-1252\")\n",
    "train_data = []\n",
    "for line in file.readlines():\n",
    "    train_data.append(line)\n",
    "\n",
    "# Now test set\n",
    "file = codecs.open(\"TREC_test.label\", \"r+\",\"windows-1252\")\n",
    "test_data = []\n",
    "for line in file.readlines():\n",
    "    test_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatisation\n",
    "# Working on list_of_words\n",
    "# TODO: it doesn't work\n",
    "def lemm(phrase):\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    lemmed = []\n",
    "    for word in phrase:\n",
    "        lemmed.append(lemmer.lemmatize(word, pos=\"v\"))\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(questions) = 15452\n",
      "Exemples of questions:\n",
      "[['How', 'do', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'Russia', '?'], ['What', 'film', 'feature', 'the', 'character', 'Popeye', 'Doyle', '?']]\n",
      "\n",
      "And in the test set:\n",
      "len(questions_test) = 500\n",
      "Exemples of questions:\n",
      "[['How', 'far', 'be', 'it', 'from', 'Denver', 'to', 'Aspen', '?'], ['What', 'county', 'be', 'Modesto', ',', 'California', 'in', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Divided file into 2 list:\n",
    "# questions = list of questions \n",
    "# labels = list of labels\n",
    "\n",
    "questions = []\n",
    "labels = []\n",
    "\n",
    "# Black list\n",
    "regex = re.compile('[@_!#$%^&*()<>?/\\|}{~:]')\n",
    "\n",
    "(train_data[0]).split()[0]\n",
    "for string in train_data:\n",
    "    question_str = [x for x in lemm(string.split()[1:]) if regex.search(x) == None]\n",
    "    labels.append(string.split()[0])\n",
    "    questions.append(lemm(string.split()[1:]))\n",
    "print(\"len(questions) = \" + str(len(questions)))\n",
    "print(\"Exemples of questions:\")\n",
    "print(questions[:2])\n",
    "\n",
    "\n",
    "\n",
    "questions_test = []\n",
    "labels_test = []\n",
    "\n",
    "(test_data[0]).split()[0]\n",
    "for string in test_data:\n",
    "    labels_test.append(string.split()[0])\n",
    "    questions_test.append(lemm(string.split()[1:]))\n",
    "print()\n",
    "print(\"And in the test set:\")\n",
    "print(\"len(questions_test) = \" + str(len(questions_test)))\n",
    "print(\"Exemples of questions:\")\n",
    "print(questions_test[:2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to add tags begin and end to phrases list.\n",
    "# /!\\ \n",
    "# WARNING : this method need to be executed only ONE time.\n",
    "# /!\\\n",
    "def add_tag(question_list):\n",
    "    for i in range(0, len(question_list)):\n",
    "        if question_list[i][0] != '<bos>' :\n",
    "            question_list[i].insert(0, '<bos>')\n",
    "            question_list[i].append('<eos>')\n",
    "add_tag(questions)\n",
    "add_tag(questions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188317\n",
      "CPU times: user 64 ms, sys: 12 ms, total: 76 ms\n",
      "Wall time: 74.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Vocabulary of unique words\n",
    "data = []\n",
    "data.append('<unk>')\n",
    "for i in range(0, len(questions)):\n",
    "    q = questions[i]\n",
    "    for word in q:\n",
    "        data.append(word.lower())\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = \"<\"\n",
    "w.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scv = np.array(data)\n",
    "unik, counts = np.unique(scv,return_counts=True)\n",
    "vocabulary = {}\n",
    "for i in range(0, len(unik)):\n",
    "    vocabulary[unik[i]] = counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'silversmith': 2,\n",
       " 'latent': 4,\n",
       " 'watchers': 5,\n",
       " 'wallpaper': 5,\n",
       " 'give': 82,\n",
       " 'inaugurate': 2,\n",
       " 'goalie': 5,\n",
       " 'feather': 5,\n",
       " 'e-mail': 21,\n",
       " 'yeat': 5,\n",
       " 'hemisphere': 8,\n",
       " 'o.j.': 2,\n",
       " 'charcoal': 4,\n",
       " 'mormons': 11,\n",
       " 'sentence': 9,\n",
       " 'league': 33,\n",
       " 'rugs': 1,\n",
       " 'super-teams': 4,\n",
       " 'nestle': 1,\n",
       " 'pregnacy': 5,\n",
       " 'travelers': 2,\n",
       " 'fell': 1,\n",
       " 'tube': 14,\n",
       " 'mig': 1,\n",
       " 'rockettes': 4,\n",
       " 'teenagers': 3,\n",
       " 'snowman': 5,\n",
       " 'morris': 1,\n",
       " 'sisters': 8,\n",
       " 'roosters': 1,\n",
       " 'distribute': 2,\n",
       " 'abdominal': 1,\n",
       " 'peller': 3,\n",
       " 'bayer': 5,\n",
       " 'crazy': 1,\n",
       " 'chile': 3,\n",
       " 'bulbs': 3,\n",
       " 'on-again': 1,\n",
       " 'pushy': 1,\n",
       " 'production': 7,\n",
       " 'sen.': 1,\n",
       " '3': 27,\n",
       " 'd.a.': 2,\n",
       " 'yom': 1,\n",
       " 'rubber': 13,\n",
       " 'toy': 15,\n",
       " 'benny': 12,\n",
       " 'quadruplets': 8,\n",
       " 'scopes': 4,\n",
       " '12601': 2,\n",
       " 'wick': 4,\n",
       " 'caine': 3,\n",
       " '.tbk': 1,\n",
       " 'rockin': 1,\n",
       " 'southern': 19,\n",
       " 'chromosome': 6,\n",
       " 'desmond': 20,\n",
       " 'topophobic': 5,\n",
       " 'habeas': 2,\n",
       " 'boardwalk': 5,\n",
       " 'spritzer': 2,\n",
       " 'another': 38,\n",
       " 'archery': 2,\n",
       " 'glove': 6,\n",
       " '2010': 2,\n",
       " 'admiral': 5,\n",
       " 'drill': 12,\n",
       " 'sprawl': 5,\n",
       " 'student': 6,\n",
       " 'course': 17,\n",
       " 'brigham': 5,\n",
       " 'wheel': 12,\n",
       " '1945': 7,\n",
       " '192': 3,\n",
       " 'ruckus': 1,\n",
       " 'deconstructionism': 1,\n",
       " 'drug': 25,\n",
       " 'inch': 23,\n",
       " 'camera-ready': 3,\n",
       " 'exercise': 11,\n",
       " 'directors': 1,\n",
       " 'farrow': 3,\n",
       " 'abandon': 2,\n",
       " 'bee': 16,\n",
       " 'alcatraz': 1,\n",
       " 'spears': 2,\n",
       " 'arabian': 5,\n",
       " 'compaq': 3,\n",
       " 'inri': 5,\n",
       " 'babe': 4,\n",
       " 'traditional': 3,\n",
       " 'hamilton': 1,\n",
       " 'bikini': 4,\n",
       " 'release': 23,\n",
       " 'gimli': 3,\n",
       " 'crib': 2,\n",
       " 'them': 33,\n",
       " 'aykroyd': 5,\n",
       " 'kyriakos': 1,\n",
       " 'heretic': 4,\n",
       " 'equal': 25,\n",
       " 'betsy': 5,\n",
       " 'tel': 1,\n",
       " 'garmat': 2,\n",
       " 'indicator': 3,\n",
       " 'intake': 5,\n",
       " 'typist': 6,\n",
       " 'commercials': 1,\n",
       " 'silkworm': 2,\n",
       " 'cool': 3,\n",
       " 'rubble': 5,\n",
       " 'out': 109,\n",
       " 'accord': 9,\n",
       " 'snoogans': 5,\n",
       " 'broadway': 18,\n",
       " 'distinction': 6,\n",
       " '528': 1,\n",
       " 'cheerios': 1,\n",
       " 'forever': 2,\n",
       " 'lung': 6,\n",
       " 'abner': 6,\n",
       " 'estate': 8,\n",
       " 'drake': 5,\n",
       " 'joan': 6,\n",
       " 'headquarters': 5,\n",
       " 'panama': 6,\n",
       " 'devil': 3,\n",
       " 'odor': 1,\n",
       " 'rolfe': 5,\n",
       " 'up-to-date': 2,\n",
       " 'avoid': 2,\n",
       " 'fcc': 4,\n",
       " 'circus': 2,\n",
       " 'zero': 4,\n",
       " 'infomatics': 2,\n",
       " 'tracking': 5,\n",
       " 'gestation': 5,\n",
       " 'parasites': 5,\n",
       " 'eight-ball': 1,\n",
       " 'center': 48,\n",
       " 'westward-flowing': 3,\n",
       " '11th-century': 3,\n",
       " 'malaysia': 6,\n",
       " '16th': 13,\n",
       " 'architect': 6,\n",
       " 'shan': 5,\n",
       " 'helps': 3,\n",
       " 'olsen': 7,\n",
       " 'journalism': 4,\n",
       " 'chandler': 3,\n",
       " 'helium': 7,\n",
       " 'hooters': 4,\n",
       " 'carpenter': 2,\n",
       " 'granary': 3,\n",
       " 'maids': 1,\n",
       " 'full': 30,\n",
       " 'macdonald': 2,\n",
       " 'this': 24,\n",
       " 'albert': 6,\n",
       " 'blind': 15,\n",
       " 'biggest-selling': 1,\n",
       " 'chihuahuas': 5,\n",
       " 'orkney': 1,\n",
       " 'surfboard': 3,\n",
       " 'frog': 6,\n",
       " 'pharmacists': 1,\n",
       " 'snowiest': 3,\n",
       " 'lucy': 6,\n",
       " 'clock': 23,\n",
       " 'panel': 5,\n",
       " 'player': 35,\n",
       " 'mankind': 2,\n",
       " 'liston': 4,\n",
       " 'suck': 1,\n",
       " 'luther': 11,\n",
       " 'havilland': 1,\n",
       " 'nautilus': 1,\n",
       " '1853': 3,\n",
       " 'habit': 1,\n",
       " 'mention': 5,\n",
       " 'cache': 8,\n",
       " 'windows': 11,\n",
       " 'textiles': 3,\n",
       " 'longest': 65,\n",
       " 'suffer': 11,\n",
       " 'ruin': 3,\n",
       " 'spritsail': 3,\n",
       " 'pregnancy': 15,\n",
       " '007': 1,\n",
       " 'dc': 20,\n",
       " 'hurt': 6,\n",
       " 'myrtle': 10,\n",
       " 'patrons': 1,\n",
       " 'skrunch': 4,\n",
       " 'pulaski': 5,\n",
       " 'janelle': 5,\n",
       " 'arabia': 3,\n",
       " 'dual-scan': 1,\n",
       " 'highest-paying': 3,\n",
       " 'once-a-year': 4,\n",
       " 'southernmost': 3,\n",
       " 'eldercare': 1,\n",
       " 'corvette': 1,\n",
       " 'beach': 28,\n",
       " 'story': 27,\n",
       " 'springs': 3,\n",
       " 'wed': 6,\n",
       " 'ge': 4,\n",
       " 'popularly': 2,\n",
       " 'jellicle': 5,\n",
       " 'purina': 1,\n",
       " 'surrender': 5,\n",
       " 'stubborn': 5,\n",
       " 'eclairs': 3,\n",
       " 'braun': 5,\n",
       " 'successfully': 2,\n",
       " 'corporal': 5,\n",
       " 'zone': 8,\n",
       " 'caffeine': 14,\n",
       " 'xv': 2,\n",
       " 'inauguration': 4,\n",
       " 'west': 55,\n",
       " 'star-spangled': 4,\n",
       " 'crop': 12,\n",
       " 'travels': 6,\n",
       " 'flame': 5,\n",
       " 'ku': 2,\n",
       " 'your': 137,\n",
       " 'head': 35,\n",
       " 'deadliest': 7,\n",
       " 'nowadays': 5,\n",
       " 'york-style': 4,\n",
       " 'left-handed': 5,\n",
       " 'flourish': 8,\n",
       " 'gangster': 2,\n",
       " 'rossetti': 4,\n",
       " 'zitoni': 2,\n",
       " 'grant': 10,\n",
       " 'brown': 21,\n",
       " 'someone': 41,\n",
       " 'scar-faced': 5,\n",
       " 'loud': 2,\n",
       " 'kramer': 3,\n",
       " 'allah': 5,\n",
       " 'entail': 3,\n",
       " 'gehrig': 8,\n",
       " 'open': 17,\n",
       " 'assume': 4,\n",
       " 'achievements': 3,\n",
       " '1969': 16,\n",
       " 'bishop': 1,\n",
       " 'irkutsk': 5,\n",
       " 'english': 81,\n",
       " 'wee': 3,\n",
       " 'narcolepsy': 3,\n",
       " 'stethoscope': 3,\n",
       " 'roll': 20,\n",
       " 'will': 72,\n",
       " 'unification': 2,\n",
       " 'galaxy': 3,\n",
       " 'david': 13,\n",
       " 'eidologist': 1,\n",
       " 'african': 24,\n",
       " 'chino': 1,\n",
       " 'billie': 5,\n",
       " 'regenerate': 5,\n",
       " 'commonwealth': 1,\n",
       " 'milt': 2,\n",
       " 'lottery': 2,\n",
       " 'mykonos': 3,\n",
       " 'else': 5,\n",
       " 'meyer': 5,\n",
       " 'near': 17,\n",
       " 'horrors': 5,\n",
       " 'spahn': 1,\n",
       " 'enemy': 4,\n",
       " 'colleges': 7,\n",
       " 'blue': 27,\n",
       " 'still': 14,\n",
       " 'cross-your-heart': 1,\n",
       " 'e.': 3,\n",
       " 'alone': 5,\n",
       " 'vats': 2,\n",
       " 'expert': 2,\n",
       " 'tics': 1,\n",
       " 'burglary': 4,\n",
       " 'star': 183,\n",
       " 'younis': 7,\n",
       " 'refer': 12,\n",
       " 'oven': 8,\n",
       " 'pursuit': 6,\n",
       " 'hormone': 1,\n",
       " 'could': 14,\n",
       " 'e-commerce': 8,\n",
       " 'little': 55,\n",
       " 'low': 14,\n",
       " 'name..': 3,\n",
       " 'copacabana': 4,\n",
       " 'leichhardt': 4,\n",
       " 'rid': 8,\n",
       " 'thompson': 1,\n",
       " 'gifford': 1,\n",
       " 'dumas': 4,\n",
       " 'belt': 5,\n",
       " 'dyck': 4,\n",
       " 'drive': 28,\n",
       " 'sculptress': 4,\n",
       " 'commanders': 4,\n",
       " 'brooklyn': 4,\n",
       " 'originally': 11,\n",
       " 'buffett': 1,\n",
       " 'lebanon': 5,\n",
       " 'eunice': 5,\n",
       " '35-millimeter': 4,\n",
       " 'free-legged': 1,\n",
       " 'stevie': 5,\n",
       " 'manuel': 2,\n",
       " 'nnp': 30,\n",
       " 'drew': 8,\n",
       " 'fitzgerald': 5,\n",
       " '21': 18,\n",
       " '43rd': 2,\n",
       " 'creatures': 2,\n",
       " 'discontinue': 1,\n",
       " 'warn': 7,\n",
       " 'least-populated': 4,\n",
       " 'catholic': 9,\n",
       " 'burnford': 4,\n",
       " 'industry': 5,\n",
       " 'ducats': 4,\n",
       " 'medical': 26,\n",
       " 'calendar': 7,\n",
       " 'lata': 4,\n",
       " 'jones': 14,\n",
       " 'domesticate': 9,\n",
       " 'methodist': 4,\n",
       " 'eternity': 11,\n",
       " 'm16': 5,\n",
       " 'commune': 7,\n",
       " 'bisset': 2,\n",
       " 'youngman': 2,\n",
       " 'usenet': 2,\n",
       " 'isthmus': 5,\n",
       " 'circulatory': 3,\n",
       " 'portly': 5,\n",
       " 'havlicek': 4,\n",
       " 'wide': 9,\n",
       " 'crocodile': 3,\n",
       " 'best': 125,\n",
       " 'ebb': 1,\n",
       " 'physical': 9,\n",
       " 'yoo-hoo': 3,\n",
       " 'frequent': 4,\n",
       " 'gamble': 19,\n",
       " 'vacation': 1,\n",
       " 'railways': 1,\n",
       " 'permit': 5,\n",
       " 'assassin': 2,\n",
       " 'sark': 4,\n",
       " 'massive': 1,\n",
       " 'haboob': 2,\n",
       " 'higher': 2,\n",
       " '16-year': 4,\n",
       " 'plains': 1,\n",
       " 'nominations': 5,\n",
       " 'www': 2,\n",
       " 'necessarily': 5,\n",
       " 'taste': 20,\n",
       " 'megan': 1,\n",
       " 'danza': 5,\n",
       " '15': 18,\n",
       " '5': 27,\n",
       " 'eruption': 11,\n",
       " 'wasps': 2,\n",
       " 'ouija': 7,\n",
       " 'wordsworth': 1,\n",
       " 'diplomatic': 4,\n",
       " 'darkness': 3,\n",
       " 'climate': 1,\n",
       " 'prinze': 4,\n",
       " '<unk>': 1,\n",
       " 'abilities': 2,\n",
       " 'element': 31,\n",
       " 'young': 14,\n",
       " 'condense': 1,\n",
       " 'jolson': 4,\n",
       " 'structure': 2,\n",
       " 'getting': 4,\n",
       " 'impact': 4,\n",
       " 'johnson': 14,\n",
       " 'docklands': 3,\n",
       " 'hyde': 1,\n",
       " 'plo': 9,\n",
       " 'agree': 5,\n",
       " 'wish': 3,\n",
       " 'greece': 6,\n",
       " '1990': 9,\n",
       " 'sunday': 5,\n",
       " 'orgin': 2,\n",
       " 'georgetown': 5,\n",
       " 'am': 5,\n",
       " 'abuse': 7,\n",
       " '33': 3,\n",
       " 'morality': 2,\n",
       " 'december': 18,\n",
       " 'tempelhol': 2,\n",
       " 'tribe': 8,\n",
       " 'mortgage': 4,\n",
       " 'recreational': 1,\n",
       " 'diane': 5,\n",
       " 'wilder': 5,\n",
       " 'shouldn': 3,\n",
       " 'dinosaur': 4,\n",
       " 'orca': 6,\n",
       " 'insanity': 3,\n",
       " 'suspension': 4,\n",
       " 'cyclone': 4,\n",
       " '7-eleven': 2,\n",
       " 'easy': 4,\n",
       " 'single': 20,\n",
       " 'seal': 4,\n",
       " 'robert': 15,\n",
       " 'ultraviolet': 3,\n",
       " 'foot-pounds': 3,\n",
       " 'r-rated': 2,\n",
       " 'ce': 1,\n",
       " 'tie': 13,\n",
       " 'bake': 11,\n",
       " 'dispose': 3,\n",
       " 'stitch': 3,\n",
       " 'martha': 6,\n",
       " 'steamboat': 4,\n",
       " 'policy': 6,\n",
       " 'jack': 32,\n",
       " 'butcher': 1,\n",
       " 'titanium': 5,\n",
       " 'contributions': 4,\n",
       " 'gulf': 13,\n",
       " 'relatives': 2,\n",
       " 'jewelry': 1,\n",
       " '1978': 13,\n",
       " 'forest': 22,\n",
       " 'jaws': 33,\n",
       " 'richards': 2,\n",
       " 'surname': 9,\n",
       " 'money': 59,\n",
       " 'domestic': 5,\n",
       " 'historical': 24,\n",
       " 'd.': 14,\n",
       " 'import': 7,\n",
       " 'adenauer': 2,\n",
       " 'deadwood': 5,\n",
       " 'whose': 55,\n",
       " 'predators': 5,\n",
       " 'thursday': 3,\n",
       " 'duck': 7,\n",
       " 'dish': 4,\n",
       " 'opera': 23,\n",
       " 'carl': 8,\n",
       " 'thalia': 1,\n",
       " 'mt.': 1,\n",
       " 'interlata': 1,\n",
       " 'eagle': 7,\n",
       " 'goal': 4,\n",
       " 's': 3,\n",
       " 'europe': 25,\n",
       " 'lloyd': 3,\n",
       " 'bull': 14,\n",
       " 'luggage': 1,\n",
       " 'winners': 6,\n",
       " 'date': 72,\n",
       " 'vbp': 9,\n",
       " 'eels': 4,\n",
       " 'vegas': 21,\n",
       " 'brooks': 11,\n",
       " 'southeast': 2,\n",
       " '1974': 5,\n",
       " 'better': 9,\n",
       " 'nato': 5,\n",
       " 'rick': 4,\n",
       " 'jolly': 3,\n",
       " 'muhammad': 10,\n",
       " 'count': 12,\n",
       " 'shakespeare': 24,\n",
       " 'circumcision': 5,\n",
       " 'jealous': 1,\n",
       " 'islam': 1,\n",
       " 'falconers': 2,\n",
       " 'menace': 6,\n",
       " 'lcd': 3,\n",
       " 'audience': 1,\n",
       " 'btu': 2,\n",
       " 'aspartame': 16,\n",
       " 'lawyer': 38,\n",
       " 'anka': 4,\n",
       " 'viii': 5,\n",
       " 'hungary': 4,\n",
       " 'goldilocks': 2,\n",
       " 'relief': 2,\n",
       " 'insure': 5,\n",
       " 'relate': 7,\n",
       " 'marble': 9,\n",
       " 'nine': 21,\n",
       " 'skein': 5,\n",
       " 'don': 14,\n",
       " 'pines': 3,\n",
       " 'winter': 25,\n",
       " 'joe': 4,\n",
       " 'sharon': 5,\n",
       " 'kinds': 8,\n",
       " 'by': 353,\n",
       " 'crewel': 3,\n",
       " 'marvin': 5,\n",
       " '1923': 9,\n",
       " 'railway': 10,\n",
       " 'beard': 4,\n",
       " 'ghana': 1,\n",
       " 'cage': 11,\n",
       " 'adventures': 5,\n",
       " 'beneath': 3,\n",
       " 'folk': 5,\n",
       " 'polka': 1,\n",
       " 'high': 29,\n",
       " 'peanut': 13,\n",
       " 'jesus': 13,\n",
       " 'asthma': 3,\n",
       " 'chance': 14,\n",
       " 'marvel': 21,\n",
       " 'speaker': 1,\n",
       " 'boost': 1,\n",
       " 'five-time': 1,\n",
       " 'arrow': 1,\n",
       " 'amen': 2,\n",
       " 'scarlet': 1,\n",
       " 'eiffel': 5,\n",
       " 'frustrate': 4,\n",
       " 'pistol': 6,\n",
       " 'drunken': 2,\n",
       " 'commonplace': 1,\n",
       " 'gametophytic': 3,\n",
       " 'history': 58,\n",
       " 'wile': 2,\n",
       " 'maze': 5,\n",
       " 'magnate': 2,\n",
       " 'hotel': 8,\n",
       " 'hazmat': 11,\n",
       " '1975': 6,\n",
       " 'license': 2,\n",
       " 'maintain': 3,\n",
       " 'craig': 2,\n",
       " 'sine': 1,\n",
       " 'caucasian': 4,\n",
       " '455-yard': 4,\n",
       " 'petersburg': 2,\n",
       " 'vladimir': 5,\n",
       " 'leper': 5,\n",
       " 'possible': 4,\n",
       " 'cruiser': 4,\n",
       " 'dna': 8,\n",
       " 'battle': 72,\n",
       " 'nickel-cadmium': 3,\n",
       " 'indianapolis': 2,\n",
       " 'fit': 4,\n",
       " 'email': 17,\n",
       " 'kitchen': 6,\n",
       " 'listeners': 4,\n",
       " 'wilbur': 5,\n",
       " 'attack': 28,\n",
       " 'a-tasket': 1,\n",
       " 'blair': 1,\n",
       " 'agreement': 3,\n",
       " 'hornby': 1,\n",
       " 'hoya': 5,\n",
       " 'almost': 7,\n",
       " 'springfield': 3,\n",
       " 'grace': 18,\n",
       " 'dimaggio': 1,\n",
       " 'melbourne': 2,\n",
       " 'lmds': 1,\n",
       " 'bricks': 5,\n",
       " 'nomadic': 3,\n",
       " 'underwater': 5,\n",
       " 'purchase': 17,\n",
       " 'libya': 4,\n",
       " 'bat': 5,\n",
       " 'males': 2,\n",
       " 'manicure': 2,\n",
       " 'alpha': 4,\n",
       " 'iq': 14,\n",
       " 'falls': 7,\n",
       " 'sperm': 12,\n",
       " 'treasure': 8,\n",
       " 'kindergarden': 3,\n",
       " 'buck': 1,\n",
       " 'elements': 19,\n",
       " 'chest': 5,\n",
       " 'now': 20,\n",
       " 'compile': 2,\n",
       " 'leoncavallo': 1,\n",
       " 'market': 33,\n",
       " 'sidney': 2,\n",
       " 'cinderslut': 5,\n",
       " 'goldenseal': 3,\n",
       " 'cystic': 4,\n",
       " 'pink': 11,\n",
       " 'boast': 46,\n",
       " 'e=mc2': 8,\n",
       " 'string': 9,\n",
       " 'jungle': 6,\n",
       " 'application': 2,\n",
       " 'jericho': 4,\n",
       " 'gatsby': 6,\n",
       " 'yellow': 17,\n",
       " 'muffins': 13,\n",
       " 'debate': 6,\n",
       " 'scorpion': 2,\n",
       " 'monopolies': 1,\n",
       " 'effect': 14,\n",
       " 'milligrams': 4,\n",
       " 'seawater': 4,\n",
       " 'july': 12,\n",
       " 'lady': 15,\n",
       " 'newspapers': 3,\n",
       " 'phil': 3,\n",
       " 'symbol': 34,\n",
       " 'linus': 3,\n",
       " 'layer': 10,\n",
       " 'treatment': 9,\n",
       " 'wasn': 1,\n",
       " 'dolphin': 1,\n",
       " 'cartier': 2,\n",
       " 'beef': 3,\n",
       " 'regan': 3,\n",
       " 'sire': 2,\n",
       " '7-minute': 5,\n",
       " 'enterprise': 11,\n",
       " 'possum': 3,\n",
       " 'edo': 1,\n",
       " 'silver': 14,\n",
       " 'madonna': 14,\n",
       " 'vitreous': 1,\n",
       " 'hairy': 2,\n",
       " 'bind': 3,\n",
       " 'protanopia': 5,\n",
       " 'italian': 20,\n",
       " 'jerry': 10,\n",
       " 'red': 42,\n",
       " 'neurological': 6,\n",
       " 'california': 61,\n",
       " 'seek': 4,\n",
       " 'cereal': 10,\n",
       " 'ossining': 2,\n",
       " 'paper': 28,\n",
       " 'cowrote': 1,\n",
       " 'continental': 7,\n",
       " 'principle': 3,\n",
       " 'court-martial': 3,\n",
       " 'size': 35,\n",
       " 'doorstep': 1,\n",
       " '142': 5,\n",
       " 'h.g.': 1,\n",
       " 'conrad': 9,\n",
       " 'muslim': 5,\n",
       " 'dealer': 6,\n",
       " 'gerald': 8,\n",
       " 'in': 3406,\n",
       " 'lender': 4,\n",
       " 'gasoline': 1,\n",
       " 'sister': 10,\n",
       " 'carta': 9,\n",
       " 'acres': 4,\n",
       " 'cohan': 1,\n",
       " 'mature': 3,\n",
       " 'council': 19,\n",
       " 'limestone': 2,\n",
       " 'now-defunct': 5,\n",
       " 'bubblegum': 1,\n",
       " 'conservationist': 3,\n",
       " 'well': 8,\n",
       " 'sleepless': 5,\n",
       " 'appropriate': 2,\n",
       " 'rolling': 7,\n",
       " 'conservation': 1,\n",
       " 'cabinet': 3,\n",
       " 'stradivarius': 2,\n",
       " 'contract': 15,\n",
       " 'conterminous': 2,\n",
       " 'drinks': 1,\n",
       " 'peninsula': 5,\n",
       " 'crouch': 2,\n",
       " 'alphabetical': 2,\n",
       " 'room': 5,\n",
       " 'whisker': 1,\n",
       " 'mistakenly': 3,\n",
       " 'porgy': 1,\n",
       " 'erle': 2,\n",
       " 'le': 4,\n",
       " 'hope': 2,\n",
       " 'crack': 3,\n",
       " 'eduard': 4,\n",
       " 'lp': 5,\n",
       " 'tonne': 2,\n",
       " 's.': 1,\n",
       " 'fight': 40,\n",
       " 'centigrade': 1,\n",
       " 'journal': 14,\n",
       " 'leg': 9,\n",
       " 'slow': 5,\n",
       " 'stardom': 5,\n",
       " 'jena': 1,\n",
       " 'laugh': 3,\n",
       " 'designer': 9,\n",
       " 'tooth': 1,\n",
       " 'corner': 11,\n",
       " 'doubleheader': 3,\n",
       " 'hours': 7,\n",
       " 'biography': 5,\n",
       " 'hold': 49,\n",
       " 'fivepin': 5,\n",
       " 'firewall': 5,\n",
       " 'loomis': 4,\n",
       " 'chickens': 5,\n",
       " 'volcano': 6,\n",
       " 'creation': 2,\n",
       " 'tourist': 11,\n",
       " 'dooby': 6,\n",
       " 'storm': 11,\n",
       " 'rico': 1,\n",
       " 'work': 105,\n",
       " 'except': 2,\n",
       " 'feynman': 5,\n",
       " 'hundred': 3,\n",
       " 'vlaja': 4,\n",
       " 'necessary': 1,\n",
       " 'balloon': 4,\n",
       " 'donor': 4,\n",
       " 'superbowls': 3,\n",
       " 'miles': 44,\n",
       " 'doughboys': 5,\n",
       " 'steal': 28,\n",
       " 'periodic': 2,\n",
       " 'defreeze': 1,\n",
       " 'resource': 9,\n",
       " 'spanish-american': 6,\n",
       " 'within': 3,\n",
       " 'dust': 3,\n",
       " 'lightning': 6,\n",
       " 'sql': 1,\n",
       " 'lawsuit': 5,\n",
       " 'document': 4,\n",
       " 'beany': 1,\n",
       " 'throat': 1,\n",
       " 'tons': 4,\n",
       " 'lemon': 2,\n",
       " '177': 5,\n",
       " 'warmup': 3,\n",
       " 'sidewinder': 4,\n",
       " 'who': 1725,\n",
       " 'flat': 2,\n",
       " 'hyperlink': 4,\n",
       " 'fifth-largest': 3,\n",
       " 'bomb': 26,\n",
       " 'eckley': 3,\n",
       " 'pageant': 5,\n",
       " 'prenatal': 1,\n",
       " 'coral': 1,\n",
       " 'solid': 2,\n",
       " 'bebrenia': 3,\n",
       " 'month': 16,\n",
       " 'eighth': 2,\n",
       " 'cultural': 1,\n",
       " 'cross': 23,\n",
       " 'sulfur': 4,\n",
       " 'viscosity': 5,\n",
       " 'jude': 23,\n",
       " 'intestines': 4,\n",
       " 'bubble': 3,\n",
       " 'record': 46,\n",
       " 'categorize': 1,\n",
       " 'oyster': 3,\n",
       " 'expose': 2,\n",
       " 'diary': 6,\n",
       " 'diskettes': 5,\n",
       " 'turn': 37,\n",
       " 'tree': 46,\n",
       " 'marquesas': 5,\n",
       " 'dane': 5,\n",
       " 'self-portrait': 3,\n",
       " 'hour': 4,\n",
       " 'recently': 8,\n",
       " 'denver': 2,\n",
       " '16th-century': 4,\n",
       " 'slit': 1,\n",
       " 'secondary': 4,\n",
       " 'revelation': 1,\n",
       " 'oop': 2,\n",
       " 'theta': 4,\n",
       " 'cracker': 4,\n",
       " 'hass': 9,\n",
       " 'alaska': 23,\n",
       " 'scrum': 2,\n",
       " 'machiavelli': 2,\n",
       " 'european': 45,\n",
       " 'incorporate': 4,\n",
       " 'batman': 1,\n",
       " '23': 5,\n",
       " 'alley': 14,\n",
       " 'century': 25,\n",
       " '1.76': 2,\n",
       " 'mercenaries': 2,\n",
       " 'cody': 4,\n",
       " 'isolationist': 1,\n",
       " 'arch-enemy': 5,\n",
       " 'ado': 1,\n",
       " 'serfdom': 5,\n",
       " 'belong': 9,\n",
       " 'catalogue': 4,\n",
       " 'alda': 3,\n",
       " '12-year-old': 5,\n",
       " 'jayne': 5,\n",
       " 'e.g.': 5,\n",
       " 'slightly': 5,\n",
       " 'pita': 3,\n",
       " 'caroll': 4,\n",
       " 'miami': 8,\n",
       " 'jules': 3,\n",
       " 'escape': 2,\n",
       " 'gender': 5,\n",
       " 'lan': 2,\n",
       " 'twenty': 2,\n",
       " 'kurt': 2,\n",
       " 'bergen-jacqueline': 2,\n",
       " 'slave': 8,\n",
       " 'voorhees': 3,\n",
       " 'collector': 1,\n",
       " 'specialize': 5,\n",
       " '1797-185': 9,\n",
       " 'babar': 5,\n",
       " 'nations': 21,\n",
       " 'rathaus': 3,\n",
       " 'grocer': 2,\n",
       " 'barbara': 5,\n",
       " 'automation': 2,\n",
       " 'oompas': 4,\n",
       " 'carrier': 5,\n",
       " 'reb': 2,\n",
       " 'what': 9631,\n",
       " '175': 4,\n",
       " 'predict': 3,\n",
       " 'rhine': 5,\n",
       " 'tornados': 3,\n",
       " 'detergent': 4,\n",
       " 'crash': 1,\n",
       " 'asia': 10,\n",
       " 'movie': 148,\n",
       " 'hound': 3,\n",
       " 'vamp': 3,\n",
       " 'iron': 13,\n",
       " 'alpert': 2,\n",
       " 'infectious': 9,\n",
       " 'isis': 5,\n",
       " 'grow': 33,\n",
       " 'eye': 49,\n",
       " 'is': 7,\n",
       " 'mel': 7,\n",
       " 'courier': 3,\n",
       " 'magnets': 3,\n",
       " 'bank': 15,\n",
       " 'reserve': 2,\n",
       " 'baseman': 2,\n",
       " \"''\": 1201,\n",
       " 'terry': 6,\n",
       " 'intranet': 1,\n",
       " 'contraceptives': 4,\n",
       " 'cologne': 4,\n",
       " 'thanksgiving': 8,\n",
       " 'homelite': 6,\n",
       " 'saratoga': 1,\n",
       " 'apocalypse': 1,\n",
       " 'alice': 11,\n",
       " 'science': 9,\n",
       " 'mast': 1,\n",
       " 'lyricist': 1,\n",
       " 'raven': 4,\n",
       " 'z': 7,\n",
       " 'razor': 8,\n",
       " 'only': 126,\n",
       " 'amazonis': 3,\n",
       " 'oswald': 8,\n",
       " 'refuse': 5,\n",
       " 'machinery': 1,\n",
       " 'execute': 15,\n",
       " 'bog': 10,\n",
       " 'massacre': 6,\n",
       " 'circumnavigator': 2,\n",
       " 'missions': 2,\n",
       " 'minnesota': 8,\n",
       " 'simpson': 4,\n",
       " 'gradual': 5,\n",
       " 'blockade': 2,\n",
       " 'manatees': 5,\n",
       " 'badgers': 4,\n",
       " 'cousteau': 6,\n",
       " 'psorisis': 4,\n",
       " 'cookbook': 3,\n",
       " 'easiest': 5,\n",
       " 'bend': 6,\n",
       " 't.s.': 2,\n",
       " 'koresh': 3,\n",
       " 'christmas': 35,\n",
       " 'neighbor': 5,\n",
       " 'hostages': 5,\n",
       " 'neurosurgeon': 5,\n",
       " 'good': 73,\n",
       " 'stefan': 8,\n",
       " 'remote': 3,\n",
       " 'paint': 68,\n",
       " 'neighborhood': 3,\n",
       " 'u-2': 1,\n",
       " 'charles': 48,\n",
       " 'backstreet': 3,\n",
       " '111': 1,\n",
       " 'bring': 16,\n",
       " 'brunettes': 3,\n",
       " 'poetic': 1,\n",
       " 'degree': 5,\n",
       " 'dramatize': 3,\n",
       " 'mauis': 1,\n",
       " 'evening': 2,\n",
       " 'bail': 1,\n",
       " 'equity': 5,\n",
       " 'equipment': 9,\n",
       " 'multiple': 4,\n",
       " 'literal': 7,\n",
       " 'tongue': 6,\n",
       " 'days': 21,\n",
       " 'hurdle': 1,\n",
       " 'bligh': 4,\n",
       " 'efficient': 9,\n",
       " 'reptiles': 2,\n",
       " '1642-1649': 6,\n",
       " 'vichy': 3,\n",
       " 'html': 3,\n",
       " 'tiffany': 4,\n",
       " 'lose': 20,\n",
       " 'twin': 25,\n",
       " 'hesse': 4,\n",
       " 'dysostosis': 2,\n",
       " 'test': 10,\n",
       " 'francisco': 12,\n",
       " 'movies': 8,\n",
       " 'droppings': 1,\n",
       " 'danube': 10,\n",
       " 'liability': 1,\n",
       " 'college': 67,\n",
       " 'cold': 31,\n",
       " 'pain': 3,\n",
       " '1977': 5,\n",
       " 'mean': 314,\n",
       " 'p-32': 4,\n",
       " 'coordinate': 1,\n",
       " 'resemble': 2,\n",
       " 'thefts': 2,\n",
       " 'transmit': 5,\n",
       " 'hockey': 14,\n",
       " 'gitchee': 2,\n",
       " 'score': 18,\n",
       " 'tide': 1,\n",
       " \"'70\": 1,\n",
       " 'sprain': 2,\n",
       " 'lunch': 9,\n",
       " 'infatuation': 1,\n",
       " 'eh': 4,\n",
       " 'ouarterly': 4,\n",
       " 'cleaveland': 1,\n",
       " 'reactivity': 9,\n",
       " 'actors': 4,\n",
       " '11': 22,\n",
       " 'recall': 1,\n",
       " 'chronic': 10,\n",
       " 'advance': 5,\n",
       " 'tasman': 1,\n",
       " 'graphic': 4,\n",
       " 'cullions': 1,\n",
       " 'cognac': 5,\n",
       " 'cooler': 10,\n",
       " 'these': 16,\n",
       " 'titans': 1,\n",
       " 'ulysses': 5,\n",
       " 'cotrubas': 10,\n",
       " 'roe': 2,\n",
       " 'judo': 2,\n",
       " 'capone': 6,\n",
       " 'dare': 8,\n",
       " 'attire': 4,\n",
       " 'biographer': 1,\n",
       " 'wet': 3,\n",
       " 'trans': 5,\n",
       " 'soundtrack': 2,\n",
       " 'records': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ways', 15),\n",
       " ('1972', 15),\n",
       " ('answer', 15),\n",
       " ('away', 15),\n",
       " ('root', 15),\n",
       " ('research', 15),\n",
       " ('substance', 15),\n",
       " ('coach', 15),\n",
       " ('move', 15),\n",
       " ('mr.', 15)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Less commons words in your vocabulary\n",
    "Counter(vocabulary).most_common(nb_input-1)[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 1000 words.\n"
     ]
    }
   ],
   "source": [
    "word_list = list([x[0] for x in Counter(vocabulary).most_common(nb_input-1)])\n",
    "\n",
    "# We add the unk word for future purpose.\n",
    "word_list.append('<unk>')\n",
    "words_array = np.array(word_list)\n",
    "print(\"Vocabulary contains\", len(words_array), \"words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Integer encoding with OneHotEncoder\n",
    "words_tre = words_array.reshape(len(words_array),1)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = one_hot_encoder.fit_transform(words_tre)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionnary of word and its one hot array\n",
    "words_onehoted = {}\n",
    "for i in range(0, len(words_array)):\n",
    "    words_onehoted[word_list[i]] = onehot_encoded[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the corresponding one hot list for a word.\n",
    "def get_onehot_word(word):\n",
    "    if word in words_onehoted:\n",
    "        return list(words_onehoted[word])\n",
    "    else:\n",
    "        return list(words_onehoted['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_word('<unk>')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing if an unknown word is transformed into a <unk>\n",
    "get_onehot_word('obviously_an_unknown_word').index(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot categories\n",
    "\n",
    "categories_onehoted = {}\n",
    "categories_onehoted['ABBR'] = [1, 0, 0, 0, 0, 0] # Abbreviation\n",
    "categories_onehoted['ENTY'] = [0, 1, 0, 0, 0, 0] # Entity\n",
    "categories_onehoted['DESC'] = [0, 0, 1, 0, 0, 0] # Description\n",
    "categories_onehoted['HUM']  = [0, 0, 0, 1, 0, 0] # Human\n",
    "categories_onehoted['LOC']  = [0, 0, 0, 0, 1, 0] # Location\n",
    "categories_onehoted['NUM']  = [0, 0, 0, 0, 0, 1] # Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the corresponding one hot list for a category.\n",
    "def get_onehot_category(category):\n",
    "    return categories_onehoted[category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = get_onehot_category('HUM')\n",
    "one = oh.index(1.0)\n",
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.1 s, sys: 476 ms, total: 14.5 s\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating training set\n",
    "\n",
    "batch_data = []\n",
    "for num_question in range(len(questions)):\n",
    "    # Construction of question_onehot list.\n",
    "    question_onehot = [get_onehot_word(word) for word in questions[num_question]]\n",
    "    \n",
    "    # Construction of category_onehot.\n",
    "    category = labels[num_question].partition(':')[0]\n",
    "    category_onehot = get_onehot_category(category)\n",
    "    batch_data.append([tensor([question_onehot]), tensor([category_onehot])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 456 ms, sys: 16 ms, total: 472 ms\n",
      "Wall time: 461 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Creating test set\n",
    "\n",
    "batch_data_test = []\n",
    "for num_question in range(len(questions_test)):\n",
    "    \n",
    "    # Construction of question_onehot list.\n",
    "    question_onehot = [get_onehot_word(word) for word in questions[num_question]]\n",
    "    \n",
    "    # Construction of category_onehot.\n",
    "    category = labels_test[num_question].partition(':')[0]\n",
    "    category_onehot = get_onehot_category(category)\n",
    "    batch_data_test.append([tensor([question_onehot]), tensor([category_onehot])])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN implementation\n",
    "Using ReLU, and CrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, nb_inputs, nb_layers, nb_neurons, nb_outputs, learning_rate):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Applying RNN layer, and softmax then\n",
    "        self.rnn = nn.RNN(input_size=nb_inputs, num_layers=nb_layers,\n",
    "                   hidden_size=nb_neurons, dropout=0.5, batch_first=True, nonlinearity='relu')\n",
    "        self.inter = nn.Linear(nb_neurons, nb_outputs)\n",
    "        self.sm = nn.Softmax(dim=2)\n",
    "        \n",
    "        # Other usefull variables here\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        self.input_dim = nb_inputs\n",
    "        self.output_dim = nb_output\n",
    "        self.nb_layers = nb_layers\n",
    "        self.nb_neurons = nb_neurons\n",
    "        #self.synapses = Variable(torch.zeros(self.nb_layers, 1, self.nb_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h0 = Variable(torch.zeros(self.nb_layers, inputs.size(0), self.nb_neurons))\n",
    "        if use_cuda:\n",
    "            h0 = h0.to(\"cuda\")\n",
    "        \n",
    "        x, hn = self.rnn(inputs, h0)\n",
    "        \n",
    "        x = self.inter(x)\n",
    "        x = nn.functional.softmax(x, dim=2)\n",
    "        return x\n",
    "\n",
    "# End of the class RNN\n",
    "\n",
    "# Now let's define learn(), which learn a RNN some data\n",
    "def learn(rnn, batch_list, num_epochs=1):\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    if use_cuda:\n",
    "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "        rnn.cuda(device)\n",
    "    \n",
    "    # Preparing\n",
    "    rnn.train()\n",
    "    losses = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Shuffling batch_list\n",
    "    shuffle(batch_list)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (data, target) in enumerate(batch_list):\n",
    "            \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            #data, target = Variable(data), Variable(target)\n",
    "            \n",
    "            output = rnn(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            losses.append(loss.data.item())\n",
    "\n",
    "            rnn.optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            rnn.optimizer.step()\n",
    "            \n",
    "            # Print the progress\n",
    "            if batch_idx % 100 == 0 or batch_idx % 100 == 1 or batch_idx == len(batch_list)-1:\n",
    "                print('\\r Train Epoch: {} [{}/{} ({:.0f}%)]\\t Loss: {:.6f}'.format(\n",
    "                        epoch, \n",
    "                        (batch_idx+1) * len(data), \n",
    "                        len(batch_list),\n",
    "                        100. * (batch_idx+1) / len(batch_list), \n",
    "                        loss.data.item()), \n",
    "                        end='')\n",
    "        print()\n",
    "        \n",
    "    # Return losses list, you can print them later if you want\n",
    "    return losses\n",
    "\n",
    "\n",
    "# return (rightAnswer, ignored, falseAnswer)\n",
    "def getEfficience(rnn, batch_list, tresh=0) :\n",
    "    rightAnswer = 0\n",
    "    ignored = 0\n",
    "    falseAnswer = 0\n",
    "    \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    for (data, target) in batch_list :\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        predicted = rnn(data).detach().cpu().numpy()[-1][-1]\n",
    "        #print(\"predicted: \")\n",
    "        #print(predicted)\n",
    "        #print(\"target: \")\n",
    "        #print(target.detach().cpu().numpy()[-1])\n",
    "        if max(predicted) < tresh :\n",
    "            ignored += 1\n",
    "        else:\n",
    "            if np.argmax(predicted) == np.argmax(target.detach().cpu().numpy()[-1]):\n",
    "                rightAnswer += 1\n",
    "            else:\n",
    "                falseAnswer += 1\n",
    "    return (rightAnswer, ignored, falseAnswer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 0 [15452/15452 (100%)]\t Loss: 2.341061\n",
      " Train Epoch: 1 [6502/15452 (42%)]\t Loss: 2.149091"
     ]
    }
   ],
   "source": [
    "rnn = RNN(nb_inputs = nb_input, nb_layers=nb_hidd_lay,\n",
    "          nb_neurons=hidden_size, nb_outputs=nb_output, learning_rate=lr)\n",
    "if use_cuda:\n",
    "    rnn = rnn.to(\"cuda\")\n",
    "losses = learn(rnn, batch_data, nb_epochs)\n",
    "print(\"Done :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "x = np.linspace(0, len(losses), len(losses))\n",
    "fig = plt.figure(figsize=(13, 8)) \n",
    "ax = fig.add_subplot(1,1,1)\n",
    "cnn_line, = ax.plot(x, losses)\n",
    "\n",
    "def update_losses(smooth=51):\n",
    "    cnn_line.set_ydata(savgol_filter(losses, smooth, 3))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "interact(update_losses, smooth=(5, 201, 2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_results = getEfficience(rnn, batch_data_test)\n",
    "total = sum(final_results)\n",
    "\n",
    "correct = final_results[0]/total*100\n",
    "ignored = final_results[1]/total*100\n",
    "false = final_results[2]/total*100\n",
    "\n",
    "\n",
    "\n",
    "print(\"Congratulations! On the test set:\")\n",
    "print(\"Corrects: \" + str(correct) + \"%\")\n",
    "#print(\"Ignored:  \" + str(ignored) + \"%\")\n",
    "print(\"False:    \" + str(false) + \"%\")\n",
    "\n",
    "print()\n",
    "print(\"A présent, tu peux copier-coller ça dans le doc sur le drive :)\")\n",
    "print(str(nb_input)+\"\\t\"+str(lr)+\"\\t\"+str(nb_epochs)+\"\\t\"+str(nb_hidd_lay)+\"\\t\"+str(hidden_size)+\"\\t\\t\"+str(correct)+\"%\")\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
